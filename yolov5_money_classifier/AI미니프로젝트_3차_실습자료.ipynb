{"cells":[{"cell_type":"markdown","metadata":{"id":"JLojLUpcGNbk"},"source":["# **차량 공유업체의 차량 파손 여부 분류하기**"]},{"cell_type":"markdown","source":["## 0.미션\n","\n","* 1) 미션1 : Data Preprocessing\n","    - **과제 수행 목표**\n","        - 본인의 구글 드라이브에 모델링 수행을 위해 적절한 폴더 및 파일로 **일관성 있게 정리**해야 합니다.\n","        - 제공된 데이터 : Car_Images.zip\n","            * Car_Images : 차량의 정상/파손 이미지 무작위 수집"],"metadata":{"id":"BbrllJY8JdkF"}},{"cell_type":"markdown","source":["* 2) 미션2 : CNN 모델링\n","    - **과제 수행 목표**\n","        - Tensorflow Keras를 이용하여 모델을 3개 이상 생성하세요.\n","            - 모델 구조와 파라미터는 자유롭게 구성하세요.\n","            - 단, 세부 목차에서 명시한 부분은 지켜주세요."],"metadata":{"id":"Hgdg96jE-mmd"}},{"cell_type":"markdown","source":["* 3) 미션3 : Data Argumentation & Transfer Learning\n","    - **과제 수행 목표**\n","        - 성능 개선을 위해 다음의 두가지를 시도하세요.\n","            * Data Augmentation을 적용하세요.(Image Generator)\n","            * Transfer Learning(VGG16)\n"],"metadata":{"id":"VRrUY4ud_rJV"}},{"cell_type":"markdown","metadata":{"id":"7MdjZtxfGNKz"},"source":["## 1.환경설정 "]},{"cell_type":"markdown","metadata":{"id":"6QgFWzN9xhlr"},"source":["### (1) 데이터셋 폴더 생성\n","- **세부요구사항**\n","    - C드라이브에 Datasets라는 폴더를 만드세요.\n","        - 구글드라이브를 사용하는경우 드라이브 첫 화면에 Datasets 라는 폴더를 만드세요. ('/content/drive/MyDrive/Datasets/')\n","    - 해당 폴더 안에 Car_Images.zip 파일을 넣으세요."]},{"cell_type":"markdown","source":["* 구글 Colab을 이용하는 경우"],"metadata":{"id":"Elg8NL8vwUs5"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"kWUbDvBzwiTq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679294871840,"user_tz":-540,"elapsed":4270,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"2b57fd55-ea89-4765-b3fc-ff6c2e46229e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"0sVNbCKnLUGc"},"source":["### (2) 데이터셋 불러오기 \n","- **세부요구사항**\n","    - Car_Images.zip 파일을 C:/Datasets/ 경로에 압축 해제합니다.\n","    - zipfile 모듈을 이용하거나 다른 방식을 사용해도 됩니다.\n","        - 참고 자료 : [zipfile document](https://docs.python.org/3/library/zipfile.html#zipfile-objects)\n","    - 폴더구조(로컬)\n","        * C:/Datasets/ : 압축파일\n","        * C:/Datasets/Car_Images_train/ : 압축 해제한 이미지 저장소\n","    - 폴더구조(구글드라이브브)\n","        * /content/drive/MyDrive/Datasets/ : 압축파일\n","        * /content/drive/MyDrive/Datasets/Car_Images_train/ : 압축 해제한 이미지 저장소\n","    - 압축을 해제하면 다음과 같은 두 하위 폴더가 생성됩니다.\n","        * normal, abnormal : 각 폴더에는 이미지들이 있습니다.\n","        * 이후 단계에서 해당 경로로 부터 validation, test 셋을 추출하게 됩니다.\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2-8EaA9x4Xm"},"outputs":[],"source":["import zipfile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMkstFLKx4Xm"},"outputs":[],"source":["# 압축파일 경로\n","# 구글 드라이브인 경우 경로에 맞게 지정하세요.\n","dataset_path  = '/content/drive/MyDrive/Datasets/'\n","# dataset_path = 'C:/Datasets/'\n","\n","file_path = dataset_path + 'Car_Images.zip'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgT_RB14Lwza"},"outputs":[],"source":["# 압축 해제\n","# data = zipfile.ZipFile(file_path)\n","# data.extractall( '/content/drive/MyDrive/Datasets/')\n","\n","!unzip /content/drive/MyDrive/Datasets/Car_Images.zip -d /content/drive/MyDrive/Datasets/Car_Images_train/"]},{"cell_type":"markdown","metadata":{"id":"8hgC0axQyMhI"},"source":["### (3) 이미지 저장을 위한 폴더 생성\n","- **세부요구사항**\n","    - train, validation, test 을 위해 각각 하위 폴더 normal과 abnormal를 준비합니다.\n","        - train\n","            * 정상 이미지 저장소 : C:/Datasets/Car_Images_train/normal/ \n","                * 구글드라이브 :   /content/drive/MyDrive/Datasets/Car_Images_train/normal/\n","            * 파손 이미지 저장소 : C:/Datasets/Car_Images_train/abnormal/\n","                * 구글드라이브 : /content/drive/MyDrive/Datasets/Car_Images_train/abnormal/\n","        - val, test 역시 동일한 구조로 생성합니다.\n","    - 직접 탐색기에서 폴더를 생성할 수도 있고, os 모듈을 이용하여 코드로 작성할 수도 있습니다.\n","        - 참고 자료 : [os document](https://docs.python.org/3/library/os.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rc8GnuauOzLf"},"outputs":[],"source":["\n","# 각각 경로 지정\n","import os \n","\n","\n","# train 폴더는 압축을 해제하면서 이미 생성 되어 있습니다.\n","\n","# test 폴더 만들기 os.mkdir()\n","os.mkdir('/content/drive/MyDrive/Datasets/Car_Images_test')\n","os.mkdir('/content/drive/MyDrive/Datasets/Car_Images_test/normal')\n","os.mkdir('/content/drive/MyDrive/Datasets/Car_Images_test/abnormal')\n","\n","# validation 폴더 만들기\n","os.mkdir('/content/drive/MyDrive/Datasets/Car_Images_validation')\n","os.mkdir('/content/drive/MyDrive/Datasets/Car_Images_validation/normal')\n","os.mkdir('/content/drive/MyDrive/Datasets/Car_Images_validation/abnormal')"]},{"cell_type":"markdown","metadata":{"id":"FYZKJrP0GtPh"},"source":["## 2.데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"j-ilpDQfInAE"},"source":["### (1) 데이터 분할 : Training set | Validation set | Test set 생성\n","- **세부요구사항**\n","    - Training set, Validation set, Test set을 만듭니다.\n","        * size\n","            * test : 전체에서 20%를 추출합니다.\n","            * validation : test를 떼어낸 나머지에서 다시 20%를 추출합니다.\n","        * 데이터는 랜덤하게 추출해야 합니다.\n","            - random, shutil 모듈을 이용하여 랜덤하게 추출할 수 있습니다.\n","                - [random document](https://docs.python.org/3/library/random.html) | [shutil document](https://docs.python.org/3/library/shutil.html)\n","            * 해당 모듈 이외에 자신이 잘 알고 있는 방법을 사용해도 됩니다.\n","---"]},{"cell_type":"markdown","source":["#### 1) test, validation 크기를 지정"],"metadata":{"id":"mFMSDA26RS-E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhQ_Gu_KNR2g"},"outputs":[],"source":["import random, shutil"]},{"cell_type":"code","source":["tr_n_path = '/content/drive/MyDrive/Datasets/Car_Images_train/normal/'\n","tr_ab_path = '/content/drive/MyDrive/Datasets/Car_Images_train/abnormal/'"],"metadata":{"id":"vq1kjgfVr_6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdU7X9e70dBu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679276679460,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"473b6f7d-d031-43a1-81da-b4bc3a15718a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(302, 303)"]},"metadata":{},"execution_count":11}],"source":["# 전체 이미지 갯수를 확인합니다.\n","len(os.listdir(tr_n_path)) , len(os.listdir(tr_ab_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oa2mxylBDVM5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679276700163,"user_tz":-540,"elapsed":284,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"81e49dc9-a3c3-4e47-a4f1-d7275b0c962e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[60, 61]\n","[48, 48]\n"]}],"source":["# test 사이즈 : 전체 이미지의 20%\n","te_data_num = [round(len(os.listdir(tr_n_path))*0.2), round(len(os.listdir(tr_ab_path))*0.2)]\n","print(te_data_num)\n","\n","# validation 사이즈 : test를 제외한 나머지 중에서 20%\n","val_data_num = [ round((len(os.listdir(tr_n_path))-te_data_num[0])*0.2) , round((len(os.listdir(tr_n_path))-te_data_num[1])*0.2) ]\n","print(val_data_num)\n","\n","# train 사이즈\n","train_data_num = [len(os.listdir(tr_n_path)) - te_data_num[0] - val_data_num[0],\n","                  len(os.listdir(tr_ab_path))- te_data_num[1] - val_data_num[1]]"]},{"cell_type":"markdown","source":["#### 2) test 셋 추출"],"metadata":{"id":"RmRhrViWRXgL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSwovHr2Fon1"},"outputs":[],"source":["random.seed(2023)\n","# tr_n_path = '/content/drive/MyDrive/Datasets/Car_Images_train/normal/'\n","# tr_ab_path = '/content/drive/MyDrive/Datasets/Car_Images_train/abnormal/'\n","\n","# Set the directory containing the files\n","\n","# Get a list of all the files in the directory\n","n_files = os.listdir(tr_n_path)\n","ab_files = os.listdir(tr_ab_path)\n","# Choose a random file from the list\n","\n","# Shuffle the list of files randomly\n","random.shuffle(n_files)\n","random.shuffle(ab_files)\n","\n","#test\n","test_n_path ='/content/drive/MyDrive/Datasets/Car_Images_test/normal/'\n","\n","# Extract the specified number of random files\n","for i in range(te_data_num[0]):\n","    random_file = n_files[i]\n","    path = tr_n_path+random_file\n","    n_files.remove(random_file)\n","    shutil.move(path, test_n_path)\n","\n","\n","#validaton after extract test image\n"]},{"cell_type":"code","source":["test_ab_path ='/content/drive/MyDrive/Datasets/Car_Images_test/abnormal/'\n","\n","for i in range(te_data_num[1]):\n","    random_file = ab_files[i]\n","    path = tr_ab_path+random_file\n","    ab_files.remove(random_file)\n","    shutil.move(path, test_ab_path)\n"],"metadata":{"id":"Sm4s_KhR5J9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AImO1ujiI2IY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679280190022,"user_tz":-540,"elapsed":355,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"1de0a413-6480-4df2-fcd0-7022d346d54a"},"outputs":[{"output_type":"stream","name":"stdout","text":["60\n","61\n","242\n","242\n"]}],"source":["# 추출 후 이미지 갯수 확인\n","print(len(os.listdir(test_n_path)))\n","print(len(os.listdir(test_ab_path)))\n","print(len(os.listdir(tr_ab_path)))\n","print(len(os.listdir(tr_n_path)))"]},{"cell_type":"markdown","source":["#### 3) validation 셋 추출"],"metadata":{"id":"2V4mh3hxRpR2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXYmEdCjAEDu"},"outputs":[],"source":["random.seed(2023)\n","\n","# Get a list of all the files in the directory\n","n_files = os.listdir(tr_n_path)\n","ab_files = os.listdir(tr_ab_path)\n","# Choose a random file from the list\n","\n","# Shuffle the list of files randomly\n","random.shuffle(n_files)\n","random.shuffle(ab_files)\n","\n","val_n_path = '/content/drive/MyDrive/Datasets/Car_Images_validation/normal'\n","val_ab_path = '/content/drive/MyDrive/Datasets/Car_Images_validation/abnormal'\n","\n","for i in range(val_data_num[0]):\n","    #validation- normal\n","    random_file = n_files[i]\n","    path = tr_n_path+random_file\n","    n_files.remove(random_file)\n","    shutil.move(path,val_n_path)\n","\n","    #validation - abnormal\n","    random_file = ab_files[i]\n","    path = tr_ab_path+random_file\n","    ab_files.remove(random_file)\n","    shutil.move(path,val_ab_path)   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIT85iSdM4U-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679280470363,"user_tz":-540,"elapsed":289,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"a5174853-c7f3-40b2-e643-d47fe5a9e800"},"outputs":[{"output_type":"stream","name":"stdout","text":["48\n","48\n","60\n","61\n","194\n","194\n"]}],"source":["# 추출 후 이미지 갯수 확인\n","print(len(os.listdir(val_ab_path)))\n","print(len(os.listdir(val_n_path)))\n","\n","print(len(os.listdir(test_n_path)))\n","print(len(os.listdir(test_ab_path)))\n","print(len(os.listdir(tr_ab_path)))\n","print(len(os.listdir(tr_n_path)))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"haSO004sgyyu"},"source":["### (2) 데이터 복사 및 이동\n","- **세부요구사항**\n","    - 분할된 데이터를 복사 이동합니다.\n","        - 새로운 폴더에 저장하는 데이터로 \"3.모델링I\"에서 사용합니다.\n","        - 기존 폴더는 \"4.모델링II > (1) Data Augmentation\"에서 사용합니다.\n","    - Training set | Validation set | Test set의 데이터를 **새로운 폴더**에 복사하세요.\n","        - 새로운 폴더 명\n","            * copy_images/trainset\n","            * copy_images/validset\n","            * copy_images/testset\n","        - 새로운 폴더에는 normal, abnormal 파일 모두를 복사합니다. \n","            * 파일을 구분하기 위해 abnormal 파일들은 파일명 앞에 접두사 'ab_'를 붙입시다.\n","        - os, shutil 모듈을 활용하세요."]},{"cell_type":"markdown","source":["#### 1) abnormal 파일 복사"],"metadata":{"id":"3UbNfTY4kOSZ"}},{"cell_type":"markdown","source":["* 복사하기 : shutil.copytree()"],"metadata":{"id":"zhkKqLfTkjGI"}},{"cell_type":"code","source":["os.mkdir('/content/drive/MyDrive/Datasets/copy_images')\n","# os.mkdir('/content/drive/MyDrive/Datasets/copy_images/trianset')\n","# os.mkdir('/content/drive/MyDrive/Datasets/copy_images/testset')\n","# os.mkdir('/content/drive/MyDrive/Datasets/copy_images/validset')"],"metadata":{"id":"M_pXE8-98syR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTMVxJJJya98","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1679282766678,"user_tz":-540,"elapsed":5188,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"5fc98db5-13af-45a3-e47a-2aeb84173f22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Datasets/copy_images/trainset/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}],"source":["import shutil\n","#train\n","tr_copy_path = '/content/drive/MyDrive/Datasets/copy_images/trainset/'\n","shutil.copytree(tr_ab_path, tr_copy_path)"]},{"cell_type":"code","source":["#test\n","test_copy_path = '/content/drive/MyDrive/Datasets/copy_images/testset'\n","shutil.copytree(test_ab_path, test_copy_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"oFxJxFLI-qgj","executionInfo":{"status":"ok","timestamp":1679282768472,"user_tz":-540,"elapsed":1798,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"c2ecc54d-320c-4afa-c201-a2ff4e66e3da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Datasets/copy_images/testset'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["#validation\n","val_copy_path = '/content/drive/MyDrive/Datasets/copy_images/validset'\n","shutil.copytree(val_ab_path, val_copy_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Gjsi7lQJ-q4U","executionInfo":{"status":"ok","timestamp":1679282769231,"user_tz":-540,"elapsed":764,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"02e41d78-3289-4ec0-c0ad-a7f0e3ee802d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Datasets/copy_images/validset'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["* abnormal 이미지 이름의 접두어 \"ab_\" 붙이기 : os.rename"],"metadata":{"id":"mU0T-ypHkV6D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cv6gafRyz6ul"},"outputs":[],"source":["#train\n","tr_ab_files = os.listdir(tr_copy_path)\n","for file in tr_ab_files:\n","    src =dst = os.path.join(tr_copy_path, file)\n","    dst = 'ab_'+file\n","    dst = os.path.join(tr_copy_path, dst)\n","    os.rename(src,dst)\n","\n","\n"]},{"cell_type":"code","source":["#test\n","test_ab_files = os.listdir(test_copy_path)\n","for file in test_ab_files:\n","    src =dst = os.path.join(test_copy_path, file)\n","    dst = 'ab_'+file\n","    dst = os.path.join(test_copy_path, dst)\n","    os.rename(src,dst)"],"metadata":{"id":"-xiTXZ8tAzAi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#validation\n","val_ab_files = os.listdir(val_copy_path)\n","for file in val_ab_files:\n","    src =dst = os.path.join(val_copy_path, file)\n","    dst = 'ab_'+file\n","    dst = os.path.join(val_copy_path, dst)\n","    os.rename(src,dst)"],"metadata":{"id":"Kzkfi1ZsAy1g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) normal 파일 복사"],"metadata":{"id":"Nk6xITmTksyK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vw3DmdTS17RM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282779076,"user_tz":-540,"elapsed":8840,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"c34c80ba-683a-4840-f327-d44041f27958"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 18.52.44 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 22.22.55 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 22.37.34 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.28.24 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.31.15 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.31.20 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.31.56 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.32.44 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.38.57 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.43.03 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.46.27 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.55.59 - a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.58.50 - a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-10 23.58.53 - a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 00.20.00 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 00.25.03 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 00.39.44 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 00.39.47 - photo of a part of car without blemish.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 00.57.16 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 00.58.44 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.00.10 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.01.17 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.01.51 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.06.10 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.07.50 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.07.52 - photo of a part of car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.12.33 - a part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.13.52 - a part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.14.11 - a part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.15.24 - a part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.16.03 - a part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.20.44 - a part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.20.46 - a part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.34.18 - photo of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 01.34.58 - photo of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.12.36 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.22.54 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.23.36 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.24.04 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.25.52 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.28.22 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.28.57 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.29.28 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.29.47 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.32.26 - part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.36.57 - photo of part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.37.01 - photo of part of a car.png',\n"," '/content/drive/MyDrive/Datasets/copy_images/validset/DALLбдE 2023-03-11 14.39.23 - photo of part of a car.png']"]},"metadata":{},"execution_count":52}],"source":["from distutils.dir_util import copy_tree\n","\n","copy_tree(tr_n_path, tr_copy_path)\n","copy_tree(test_n_path, test_copy_path)\n","copy_tree(val_n_path, val_copy_path)"]},{"cell_type":"markdown","source":["* 데이터 갯수 조회"],"metadata":{"id":"xzEXHZrqkz88"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugNprP9d-Gti","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679282795573,"user_tz":-540,"elapsed":386,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"fcd1697a-a683-449f-cbf8-6f9136247de4"},"outputs":[{"output_type":"stream","name":"stdout","text":["388\n","96\n","121\n"]}],"source":["print(len(os.listdir('/content/drive/MyDrive/Datasets/copy_images/trainset/')))\n","print(len(os.listdir('/content/drive/MyDrive/Datasets/copy_images/validset/')))\n","print(len(os.listdir('/content/drive/MyDrive/Datasets/copy_images/testset/')))"]},{"cell_type":"markdown","metadata":{"id":"VfYDW1Pj7ZdU"},"source":["## 3.모델링 I\n","* **세부요구사항**\n","    * 모델링을 위한 데이터 구조 만들기\n","        * x : 이미지를 array로 변환합니다.\n","        * y : 이미지 갯수만큼 normal - 0, abnormal - 1 로 array를 만듭니다.\n","    * 모델을 최소 3개 이상 만들고 성능을 비교합니다.\n","        * 모델 학습 과정에 알맞은 보조 지표를 사용하세요.\n","        * 전처리 과정에서 생성한 Validation set을 적절하게 사용하세요.\n","        * Early Stopping을 반드시 사용하세요.\n","            * 최적의 가중치를 모델에 적용하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rg553KIvxE6W","executionInfo":{"status":"ok","timestamp":1679460162368,"user_tz":-540,"elapsed":34470,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ff87abaf-ee85-4f3c-c035-00e6d0e4e92d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import classification_report, confusion_matrix\n","import os \n","dataset_path  = '/content/drive/MyDrive/Datasets/'\n","from tensorflow.keras.preprocessing import image\n","import glob\n","\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D, Dropout, Concatenate\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"wIfqg6e0xE6A"},"source":["### (1) X : image to array\n","- **세부요구사항**\n","    * 모델링을 위해서는 np.array 형태로 데이터셋을 만들어야 합니다.\n","    * Training set / Validation set / Test set의 X는 이미지 형태로 되어있습니다. \n","    * 이미지 파일을 불러와 train, valid, test 각각 array 형태로 변환해 봅시다.\n","        * 각 폴더로 부터 이미지 목록을 만들고\n","        * 이미지 한장씩 적절한 크기로 로딩하여 (keras.utils.load_img)\n","            * 이미지가 너무 크면 학습시간이 많이 걸리고, 메모리 부족현상이 발생될 수 있습니다.\n","            * 이미지 크기를 280 * 280 * 3 이내의 크기를 설정하여 로딩하시오.\n","            * array로 변환 (keras.utils.img_to_array, np.expand_dims)\n","        * 데이터셋에 추가합니다.(데이터셋도 array)"]},{"cell_type":"markdown","source":["#### 1) 이미지 목록 만들기\n","* train, validation, test 폴더로 부터 이미지 목록을 생성합니다."],"metadata":{"id":"FovkIeSDT367"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SM-ZZ82na2a","executionInfo":{"status":"ok","timestamp":1679462358788,"user_tz":-540,"elapsed":4307,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"5eab3010-fc88-4ba6-c7b7-3d9f3dd2ad7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X022f0QMxE6W"},"outputs":[],"source":["# 이미지 목록 저장\n","img_train_list = os.listdir(dataset_path+'copy_images/trainset/')\n","img_valid_list = os.listdir(dataset_path+'copy_images/validset/')\n","img_test_list = os.listdir(dataset_path+'copy_images/testset/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgvW_LQfxE6X"},"outputs":[],"source":["# 메모리, 처리시간을 위해서 이미지 크기 조정\n","img_size = 280 ## 사이즈 조정 가능"]},{"cell_type":"markdown","source":["#### 2) 이미지들을 배열 데이터셋으로 만들기"],"metadata":{"id":"LSt88mjPV33u"}},{"cell_type":"code","source":[],"metadata":{"id":"iPMFmYPHYCzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhEdBiKfxE6Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679462377881,"user_tz":-540,"elapsed":19097,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"eba7d95f-4c48-459d-bd01-9216503e447f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(388, 280, 280, 3)\n"]}],"source":["from tensorflow.keras.preprocessing import image\n","import glob\n","\n","# train\n","images = []\n","# labels = []\n","tr_files = glob.glob('/content/drive/MyDrive/Datasets/copy_images/trainset/*')\n","for path in tr_files:\n","    img = image.load_img(path, target_size=(280,280) )\n","    img = image.img_to_array(img)\n","    \n","    images.append(img)\n","    # labels.append(names[path.split('/')[-2]])\n","    \n","\n","x_train = np.array(images)\n","#x_train /=255\n","# labels_arr = np.array(labels)\n","\n","print(x_train.shape)\n","# print(labels_arr.shape)\n","\n","\n","\n"]},{"cell_type":"code","source":["#test\n","images = []\n","# labels = []\n","test_files = glob.glob('/content/drive/MyDrive/Datasets/copy_images/testset/*')\n","for path in test_files:\n","    img = image.load_img(path, target_size=(280,280) )\n","    img = image.img_to_array(img)\n","    \n","    images.append(img)\n","    # labels.append(names[path.split('/')[-2]])\n","    \n","\n","x_test = np.array(images)\n","#x_test /= 255\n","# labels_arr = np.array(labels)\n","\n","print(x_test.shape)\n","# print(labels_arr.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61CTXK1pXv32","executionInfo":{"status":"ok","timestamp":1679462382766,"user_tz":-540,"elapsed":4888,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"ae6041b2-5caa-4052-9a73-4779406be1d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(121, 280, 280, 3)\n"]}]},{"cell_type":"code","source":["#validation\n","images = []\n","# labels = []\n","val_files = glob.glob('/content/drive/MyDrive/Datasets/copy_images/validset/*')\n","for path in val_files:\n","    img = image.load_img(path, target_size=(280,280) )\n","    img = image.img_to_array(img)\n","    \n","    images.append(img)\n","    # labels.append(names[path.split('/')[-2]])\n","    \n","\n","x_val = np.array(images)\n","#x_val /=255\n","# labels_arr = np.array(labels)\n","\n","print(x_val.shape)\n","# print(labels_arr.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m33I6d9bZLzp","executionInfo":{"status":"ok","timestamp":1679462399435,"user_tz":-540,"elapsed":16705,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"d3e10181-30f6-4fc7-ec68-fb525ea58fe0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(96, 280, 280, 3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"doUM37LxxE6Z"},"source":["### (2) y : 클래스 만들기\n","- **세부요구사항**\n","    - Training set / Validation set / Test set의 y를 생성합니다.\n","        - 각각 normal, abnormal 데이터의 갯수를 다시 확인하고\n","        - normal을 0, abnormal을 1로 지정합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nl1Uv9UxE6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679462399435,"user_tz":-540,"elapsed":27,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"e7e87f73-5d93-4122-bb3e-de4655134366"},"outputs":[{"output_type":"stream","name":"stdout","text":["388\n","194\n","---\n","96\n","48\n","---\n","121\n","61\n"]}],"source":["# 데이터 갯수 확인\n","print( len(img_train_list) )\n","print( len([val for val in img_train_list if val.startswith('ab_')]) )\n","print('---')\n","print( len(img_valid_list) )\n","print( len([val for val in img_valid_list if val.startswith('ab_')]) )\n","print('---')\n","print( len(img_test_list) )\n","print( len([val for val in img_test_list if val.startswith('ab_')]) )"]},{"cell_type":"markdown","source":["* y_train, y_valid, y_test 만들기\n","    * normal, abnormal 데이터의 갯수를 다시 확인하고 normal을 0, abnormal을 1로 지정합니다."],"metadata":{"id":"HIfaCLlNn04C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVrPQdhTxE6b"},"outputs":[],"source":["\n","labels=[]\n","for x in img_train_list:\n","    if x[:3] == 'ab_':\n","        labels.append(1)\n","    else:\n","        labels.append(0)\n"]},{"cell_type":"code","source":["\n","y_train = np.array(labels)\n","#y_train = to_categorical(labels_tr,2)"],"metadata":{"id":"pnUMwSsmdpxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fMHWYzgeEKN","executionInfo":{"status":"ok","timestamp":1679462399437,"user_tz":-540,"elapsed":26,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"5d285efe-5a4a-440e-de03-8b4011ad9f76"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(388,)"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["\n","labels=[]\n","for x in img_test_list:\n","    if x[:3] == 'ab_':\n","        labels.append(1)\n","    else:\n","        labels.append(0)\n","y_test= np.array(labels)"],"metadata":{"id":"F5hAkqlyePDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels=[]\n","for x in img_valid_list:\n","    if x[:3] == 'ab_':\n","        labels.append(1)\n","    else:\n","        labels.append(0)\n","y_val= np.array(labels)\n","#y_test = to_categorical(labels_val,2)"],"metadata":{"id":"J6TwaR_IeO2l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z586wXFu7ZgT"},"source":["### (3) 모델1\n","- **세부요구사항**\n","    - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 validation_data로 validation set을 사용하시오.\n","    - 반드시 Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["#### 1) 구조 설계"],"metadata":{"id":"NIvIO6RKa0mp"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"7TtIIz6XJQ5E","executionInfo":{"status":"ok","timestamp":1679471973105,"user_tz":-540,"elapsed":2719,"user":{"displayName":"김주환","userId":"07015590008728781094"}}},"outputs":[],"source":["from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D, Dropout, Concatenate"]},{"cell_type":"code","source":["clear_session()\n","#model\n","model = Sequential()\n","\n","# 모델 조립\n","model.add(Input(shape=(280,280,3)))\n","model.add(Conv2D(\n","                filters=32,\n","                padding='same',\n","                 kernel_size=(3,3),\n","                 strides=(1,1),\n","                 activation='relu' \n","                 ))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(\n","                filters=32,\n","                padding='same',\n","                 kernel_size=(3,3),\n","                 strides=(1,1),\n","                 activation='relu' \n","                 ))\n","\n","model.add(BatchNormalization())\n","\n","model.add(MaxPool2D(pool_size=(2,2)))\n","\n","model.add(keras.layers.Dropout(0.25))\n","\n","model.add(Conv2D(\n","                filters=64,\n","                padding='same',\n","                 kernel_size=(3,3),\n","                 strides=(1,1),\n","                 activation='relu' \n","                 ))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(\n","                filters=64,\n","                padding='same',\n","                 kernel_size=(3,3),\n","                 strides=(1,1),\n","                 activation='relu' \n","                 ))\n","\n","model.add(BatchNormalization())\n","\n","model.add(MaxPool2D(pool_size=(2,2)))\n","\n","model.add(keras.layers.Dropout(0.25))\n","\n","model.add( Flatten() )\n","model.add( Dense(512, activation='relu') )\n","model.add( Dense(1, activation='sigmoid'))\n","\n","# 컴파일! \n","# y에 대한 전처리를 하지 않고도 자동으로 판단하고 cross entropy\n","# binary cross entropy를 쓰면 에러 \n","model.compile(loss=keras.losses.binary_crossentropy, metrics=['accuracy'],\n","              optimizer=keras.optimizers.Adam(lr=0.001) )\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCRd7Ap7e_V0","executionInfo":{"status":"ok","timestamp":1679455063084,"user_tz":-540,"elapsed":526,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"b22cf2ce-fbbb-4d98-b35d-5221bb0f9408"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 280, 280, 32)      896       \n","                                                                 \n"," batch_normalization (BatchN  (None, 280, 280, 32)     128       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 280, 280, 32)      9248      \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 280, 280, 32)     128       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 140, 140, 32)     0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 140, 140, 32)      0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 140, 140, 64)      18496     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 140, 140, 64)     256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 140, 140, 64)      36928     \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 140, 140, 64)     256       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 70, 70, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 70, 70, 64)        0         \n","                                                                 \n"," flatten (Flatten)           (None, 313600)            0         \n","                                                                 \n"," dense (Dense)               (None, 512)               160563712 \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 160,630,561\n","Trainable params: 160,630,177\n","Non-trainable params: 384\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"DHM91_bha3Kc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHnFVZuKa42f"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","es  = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n"]},{"cell_type":"code","source":["x_train.shape, y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXj6WQgUgb1-","executionInfo":{"status":"ok","timestamp":1679454920659,"user_tz":-540,"elapsed":5,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"6264540e-979b-439e-f7a4-f7906ce4db4b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((388, 280, 280, 3), (388,))"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BnrTSupKa42f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679455169627,"user_tz":-540,"elapsed":60116,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"b1a6787c-f91b-4ca6-fc3d-61dbae9171c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","25/25 [==============================] - 11s 308ms/step - loss: 48.5745 - accuracy: 0.6521 - val_loss: 51.0954 - val_accuracy: 0.5208\n","Epoch 2/1000\n","25/25 [==============================] - 6s 249ms/step - loss: 1.3965 - accuracy: 0.7552 - val_loss: 42.8754 - val_accuracy: 0.5208\n","Epoch 3/1000\n","25/25 [==============================] - 6s 233ms/step - loss: 0.4000 - accuracy: 0.8273 - val_loss: 17.7756 - val_accuracy: 0.5729\n","Epoch 4/1000\n","25/25 [==============================] - 6s 251ms/step - loss: 0.2901 - accuracy: 0.8995 - val_loss: 7.2521 - val_accuracy: 0.6875\n","Epoch 5/1000\n","25/25 [==============================] - 6s 227ms/step - loss: 0.2322 - accuracy: 0.8840 - val_loss: 3.5290 - val_accuracy: 0.6667\n","Epoch 6/1000\n","25/25 [==============================] - 5s 192ms/step - loss: 0.1702 - accuracy: 0.9149 - val_loss: 3.7226 - val_accuracy: 0.7083\n","Epoch 7/1000\n","25/25 [==============================] - 5s 187ms/step - loss: 0.1615 - accuracy: 0.9459 - val_loss: 4.4625 - val_accuracy: 0.6875\n","Epoch 8/1000\n","25/25 [==============================] - 5s 186ms/step - loss: 0.1106 - accuracy: 0.9407 - val_loss: 3.9514 - val_accuracy: 0.7292\n","Epoch 9/1000\n","25/25 [==============================] - 5s 194ms/step - loss: 0.0713 - accuracy: 0.9562 - val_loss: 4.2146 - val_accuracy: 0.6979\n","Epoch 10/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9613Restoring model weights from the end of the best epoch: 5.\n","25/25 [==============================] - 5s 217ms/step - loss: 0.0838 - accuracy: 0.9613 - val_loss: 3.9430 - val_accuracy: 0.6979\n","Epoch 10: early stopping\n"]}],"source":["hist = model.fit(x_train , y_train, epochs=1000, batch_size=16, validation_data=(x_val, y_val), verbose=1, callbacks=[es])"]},{"cell_type":"markdown","source":["#### 3) test set으로 예측하고 평가하기\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"zage6-Z0a6DX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xkFFlFdbBZb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679455210136,"user_tz":-540,"elapsed":972,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"f67c58e5-b647-469e-8c59-69a812c2a838"},"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 0s 45ms/step - loss: 3.5628 - accuracy: 0.7107\n","Test Loss : 3.562834,  Test Accuracy : 71.074%\n"]}],"source":["performance_test = model.evaluate(x_test, y_test, batch_size=100)\n","\n","print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-EQFVkCbBZc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679381776257,"user_tz":-540,"elapsed":857,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"db53fa3f-0991-4e32-d466-f8ac379c3ae5"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 1s 182ms/step\n"]}],"source":["y_pred = model.predict(x_test)\n","\n","for i in range(len(y_pred)):\n","    if y_pred[i] >= 0.5:\n","        y_pred[i] = 1\n","    else:\n","        y_pred[i] = 0"]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEH6XSKz9VM7","executionInfo":{"status":"ok","timestamp":1679381776258,"user_tz":-540,"elapsed":13,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"fbe17770-75d6-4016-d88f-56a26768b9c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.87      0.77      0.81        60\n","           1       0.79      0.89      0.84        61\n","\n","    accuracy                           0.83       121\n","   macro avg       0.83      0.83      0.83       121\n","weighted avg       0.83      0.83      0.83       121\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mEbBXkRg-7JG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRoacK2mcLPb"},"source":["### (4) 모델2\n","- **세부요구사항**\n","    - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 validation_data로 validation set을 사용하시오.\n","    - 반드시 Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["#### 1) 구조 설계\n","\n","# GoogLeNet\n","\n","* **자기 자신의 미니버전을 만들어내는 것이 좋다.**\n","* 어렵다면, 일단 강사의 미니버전을 코드짜본다!\n","\n","1. 인풋레이어를 제작한다.\n","2. 모든 히든레이어의 activation은 'relu'로 통일\n","3. 첫번째 히든레이어 : Conv 3*3, 32, same\n","4. BatchNorm\n","3. 두번째 히든레이어 : Conv 3*3, 64, same\n","4. BatchNorm\n","5. Maxpooling 2*2\n","6. 드랍아웃 (0.2)\n","2. 위의 드랍아웃 레이어에 연결되는 (왼쪽부터)첫번째 가지\n","    * Convolution : 필터개수 32개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","3. 위의 드랍아웃 레이어에 연결되는 두번째 가지\n","    * Convolution : 필터개수 64개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","    * Convolution : 필터개수 64개, 필터사이즈(3,3), 스트라이드 (1,1), 패딩='same'\n","4. 위의 드랍아웃 레이어에 연결되는 세번째 가지\n","    * Convolution : 필터개수 16개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","    * Convolution : 필터개수 16개, 필터사이즈(5,5), 스트라이드 (1,1), 패딩='same'\n","5. 위의 드랍아웃 레이어에 연결되는 네번째 가지\n","    * MaxPooling : 사이즈(3,3), 스트라이드(1,1), 패딩='same'\n","    * Convolution : 필터개수 16개, 필터사이즈(1,1), 패딩='same'\n","6. Concat.(채널기준으로 통합함. axis=-1 또는 axis=3)\n","7. GlobalAveragePooling2D 레이어\n","13. 아웃풋레이어 \n","\n"],"metadata":{"id":"5WTwG8NFoLBQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHu5gey1oLBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679449304395,"user_tz":-540,"elapsed":809,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"d2924232-e082-4194-fff2-d08fec430e4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 280, 280, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 280, 280, 32  896         ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 280, 280, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 280, 280, 64  18496       ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 280, 280, 64  256        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 140, 140, 64  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," dropout (Dropout)              (None, 140, 140, 64  0           ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 140, 140, 64  4160        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 140, 140, 16  1040        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 140, 140, 64  0          ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 140, 140, 32  2080        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 140, 140, 64  36928       ['conv2d_3[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 140, 140, 16  6416        ['conv2d_5[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 140, 140, 16  1040        ['max_pooling2d_1[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 140, 140, 12  0           ['conv2d_2[0][0]',               \n","                                8)                                'conv2d_4[0][0]',               \n","                                                                  'conv2d_6[0][0]',               \n","                                                                  'conv2d_7[0][0]']               \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 128)         0           ['concatenate[0][0]']            \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," dense (Dense)                  (None, 1)            129         ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 71,569\n","Trainable params: 71,377\n","Non-trainable params: 192\n","__________________________________________________________________________________________________\n"]}],"source":["clear_session()\n","#model\n","# 인풋레이어를 제작한다.\n","il = Input(shape=(280,280,3))\n","# 모든 히든레이어의 activation은 'relu'로 통일\n","# 첫번째 히든레이어 : Conv 3*3, 32, same\n","hl = Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu')(il)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# 두번째 히든레이어 : Conv 3*3, 64, same\n","hl = Conv2D(filters=64, kernel_size=(3,3), padding='same',  activation='relu')(hl)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# Maxpooling 2*2\n","hl = MaxPool2D(pool_size=(2,2))(hl)\n","# 드랍아웃 (0.2)\n","path1 = Dropout(0.25)(hl)\n","# 위의 드랍아웃 레이어에 연결되는 (왼쪽부터)첫번째 가지\n","# Convolution : 필터개수 32개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_1 = Conv2D(filters=32, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# 위의 드랍아웃 레이어에 연결되는 두번째 가지\n","# Convolution : 필터개수 64개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# Convolution : 필터개수 64개, 필터사이즈(3,3), 스트라이드 (1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(3,3),padding='same',  activation='relu')(path1_2)\n","# 위의 드랍아웃 레이어에 연결되는 세번째 가지\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(5,5), 스트라이드 (1,1), 패딩='same'\n","path1_3 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","path1_3 = Conv2D(filters=16, kernel_size=(5,5),padding='same',  activation='relu')(path1_3)\n","# 위의 드랍아웃 레이어에 연결되는 네번째 가지\n","# MaxPooling : 사이즈(3,3), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 패딩='same'\n","path1_4 = MaxPool2D(pool_size=(3,3),strides=(1,1), padding='same')(path1)\n","path1_4 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1_4)\n","# Concat.(채널기준으로 통합함. axis=-1 또는 axis=3)\n","concate = Concatenate()([path1_1, path1_2, path1_3, path1_4])\n","# GlobalAveragePooling2D 레이어\n","hl = GlobalAveragePooling2D()(concate)\n","# 아웃풋레이어\n","ol = Dense(1,activation='sigmoid')(hl)\n","\n","model = Model(il,ol)\n","model.compile(loss=keras.losses.binary_crossentropy, metrics=['accuracy'], optimizer='adam')\n","model.summary()"]},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"DqTzgRTroLBR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcVDXnpQoLBR"},"outputs":[],"source":["es = EarlyStopping(monitor='val_loss',\n","                   min_delta=0,\n","                   patience=7,\n","                   verbose=1,\n","                   restore_best_weights=True\n","                   )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAhXnGmXoLBS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e92829dc-02d0-4680-ffe0-e85b2998f5ee","executionInfo":{"status":"ok","timestamp":1679449452658,"user_tz":-540,"elapsed":131244,"user":{"displayName":"김주환","userId":"07015590008728781094"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","25/25 [==============================] - 92s 246ms/step - loss: 0.5594 - accuracy: 0.7268 - val_loss: 0.7041 - val_accuracy: 0.5000\n","Epoch 2/1000\n","25/25 [==============================] - 6s 221ms/step - loss: 0.4799 - accuracy: 0.7912 - val_loss: 0.7557 - val_accuracy: 0.5000\n","Epoch 3/1000\n","25/25 [==============================] - 6s 221ms/step - loss: 0.4714 - accuracy: 0.7732 - val_loss: 0.8431 - val_accuracy: 0.5000\n","Epoch 4/1000\n","25/25 [==============================] - 6s 224ms/step - loss: 0.4586 - accuracy: 0.7964 - val_loss: 0.9505 - val_accuracy: 0.5000\n","Epoch 5/1000\n","25/25 [==============================] - 6s 222ms/step - loss: 0.4112 - accuracy: 0.8273 - val_loss: 1.0651 - val_accuracy: 0.5000\n","Epoch 6/1000\n","25/25 [==============================] - 6s 223ms/step - loss: 0.4333 - accuracy: 0.8067 - val_loss: 1.2168 - val_accuracy: 0.5000\n","Epoch 7/1000\n","25/25 [==============================] - 6s 220ms/step - loss: 0.4031 - accuracy: 0.8247 - val_loss: 1.3124 - val_accuracy: 0.5000\n","Epoch 8/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.8505Restoring model weights from the end of the best epoch: 1.\n","25/25 [==============================] - 6s 227ms/step - loss: 0.4014 - accuracy: 0.8505 - val_loss: 1.3769 - val_accuracy: 0.5000\n","Epoch 8: early stopping\n"]}],"source":["hist = model.fit(x_train , y_train, epochs=1000, batch_size=16, validation_data=(x_val, y_val), verbose=1, callbacks=[es])"]},{"cell_type":"markdown","source":["#### 3) test set으로 예측하고 평가하기\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"qxZ0U7K1oLBS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShruikbsoLBS"},"outputs":[],"source":["performance_test = model.evaluate(x_test, y_test, batch_size=100)\n","\n","print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8MC8l07oLBS"},"outputs":[],"source":["y_pred = model.predict(x_test)\n","\n","for i in range(len(y_pred)):\n","    if y_pred[i] >= 0.5:\n","        y_pred[i] = 1\n","    else:\n","        y_pred[i] = 0"]},{"cell_type":"code","source":["print(classification_report(y_test,y_pred))"],"metadata":{"id":"G7FVsFiCFSGa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRqzBw8eccwj"},"source":["### (5) 모델3\n","- **세부요구사항**\n","    - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 validation_data로 validation set을 사용하시오.\n","    - 반드시 Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["#### 1) 구조 설계 - 모델2에 data argumentation 적용"],"metadata":{"id":"LtNd8u5RoNJo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KM-Npn6WoNJo"},"outputs":[],"source":[]},{"cell_type":"code","source":["idg = ImageDataGenerator(rotation_range=20, #이미지 회전\n","                         width_shift_range=0.1, #이미지 좌우 이동\n","                         height_shift_range=0.1, #이미지 상하 이송\n","                         zoom_range=0.1,#확대 축소 범위\n","                         shear_range=0.1, \n","                         horizontal_flip=True,\n","                         vertical_flip=True,\n","                         fill_mode='nearest')\n","\n","idg.fit(x_train)"],"metadata":{"id":"vX7EUZU55D2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_idg = idg.flow(x_train, y_train, batch_size=16)"],"metadata":{"id":"ZTPu4ynn5Duy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clear_session()\n","#model\n","# 인풋레이어를 제작한다.\n","il = Input(shape=(280,280,3))\n","# 모든 히든레이어의 activation은 'relu'로 통일\n","# 첫번째 히든레이어 : Conv 3*3, 32, same\n","hl = Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu')(il)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# 두번째 히든레이어 : Conv 3*3, 64, same\n","hl = Conv2D(filters=64, kernel_size=(3,3), padding='same',  activation='relu')(hl)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# Maxpooling 2*2\n","hl = MaxPool2D(pool_size=(2,2))(hl)\n","# 드랍아웃 (0.2)\n","path1 = Dropout(0.25)(hl)\n","# 위의 드랍아웃 레이어에 연결되는 (왼쪽부터)첫번째 가지\n","# Convolution : 필터개수 32개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_1 = Conv2D(filters=32, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# 위의 드랍아웃 레이어에 연결되는 두번째 가지\n","# Convolution : 필터개수 64개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# Convolution : 필터개수 64개, 필터사이즈(3,3), 스트라이드 (1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(3,3),padding='same',  activation='relu')(path1_2)\n","# 위의 드랍아웃 레이어에 연결되는 세번째 가지\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(5,5), 스트라이드 (1,1), 패딩='same'\n","path1_3 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","path1_3 = Conv2D(filters=16, kernel_size=(5,5),padding='same',  activation='relu')(path1_3)\n","# 위의 드랍아웃 레이어에 연결되는 네번째 가지\n","# MaxPooling : 사이즈(3,3), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 패딩='same'\n","path1_4 = MaxPool2D(pool_size=(3,3),strides=(1,1), padding='same')(path1)\n","path1_4 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1_4)\n","# Concat.(채널기준으로 통합함. axis=-1 또는 axis=3)\n","concate = Concatenate()([path1_1, path1_2, path1_3, path1_4])\n","# GlobalAveragePooling2D 레이어\n","hl = GlobalAveragePooling2D()(concate)\n","# 아웃풋레이어\n","ol = Dense(1,activation='sigmoid')(hl)\n","\n","model = Model(il,ol)\n","model.compile(loss=keras.losses.binary_crossentropy, metrics=['accuracy'], optimizer='adam')\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkPeQav15Dp3","executionInfo":{"status":"ok","timestamp":1679375789165,"user_tz":-540,"elapsed":1131,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"b1ef5fe2-90e7-4ef1-9723-1ea3a27c7e01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 280, 280, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 280, 280, 32  896         ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 280, 280, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 280, 280, 64  18496       ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 280, 280, 64  256        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 140, 140, 64  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," dropout (Dropout)              (None, 140, 140, 64  0           ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 140, 140, 64  4160        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 140, 140, 16  1040        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 140, 140, 64  0          ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 140, 140, 32  2080        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 140, 140, 64  36928       ['conv2d_3[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 140, 140, 16  6416        ['conv2d_5[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 140, 140, 16  1040        ['max_pooling2d_1[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 140, 140, 12  0           ['conv2d_2[0][0]',               \n","                                8)                                'conv2d_4[0][0]',               \n","                                                                  'conv2d_6[0][0]',               \n","                                                                  'conv2d_7[0][0]']               \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 128)         0           ['concatenate[0][0]']            \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," dense (Dense)                  (None, 1)            129         ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 71,569\n","Trainable params: 71,377\n","Non-trainable params: 192\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"4zgVkXLHoNJo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTlUNbkhoNJo"},"outputs":[],"source":["es = EarlyStopping(monitor='val_loss',\n","                   min_delta=0,\n","                   patience=7,\n","                   verbose=1,\n","                   restore_best_weights=True)\n","# hist = model.fit(train_idg, validation_data=(x_val, y_val)\n","#                  ,epochs=10000, verbose=1, callbacks=[es])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4GYo0dboNJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679375992370,"user_tz":-540,"elapsed":114995,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"dddcb945-5481-4f89-c6cf-7a10320c8bf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10000\n","25/25 [==============================] - 12s 354ms/step - loss: 0.6760 - accuracy: 0.6572 - val_loss: 1.0818 - val_accuracy: 0.5000\n","Epoch 2/10000\n","25/25 [==============================] - 9s 351ms/step - loss: 0.4928 - accuracy: 0.7680 - val_loss: 1.6395 - val_accuracy: 0.5000\n","Epoch 3/10000\n","25/25 [==============================] - 8s 311ms/step - loss: 0.4631 - accuracy: 0.7809 - val_loss: 1.1988 - val_accuracy: 0.5000\n","Epoch 4/10000\n","25/25 [==============================] - 9s 350ms/step - loss: 0.4788 - accuracy: 0.7861 - val_loss: 1.3579 - val_accuracy: 0.5000\n","Epoch 5/10000\n","25/25 [==============================] - 9s 353ms/step - loss: 0.4653 - accuracy: 0.7912 - val_loss: 0.8106 - val_accuracy: 0.6875\n","Epoch 6/10000\n","25/25 [==============================] - 8s 309ms/step - loss: 0.4969 - accuracy: 0.7680 - val_loss: 0.7309 - val_accuracy: 0.6875\n","Epoch 7/10000\n","25/25 [==============================] - 9s 354ms/step - loss: 0.4441 - accuracy: 0.8015 - val_loss: 0.8073 - val_accuracy: 0.6979\n","Epoch 8/10000\n","25/25 [==============================] - 9s 346ms/step - loss: 0.4311 - accuracy: 0.8222 - val_loss: 0.7667 - val_accuracy: 0.7917\n","Epoch 9/10000\n","25/25 [==============================] - 8s 309ms/step - loss: 0.4246 - accuracy: 0.8170 - val_loss: 0.9377 - val_accuracy: 0.7188\n","Epoch 10/10000\n","25/25 [==============================] - 9s 345ms/step - loss: 0.4171 - accuracy: 0.8196 - val_loss: 0.7758 - val_accuracy: 0.7708\n","Epoch 11/10000\n","25/25 [==============================] - 9s 351ms/step - loss: 0.4411 - accuracy: 0.8015 - val_loss: 0.7660 - val_accuracy: 0.7500\n","Epoch 12/10000\n","25/25 [==============================] - 8s 310ms/step - loss: 0.4526 - accuracy: 0.7912 - val_loss: 0.7659 - val_accuracy: 0.7917\n","Epoch 13/10000\n","25/25 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8015Restoring model weights from the end of the best epoch: 6.\n","25/25 [==============================] - 9s 351ms/step - loss: 0.4159 - accuracy: 0.8015 - val_loss: 0.8197 - val_accuracy: 0.7708\n","Epoch 13: early stopping\n"]}],"source":["hist = model.fit(train_idg, validation_data=(x_val, y_val)\n","                 ,epochs=10000, verbose=1, callbacks=[es])"]},{"cell_type":"markdown","source":["#### 3) test set으로 예측하고 평가하기\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"uZV9zbsroNJo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc9UmjZ0oNJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376026179,"user_tz":-540,"elapsed":16131,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"1bc6765c-8177-48c2-c730-73a4537137b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 0s 55ms/step - loss: 0.5526 - accuracy: 0.7438\n","Test Loss : 0.552594,  Test Accuracy : 74.380%\n"]}],"source":["performance_test = model.evaluate(x_test, y_test, batch_size=100)\n","\n","print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aP-p0_y9oNJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679376034602,"user_tz":-540,"elapsed":1799,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"1ef5f67d-6343-40fe-8819-7f7b312784ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 0s 87ms/step\n"]}],"source":["y_pred = model.predict(x_test)\n","\n","for i in range(len(y_pred)):\n","    if y_pred[i] >= 0.5:\n","        y_pred[i] = 1\n","    else:\n","        y_pred[i] = 0"]},{"cell_type":"code","source":["print(classification_report(y_test,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXdQQ0va6u4k","executionInfo":{"status":"ok","timestamp":1679376058549,"user_tz":-540,"elapsed":282,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"0b6c4aa4-f5a0-406e-d25a-78f1de83ac32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.67      0.97      0.79        60\n","           1       0.94      0.52      0.67        61\n","\n","    accuracy                           0.74       121\n","   macro avg       0.80      0.75      0.73       121\n","weighted avg       0.81      0.74      0.73       121\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3L7mrmby6-nH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g2vGafU27Bz1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ii_7yKcZ7DDK"},"source":["### (6) 모델4\n","- **세부요구사항*\n","   - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 validation_data로 validation set을 사용하시오.\n","    - 반드시 Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["#### 1) 구조 설계 - minmax scaling + ResNet"],"metadata":{"id":"ujXH7zwD7DDK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9OdE7ok7DDK"},"outputs":[],"source":["max_num, min_num = x_train.max(), x_train.min()\n","x_train = (x_train-min_num)/(max_num-min_num)\n","x_val = (x_val-min_num)/(max_num-min_num)\n","x_test = (x_test-min_num)/(max_num-min_num)"]},{"cell_type":"code","source":["# idg = ImageDataGenerator(rotation_range=20,\n","#                          width_shift_range=0.1,\n","#                          height_shift_range=0.1,\n","#                          zoom_range=0.1,\n","#                          shear_range=0.1,\n","#                          horizontal_flip=True,\n","#                          vertical_flip=True)\n","\n","# idg.fit(x_train)"],"metadata":{"id":"Pt23z3327DDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_idg = idg.flow(x_train, y_train)"],"metadata":{"id":"uT1YH2iQ7DDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8V3v3UcHtjiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clear_session()\n","#model\n","# 인풋레이어를 제작한다.\n","il = Input(shape=(280,280,3))\n","# 모든 히든레이어의 activation은 'relu'로 통일\n","# 첫번째 히든레이어 : Conv 3*3, 32, same\n","hl = Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu')(il)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# 두번째 히든레이어 : Conv 3*3, 64, same\n","hl = Conv2D(filters=64, kernel_size=(3,3), padding='same',  activation='relu')(hl)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# Maxpooling 2*2\n","hl = MaxPool2D(pool_size=(2,2))(hl)\n","# 드랍아웃 (0.2)\n","path1 = Dropout(0.25)(hl)\n","# 위의 드랍아웃 레이어에 연결되는 (왼쪽부터)첫번째 가지\n","# Convolution : 필터개수 32개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_1 = Conv2D(filters=32, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# 위의 드랍아웃 레이어에 연결되는 두번째 가지\n","# Convolution : 필터개수 64개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# Convolution : 필터개수 64개, 필터사이즈(3,3), 스트라이드 (1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(3,3),padding='same',  activation='relu')(path1_2)\n","# 위의 드랍아웃 레이어에 연결되는 세번째 가지\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(5,5), 스트라이드 (1,1), 패딩='same'\n","path1_3 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","path1_3 = Conv2D(filters=16, kernel_size=(5,5),padding='same',  activation='relu')(path1_3)\n","# 위의 드랍아웃 레이어에 연결되는 네번째 가지\n","# MaxPooling : 사이즈(3,3), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 패딩='same'\n","path1_4 = MaxPool2D(pool_size=(3,3),strides=(1,1), padding='same')(path1)\n","path1_4 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1_4)\n","# Concat.(채널기준으로 통합함. axis=-1 또는 axis=3)\n","concate = Concatenate()([path1_1, path1_2, path1_3, path1_4])\n","# GlobalAveragePooling2D 레이어\n","hl = GlobalAveragePooling2D()(concate)\n","# 아웃풋레이어\n","ol = Dense(1,activation='sigmoid')(hl)\n","\n","model = Model(il,ol)\n","model.compile(loss=keras.losses.binary_crossentropy, metrics=['accuracy'], optimizer='adam')\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679378121624,"user_tz":-540,"elapsed":472,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"86860489-6514-4671-ff61-3b71b14f72e3","id":"QAktF4No7DDL"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 280, 280, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 280, 280, 32  896         ['input_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 280, 280, 32  128        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 280, 280, 64  18496       ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 280, 280, 64  256        ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 140, 140, 64  0           ['batch_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," dropout (Dropout)              (None, 140, 140, 64  0           ['max_pooling2d[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 140, 140, 64  4160        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 140, 140, 16  1040        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 140, 140, 64  0          ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 140, 140, 32  2080        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 140, 140, 64  36928       ['conv2d_3[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 140, 140, 16  6416        ['conv2d_5[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 140, 140, 16  1040        ['max_pooling2d_1[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 140, 140, 12  0           ['conv2d_2[0][0]',               \n","                                8)                                'conv2d_4[0][0]',               \n","                                                                  'conv2d_6[0][0]',               \n","                                                                  'conv2d_7[0][0]']               \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 128)         0           ['concatenate[0][0]']            \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," dense (Dense)                  (None, 1)            129         ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 71,569\n","Trainable params: 71,377\n","Non-trainable params: 192\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["#### 2) 학습\n","* EarlyStopping 설정하고 학습시키기"],"metadata":{"id":"r-ggkkCh7DDL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0kHqHai7DDL"},"outputs":[],"source":["es = EarlyStopping(monitor='val_loss',\n","                   min_delta=0,\n","                   patience=7,\n","                   verbose=1,\n","                   restore_best_weights=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679378913245,"user_tz":-540,"elapsed":61049,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"67501443-d7a3-4fd1-b725-ba1d772f3d1c","id":"I8WNJw8M7DDL"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10000\n","25/25 [==============================] - 6s 230ms/step - loss: 0.5134 - accuracy: 0.7577 - val_loss: 1.1070 - val_accuracy: 0.5000\n","Epoch 2/10000\n","25/25 [==============================] - 5s 219ms/step - loss: 0.4827 - accuracy: 0.7680 - val_loss: 1.1079 - val_accuracy: 0.5000\n","Epoch 3/10000\n","25/25 [==============================] - 5s 219ms/step - loss: 0.5284 - accuracy: 0.7500 - val_loss: 0.8313 - val_accuracy: 0.5000\n","Epoch 4/10000\n","25/25 [==============================] - 5s 217ms/step - loss: 0.4596 - accuracy: 0.7861 - val_loss: 0.6997 - val_accuracy: 0.5000\n","Epoch 5/10000\n","25/25 [==============================] - 5s 214ms/step - loss: 0.4473 - accuracy: 0.7964 - val_loss: 0.7297 - val_accuracy: 0.5000\n","Epoch 6/10000\n","25/25 [==============================] - 5s 219ms/step - loss: 0.4315 - accuracy: 0.8067 - val_loss: 1.1911 - val_accuracy: 0.5000\n","Epoch 7/10000\n","25/25 [==============================] - 5s 218ms/step - loss: 0.4443 - accuracy: 0.8015 - val_loss: 1.4124 - val_accuracy: 0.5000\n","Epoch 8/10000\n","25/25 [==============================] - 5s 217ms/step - loss: 0.4265 - accuracy: 0.8144 - val_loss: 1.7065 - val_accuracy: 0.5000\n","Epoch 9/10000\n","25/25 [==============================] - 5s 214ms/step - loss: 0.4368 - accuracy: 0.8273 - val_loss: 2.1198 - val_accuracy: 0.5000\n","Epoch 10/10000\n","25/25 [==============================] - 5s 215ms/step - loss: 0.4061 - accuracy: 0.8428 - val_loss: 2.5095 - val_accuracy: 0.5000\n","Epoch 11/10000\n","25/25 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.8041Restoring model weights from the end of the best epoch: 4.\n","25/25 [==============================] - 5s 219ms/step - loss: 0.4307 - accuracy: 0.8041 - val_loss: 2.1213 - val_accuracy: 0.5000\n","Epoch 11: early stopping\n"]}],"source":["hist = model.fit(x_train, y_train,validation_data=(x_val, y_val), batch_size=16,\n","                 epochs=10000, verbose=1, callbacks=[es])"]},{"cell_type":"markdown","source":["#### 3) test set으로 예측하고 평가하기\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"cgFHpBxG7DDL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679379133660,"user_tz":-540,"elapsed":1283,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"d5c5e53d-6b6d-471e-a180-7608bcabb892","id":"qy0nxE487DDM"},"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 1s 51ms/step - loss: 0.6908 - accuracy: 0.4959\n","Test Loss : 0.690752,  Test Accuracy : 49.587%\n"]}],"source":["performance_test = model.evaluate(x_test, y_test, batch_size=100)\n","\n","print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679380985042,"user_tz":-540,"elapsed":1976,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"33096483-a6bc-4735-89a3-fc37307be8a5","id":"8oBvMcpa7DDM"},"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 0s 97ms/step\n"]}],"source":["# performance_test = model.evaluate(x_test, y_test, batch_size=100)\n","\n","# print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))\n","\n","y_pred = model.predict(x_test)\n","\n","for i in range(len(y_pred)):\n","    if y_pred[i] >= 0.5:\n","        y_pred[i] = 1\n","    else:\n","        y_pred[i] = 0\n","# print(classification_report(y_test,y_pred))"]},{"cell_type":"code","source":["print(classification_report(y_test,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679379140636,"user_tz":-540,"elapsed":338,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"ee851848-e163-41bc-af24-20575730433f","id":"voDPOh4P7DDM"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.50      1.00      0.66        60\n","           1       0.00      0.00      0.00        61\n","\n","    accuracy                           0.50       121\n","   macro avg       0.25      0.50      0.33       121\n","weighted avg       0.25      0.50      0.33       121\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","metadata":{"id":"AxUpfhJ1xXle"},"source":["## 4.모델링 II\n","* **세부요구사항**\n","    - 성능을 높이기 위해서 다음의 두가지를 시도해 봅시다.\n","        - Data Augmentation을 통해 데이터를 증가 시킵니다.\n","            - ImageDataGenerator를 사용합니다.\n","        - 사전 학습된 모델(Transfer Learning)을 가져다 사용해 봅시다.\n","            - VGG16(이미지넷)을 사용해 봅시다."]},{"cell_type":"markdown","metadata":{"id":"ouCRBdKPxCut"},"source":["### (1) Data Augmentation\n","- **세부요구사항**\n","    * 모델 학습에 이용할 이미지 데이터를 증강시키세요.\n","    * Keras의 ImageDataGenerator를 이용\n","        - [ImageDataGenerator document](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n","\n","    * image generator를 이용하여 학습\n","        * 모델 구조는 이미 생성한 1,2,3 중 하나를 선택하여 학습\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qe6yjs8F7Zox"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYae9YFt8Q03"},"outputs":[],"source":["img_size = 280 ## 사이즈 조정 가능\n","\n","train_path = dataset_path+'Car_Images_train/'\n","valid_path = dataset_path+'Car_Images_validation/'\n","test_path = dataset_path+'Car_Images_test/'"]},{"cell_type":"markdown","source":["#### 1) ImageGenerator 생성\n","* ImageDataGenerator 함수 사용\n","    * 주요 옵션\n","        * rotation_range: 무작위 회전을 적용할 각도 범위\n","        * zoom_range: 무작위 줌을 적용할 범위 [1-zoom_range, 1+zoom_range]\n","        * horizontal_flip: 무작위 좌우반전을 적용할지 여부\n","        * vertical_flip: 무작위 상하반전을 적용할지 여부\n","        * rescale: 텐서의 모든 값을 rescale 값으로 나누어줌 (이 경우에는 255로 나누어서 0~1사이의 값으로 변경)"],"metadata":{"id":"IP4jIyTGfXD_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKPPSwYn7Zrj"},"outputs":[],"source":["train_datagen = ImageDataGenerator(\n","    rotation_range=30,\n","    zoom_range = 0.2,\n","    width_shift_range = 0.1,\n","    height_shift_range = 0.1,\n","    horizontal_flip = True,\n","    vertical_flip = True,\n","    rescale=1./255,\n",")\n","\n","valid_datagen =  ImageDataGenerator(\n","    rotation_range=30,\n","    zoom_range = 0.2,\n","    width_shift_range = 0.1,\n","    height_shift_range = 0.1,\n","    horizontal_flip = True,\n","    vertical_flip = True,\n","    rescale=1./255,\n",")\n"]},{"cell_type":"markdown","source":["#### 2) 경로로 부터 이미지 불러 올 준비\n","* .flow_from_directory 이용\n","    * 디렉토리에서 이미지를 가져와서 데이터 증강을 적용하고 batch 단위로 제공하는 generator를 생성합니다.\n","    * 이미지를 불러올 때 target_size로 크기를 맞추고, \n","    * class_mode로 이진 분류(binary)를 수행하도록 지정합니다.\n"],"metadata":{"id":"dKwSYYkufanb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bwvQ4hHSCwY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679460313087,"user_tz":-540,"elapsed":3568,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"211a1bd2-30fa-4525-cd01-83bc5164d5ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 388 images belonging to 2 classes.\n","Found 96 images belonging to 2 classes.\n"]}],"source":["train_generator = train_datagen.flow_from_directory(directory=train_path,\n","                                                    target_size=(280,280),\n","                                                    class_mode='binary',\n","                                                    batch_size=16,\n","                                                    shuffle=True\n","                                                    )\n","\n","\n","valid_generator =  valid_datagen.flow_from_directory(directory=valid_path,\n","                                                    target_size=(280,280),\n","                                                    class_mode='binary',\n","                                                    batch_size=16,\n","                                                    shuffle=True\n","                                                    )\n"]},{"cell_type":"code","source":["test_datagen =  ImageDataGenerator(\n","    rescale=1./255,\n",")\n","test_generator =  valid_datagen.flow_from_directory(directory=test_path,                                                    \n","                                                    target_size=(280,280),\n","                                                    class_mode='binary',\n","                                                    batch_size=16,\n","                                                    shuffle=True )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwpZVOg1OYdv","executionInfo":{"status":"ok","timestamp":1679464797747,"user_tz":-540,"elapsed":546,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"e76f5bcd-6a8a-43d3-b9a2-e66dd1010759"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 121 images belonging to 2 classes.\n"]}]},{"cell_type":"markdown","metadata":{"id":"g4RPCjU5f662"},"source":["#### 3) 학습\n","- **세부요구사항**\n","    - Conv2D, MaxPooling2D, Flatten, Dense 레이어들을 이용하여 모델을 설계\n","    - 학습시 train_generator 이용. \n","    - validation_data = valid_generator 지정\n","    - Early Stopping 적용\n","    - 평가시, confusion matrix, accuracy, recall, precision, f1 score 등을 이용하시오."]},{"cell_type":"markdown","source":["* 구조 설계"],"metadata":{"id":"wVMLsXw6f663"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_W7rqgH1f663"},"outputs":[],"source":["clear_session()\n","#model\n","# 인풋레이어를 제작한다.\n","il = Input(shape=(280,280,3))\n","# 모든 히든레이어의 activation은 'relu'로 통일\n","# 첫번째 히든레이어 : Conv 3*3, 32, same\n","hl = Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu')(il)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# 두번째 히든레이어 : Conv 3*3, 64, same\n","hl = Conv2D(filters=64, kernel_size=(3,3), padding='same',  activation='relu')(hl)\n","# BatchNorm\n","hl = BatchNormalization()(hl)\n","# Maxpooling 2*2\n","hl = MaxPool2D(pool_size=(2,2))(hl)\n","# 드랍아웃 (0.2)\n","path1 = Dropout(0.25)(hl)\n","# 위의 드랍아웃 레이어에 연결되는 (왼쪽부터)첫번째 가지\n","# Convolution : 필터개수 32개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_1 = Conv2D(filters=32, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# 위의 드랍아웃 레이어에 연결되는 두번째 가지\n","# Convolution : 필터개수 64개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","# Convolution : 필터개수 64개, 필터사이즈(3,3), 스트라이드 (1,1), 패딩='same'\n","path1_2 = Conv2D(filters=64, kernel_size=(3,3),padding='same',  activation='relu')(path1_2)\n","# 위의 드랍아웃 레이어에 연결되는 세번째 가지\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(5,5), 스트라이드 (1,1), 패딩='same'\n","path1_3 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1)\n","path1_3 = Conv2D(filters=16, kernel_size=(5,5),padding='same',  activation='relu')(path1_3)\n","# 위의 드랍아웃 레이어에 연결되는 네번째 가지\n","# MaxPooling : 사이즈(3,3), 스트라이드(1,1), 패딩='same'\n","# Convolution : 필터개수 16개, 필터사이즈(1,1), 패딩='same'\n","path1_4 = MaxPool2D(pool_size=(3,3),strides=(1,1), padding='same')(path1)\n","path1_4 = Conv2D(filters=16, kernel_size=(1,1),padding='same',  activation='relu')(path1_4)\n","# Concat.(채널기준으로 통합함. axis=-1 또는 axis=3)\n","concate = Concatenate()([path1_1, path1_2, path1_3, path1_4])\n","# GlobalAveragePooling2D 레이어\n","hl = GlobalAveragePooling2D()(concate)\n","# 아웃풋레이어\n","ol = Dense(1,activation='sigmoid')(hl)\n","\n","model = Model(il,ol)\n","model.compile(loss=keras.losses.binary_crossentropy, metrics=['accuracy'], optimizer=keras.optimizers.Adam() )\n","#model.summary()"]},{"cell_type":"markdown","source":["* 학습\n","    * EarlyStopping 설정하기\n","    * 학습 데이터에 train_generator, validation_data=valid_generator 사용"],"metadata":{"id":"nw2_G7zdf663"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6m5mRE9Nf663","colab":{"base_uri":"https://localhost:8080/","height":557},"executionInfo":{"status":"error","timestamp":1679463347722,"user_tz":-540,"elapsed":124037,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"350b9dee-3fd7-4da3-93df-d60c44959cee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10000\n","25/25 [==============================] - 36s 1s/step - loss: 0.5604 - accuracy: 0.6753 - val_loss: 0.7104 - val_accuracy: 0.5000\n","Epoch 2/10000\n","25/25 [==============================] - 29s 1s/step - loss: 0.4922 - accuracy: 0.7706 - val_loss: 0.7949 - val_accuracy: 0.5000\n","Epoch 3/10000\n","25/25 [==============================] - 29s 1s/step - loss: 0.4826 - accuracy: 0.7706 - val_loss: 0.9227 - val_accuracy: 0.5000\n","Epoch 4/10000\n","25/25 [==============================] - ETA: 0s - loss: 0.4732 - accuracy: 0.7912"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-d7c8ed744dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    restore_best_weights=True)\n\u001b[0;32m----> 7\u001b[0;31m hist = model.fit(train_generator, validation_data=valid_generator\n\u001b[0m\u001b[1;32m      8\u001b[0m                  ,epochs=10000, verbose=1, callbacks=[es])\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                         )\n\u001b[0;32m-> 1694\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1695\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                         ):\n\u001b[1;32m   2039\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","es = EarlyStopping(monitor='val_loss',\n","                   min_delta=0,\n","                   patience=7,\n","                   verbose=1,\n","                   restore_best_weights=True)\n","hist = model.fit(train_generator, validation_data=valid_generator\n","                 ,epochs=10000, verbose=1, callbacks=[es])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCWzBSYqf663"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#### 4) 성능 평가\n","* 평가는 confusion_matrix, classification_report 활용"],"metadata":{"id":"BdKiY1uIf663"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qjnvt0lf663"},"outputs":[],"source":["performance_test = model.evaluate(x_test,y_test,  batch_size=100)\n","\n","print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))\n","\n","# y_pred = model.predict(test_ds)\n","\n","# for i in range(len(y_pred)):\n","#     if y_pred[i] >= 0.5:\n","#         y_pred[i] = 1\n","#     else:\n","#         y_pred[i] = 0\n","# print(classification_report(single_y_test,single_y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBl4Do0af663"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"S1iv22vSxXle"},"source":["### (2) Transfer Learning\n","- **세부요구사항**\n","    * VGG16 모델은 1000개의 클래스를 분류하는 데 사용된 ImageNet 데이터셋을 기반으로 사전 학습된 가중치를 가지고 있습니다. \n","        * 따라서 이 모델은 이미지 분류 문제에 대한 높은 성능을 보입니다.\n","        * 이 모델은 보통 전이학습(transfer learning)에서 기본적으로 사용되며, 특히 대규모 데이터셋이 없을 때는 기본 모델로 사용되어 fine-tuning을 수행합니다.\n","    * VGG16 함수로 부터 base_model 저장\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnS12YhDxXle"},"outputs":[],"source":["from tensorflow.keras.applications import VGG16"]},{"cell_type":"code","source":["from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B2\n","from tensorflow.keras.applications.efficientnet_v2 import  preprocess_input\n","from tensorflow.keras.applications.efficientnet_v2 import decode_predictions\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# from tensorflow.keras.preprocessing import image\n","# from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense"],"metadata":{"id":"VO4paunaooHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1) VGG16 불러와서 저장하기\n","* include_top=False로 설정하여 분류기를 제외하고 미리 학습된 가중치 imagenet을 로드합니다.\n","* .trainable을 True로 설정하여 모델의 모든 레이어들이 fine-tuning에 대해 업데이트되도록 합니다.\n"],"metadata":{"id":"d3kyvCwIiAfi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFf3IxbBGe9B"},"outputs":[],"source":["clear_session()\n","base_model = VGG16( weights='imagenet',include_top=False, input_shape=(280,280,3))\n","new_output = GlobalAveragePooling2D()(base_model.output)\n","new_output = Dense(1, activation='sigmoid')(new_output)\n","\n","model = keras.models.Model(base_model.inputs, new_output)\n","\n","\n"]},{"cell_type":"code","source":["for idx, layer in enumerate(model.layers):\n","    layer.trainable =True"],"metadata":{"id":"3XsI45j867VH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) VGG16과 연결한 구조 설계\n","* VGG16을 불러와서 Flatten, Dense 등으로 레이어 연결하기"],"metadata":{"id":"D-JjBLZZiIxA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yg4KhHQ8xXlf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5V5heiDxxXlf"},"source":["#### 3) 학습\n","- **세부요구사항**\n","    - 모델 학습 과정에 알맞은 보조 지표를 사용하세요.\n","    - 데이터\n","        * Image Generator를 연결하거나\n","        * 기존 train, validation 셋을 이용해도 됩니다.\n","        - Early Stopping을 반드시 사용하세요.\n","        - 최적의 가중치를 모델에 적용하세요."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtqQIS-HxXlg"},"outputs":[],"source":["model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n","             optimizer=keras.optimizers.Adam(learning_rate=0.0001) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zg0L88Gwf4l"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n","                                 patience=4,\n","                                 verbose=1,\n","                                 factor=0.5,\n","                                 min_lr=0.000001)"]},{"cell_type":"code","source":["hist = model.fit(train_generator,\n","                 epochs=1000, validation_data=valid_generator,\n","                 verbose=1, callbacks=[es, lr_reduction] )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WO0sYUYg8Uli","executionInfo":{"status":"ok","timestamp":1679466294564,"user_tz":-540,"elapsed":1444014,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"6061e30b-228f-4090-86a7-4984c5dfdd99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","25/25 [==============================] - 35s 1s/step - loss: 0.6081 - accuracy: 0.6959 - val_loss: 0.5120 - val_accuracy: 0.7500 - lr: 1.0000e-04\n","Epoch 2/1000\n","25/25 [==============================] - 31s 1s/step - loss: 0.4503 - accuracy: 0.8222 - val_loss: 0.3565 - val_accuracy: 0.8750 - lr: 1.0000e-04\n","Epoch 3/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.3701 - accuracy: 0.8454 - val_loss: 0.2875 - val_accuracy: 0.8438 - lr: 1.0000e-04\n","Epoch 4/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.3004 - accuracy: 0.8814 - val_loss: 0.2571 - val_accuracy: 0.9167 - lr: 1.0000e-04\n","Epoch 5/1000\n","25/25 [==============================] - 31s 1s/step - loss: 0.2913 - accuracy: 0.9072 - val_loss: 0.2248 - val_accuracy: 0.9167 - lr: 1.0000e-04\n","Epoch 6/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.2437 - accuracy: 0.9124 - val_loss: 0.2131 - val_accuracy: 0.9167 - lr: 1.0000e-04\n","Epoch 7/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.3239 - accuracy: 0.8686 - val_loss: 0.3692 - val_accuracy: 0.8125 - lr: 1.0000e-04\n","Epoch 8/1000\n","25/25 [==============================] - 35s 1s/step - loss: 0.2719 - accuracy: 0.8943 - val_loss: 0.2649 - val_accuracy: 0.8854 - lr: 1.0000e-04\n","Epoch 9/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.3086 - accuracy: 0.8814 - val_loss: 0.4030 - val_accuracy: 0.8229 - lr: 1.0000e-04\n","Epoch 10/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.1885 - accuracy: 0.9407 - val_loss: 0.2104 - val_accuracy: 0.8854 - lr: 1.0000e-04\n","Epoch 11/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.1545 - accuracy: 0.9433 - val_loss: 0.4040 - val_accuracy: 0.8021 - lr: 1.0000e-04\n","Epoch 12/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.1366 - accuracy: 0.9562 - val_loss: 0.1302 - val_accuracy: 0.9375 - lr: 1.0000e-04\n","Epoch 13/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.1174 - accuracy: 0.9665 - val_loss: 0.1953 - val_accuracy: 0.9271 - lr: 1.0000e-04\n","Epoch 14/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.1242 - accuracy: 0.9562 - val_loss: 0.1149 - val_accuracy: 0.9583 - lr: 1.0000e-04\n","Epoch 15/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.1155 - accuracy: 0.9613 - val_loss: 0.1324 - val_accuracy: 0.9792 - lr: 1.0000e-04\n","Epoch 16/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.1441 - accuracy: 0.9536 - val_loss: 0.3213 - val_accuracy: 0.9062 - lr: 1.0000e-04\n","Epoch 17/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.1119 - accuracy: 0.9562 - val_loss: 0.1449 - val_accuracy: 0.9375 - lr: 1.0000e-04\n","Epoch 18/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9562\n","Epoch 18: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n","25/25 [==============================] - 30s 1s/step - loss: 0.1186 - accuracy: 0.9562 - val_loss: 0.5047 - val_accuracy: 0.7812 - lr: 1.0000e-04\n","Epoch 19/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.2165 - accuracy: 0.9149 - val_loss: 0.2531 - val_accuracy: 0.8646 - lr: 5.0000e-05\n","Epoch 20/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.0792 - accuracy: 0.9845 - val_loss: 0.0844 - val_accuracy: 0.9792 - lr: 5.0000e-05\n","Epoch 21/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.0510 - accuracy: 0.9820 - val_loss: 0.2113 - val_accuracy: 0.9062 - lr: 5.0000e-05\n","Epoch 22/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0841 - accuracy: 0.9691 - val_loss: 0.1604 - val_accuracy: 0.9375 - lr: 5.0000e-05\n","Epoch 23/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.0855 - accuracy: 0.9768 - val_loss: 0.1236 - val_accuracy: 0.9792 - lr: 5.0000e-05\n","Epoch 24/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9716\n","Epoch 24: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n","25/25 [==============================] - 30s 1s/step - loss: 0.0710 - accuracy: 0.9716 - val_loss: 0.0921 - val_accuracy: 0.9688 - lr: 5.0000e-05\n","Epoch 25/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0416 - accuracy: 0.9871 - val_loss: 0.0892 - val_accuracy: 0.9792 - lr: 2.5000e-05\n","Epoch 26/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0439 - accuracy: 0.9871 - val_loss: 0.0567 - val_accuracy: 0.9792 - lr: 2.5000e-05\n","Epoch 27/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.0206 - accuracy: 0.9948 - val_loss: 0.1908 - val_accuracy: 0.9375 - lr: 2.5000e-05\n","Epoch 28/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0194 - accuracy: 0.9948 - val_loss: 0.0713 - val_accuracy: 0.9896 - lr: 2.5000e-05\n","Epoch 29/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0141 - accuracy: 0.9974 - val_loss: 0.2086 - val_accuracy: 0.9375 - lr: 2.5000e-05\n","Epoch 30/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9897\n","Epoch 30: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n","25/25 [==============================] - 30s 1s/step - loss: 0.0218 - accuracy: 0.9897 - val_loss: 0.0979 - val_accuracy: 0.9688 - lr: 2.5000e-05\n","Epoch 31/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0259 - accuracy: 0.9948 - val_loss: 0.0471 - val_accuracy: 0.9792 - lr: 1.2500e-05\n","Epoch 32/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.0127 - accuracy: 0.9974 - val_loss: 0.0883 - val_accuracy: 0.9688 - lr: 1.2500e-05\n","Epoch 33/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0086 - accuracy: 0.9948 - val_loss: 0.1188 - val_accuracy: 0.9583 - lr: 1.2500e-05\n","Epoch 34/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0099 - accuracy: 0.9974 - val_loss: 0.0467 - val_accuracy: 0.9792 - lr: 1.2500e-05\n","Epoch 35/1000\n","25/25 [==============================] - 29s 1s/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.0671 - val_accuracy: 0.9792 - lr: 1.2500e-05\n","Epoch 36/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0592 - val_accuracy: 0.9583 - lr: 1.2500e-05\n","Epoch 37/1000\n","25/25 [==============================] - 31s 1s/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1062 - val_accuracy: 0.9688 - lr: 1.2500e-05\n","Epoch 38/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9974\n","Epoch 38: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n","25/25 [==============================] - 29s 1s/step - loss: 0.0153 - accuracy: 0.9974 - val_loss: 0.1319 - val_accuracy: 0.9583 - lr: 1.2500e-05\n","Epoch 39/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0218 - accuracy: 0.9871 - val_loss: 0.0146 - val_accuracy: 1.0000 - lr: 6.2500e-06\n","Epoch 40/1000\n","25/25 [==============================] - 35s 1s/step - loss: 0.0137 - accuracy: 0.9948 - val_loss: 0.0399 - val_accuracy: 0.9792 - lr: 6.2500e-06\n","Epoch 41/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9583 - lr: 6.2500e-06\n","Epoch 42/1000\n","25/25 [==============================] - 35s 1s/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1347 - val_accuracy: 0.9688 - lr: 6.2500e-06\n","Epoch 43/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n","Epoch 43: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n","25/25 [==============================] - 30s 1s/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 0.9792 - lr: 6.2500e-06\n","Epoch 44/1000\n","25/25 [==============================] - 31s 1s/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 0.0687 - val_accuracy: 0.9688 - lr: 3.1250e-06\n","Epoch 45/1000\n","25/25 [==============================] - 30s 1s/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.0729 - val_accuracy: 0.9792 - lr: 3.1250e-06\n","Epoch 46/1000\n","25/25 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9948Restoring model weights from the end of the best epoch: 39.\n","25/25 [==============================] - 30s 1s/step - loss: 0.0105 - accuracy: 0.9948 - val_loss: 0.0866 - val_accuracy: 0.9792 - lr: 3.1250e-06\n","Epoch 46: early stopping\n"]}]},{"cell_type":"markdown","source":["#### 4) 성능 평가"],"metadata":{"id":"BbhiWcS5i735"}},{"cell_type":"code","source":["# from tensorflow.keras.preprocessing import image_dataset_from_directory\n","# test_ds = image_dataset_from_directory(\n","#     directory=test_path,\n","#     labels='inferred',\n","#     label_mode='categorical',\n","#     batch_size=32,\n","#     image_size=(280,280)\n","# )"],"metadata":{"id":"6OfXTAd4tPRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nA5yKqHutR40"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik4AFzCQi735","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679467475790,"user_tz":-540,"elapsed":12301,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"5134141a-11a2-40d6-ebe0-c3fb6a7a742e"},"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 8s 934ms/step - loss: 0.0605 - accuracy: 0.9752\n","Test Loss : 0.060507,  Test Accuracy : 97.521%\n","4/4 [==============================] - 4s 211ms/step\n"]}],"source":["\n","performance_test = model.evaluate(test_generator,  batch_size=100)\n","\n","print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))\n","\n","# for i in range(len(y_pred)):\n","#     if y_pred[i] >= 0.5:\n","#         y_pred[i] = 1\n","#     else:\n","#         y_pred[i] = 0\n","# print(classification_report(single_y_test,single_y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkkSsyMoi735","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679467496830,"user_tz":-540,"elapsed":9073,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"30a764b2-ba7c-4d41-bb9e-cc63ffb568a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 7s 934ms/step\n"]}],"source":["y_pred = model.predict(test_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGuQMUJNxXSy"},"outputs":[],"source":["print(classification_report(test_generator, y_pred))"]},{"cell_type":"code","source":["test_generator.image_shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4G-z56p4eGH","executionInfo":{"status":"ok","timestamp":1679466949689,"user_tz":-540,"elapsed":1069,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"0cb49a31-b586-4252-c496-50fe4a475421"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(280, 280, 3)"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["y_pred.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwlv_44Y4eH_","executionInfo":{"status":"ok","timestamp":1679467298980,"user_tz":-540,"elapsed":533,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"b2d626ae-7bde-4467-9e02-38af8550bb86"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(121, 1)"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score"],"metadata":{"id":"ES-UhpRl4eMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_generator에 관한 전처리\n","y_pred = model.predict(test_generator)\n","y_pred = y_pred.flatten()\n","y_pred = np.where(y_pred > 0.5, 1, 0)\n","y_test = test_generator.classes\n","\n","print(classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhWGQtT_4ePK","executionInfo":{"status":"ok","timestamp":1679469021180,"user_tz":-540,"elapsed":8878,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"20a0c3b9-e60a-46d3-d1d6-d2a3cf97bca8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 7s 911ms/step\n","              precision    recall  f1-score   support\n","\n","           0       0.48      0.49      0.48        61\n","           1       0.47      0.45      0.46        60\n","\n","    accuracy                           0.47       121\n","   macro avg       0.47      0.47      0.47       121\n","weighted avg       0.47      0.47      0.47       121\n","\n"]}]},{"cell_type":"code","source":["print(classification_report(y_test,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"o1aEcOocCQhR","executionInfo":{"status":"error","timestamp":1679467263497,"user_tz":-540,"elapsed":739,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"04f0019b-9828-4b2b-d596-25dcc833621f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-221bda6bfc4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \"\"\"\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2310\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"]}]},{"cell_type":"code","source":[],"metadata":{"id":"aFvBw4nrCWzk"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}