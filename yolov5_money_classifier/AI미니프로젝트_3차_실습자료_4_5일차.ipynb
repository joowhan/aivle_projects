{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **저시력자를 위한 원화 화폐 분류**\n","---\n","- 본 과제는 UltraLytics YOLO v5 모델 사용을 권장합니다.\n","    - 본 파일의 목차는 UltraLytics YOLO v5에 맞게 작성되어 있습니다.\n","    - 다른 모델을 찾아서 사용하셔도 좋습니다.\n","    - 산출물이 잘 나오면 됩니다 : )\n","---"],"metadata":{"id":"XT7PRhnMf-kI"}},{"cell_type":"markdown","source":["## 0.미션\n","---\n","- **과제 수행 목표**\n","    - 본 과제는 Object Detection 문제입니다.\n","    - Object Detection 문제로 접근하기 위해 **데이터셋 전처리**를 하셔야 합니다.\n","    - 데이터셋 : money_dataset.zip\n","        1. 데이터셋은 압축 파일로 제공됩니다.\n","        2. 압축 파일 안에는 화폐마다 폴더가 개별적으로 존재합니다.\n","        3. 폴더 안에는 화폐 이미지와 화폐 정보가 담긴 json 파일이 있습니다.\n","    - 여러분이 직접 촬영한 화폐 사진들을 탐지 과정에서 이용 해보세요.\n","    - 이미지에 화폐 하나만 나오게 촬영하는 것은 지양해주세요.\n","    - 다양한 방법으로 화폐를 촬영하고 결과를 확인해보세요.\n","        - ex 1) 화폐의 모든 종류를 한 이미지에 나오게 촬영\n","        - ex 2) 여러 화폐를 겹치게 하여 촬영\n","---\n","- **Key Point**\n","    1. 모델에 맞는 폴더 구조 확인\n","    2. 이미지 축소 비율에 맞춰 좌표값 변경\n","        - 좌표를 이미지 리사이즈한 비율로 변경\n","    3. 모델에 맞는 정보 추출/형식 변경\n","        - json 파일에서 정보 추출 및 모델 형식에 맞게 변경\n","    4. 화폐당 하나의 클래스로 변경\n","        - 총 8개 클래스\n","    5. 모델 선택 필요\n","---"],"metadata":{"id":"47D2vGDYdCOz"}},{"cell_type":"markdown","source":["## 1.환경설정"],"metadata":{"id":"aZon1K-Ag9be"}},{"cell_type":"markdown","source":["### (1) 구글 드라이브 연동\n","---\n","- 아래의 코드 셀을 반드시 실행시켜야 합니다.\n","---"],"metadata":{"id":"CMgnHN9ZBF05"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"xCplyiojBFwh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679633830263,"user_tz":-540,"elapsed":29905,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"4bd1356a-276e-4a9f-e04c-fe94f07cb28d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","source":["### (2) 데이터셋 불러오기\n","---\n","- **세부요구사항**\n","    - 데이터셋 파일의 압축을 해제하세요.\n","---\n","- 예제 코드에서는 zipfile 모듈을 이용하였습니다.\n","    - [zipfile document](https://docs.python.org/3/library/zipfile.html#zipfile-objects)\n","    - 해당 모듈 이외에 자신이 잘 알고 있는 방법을 사용해도 됩니다.\n","---"],"metadata":{"id":"J8vjv0acBAV4"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"bkSa5ejf8LMe","executionInfo":{"status":"ok","timestamp":1679633836877,"user_tz":-540,"elapsed":1086,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"outputs":[],"source":["import zipfile"]},{"cell_type":"code","source":["# 데이터셋 압축 파일 경로 : 유저별로 상이할 수 있음\n","money_data = zipfile.ZipFile('/content/drive/MyDrive/Datasets/money_dataset.zip' )"],"metadata":{"id":"N4cdpkRv86QQ","executionInfo":{"status":"ok","timestamp":1679633839151,"user_tz":-540,"elapsed":1267,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 압축 해제\n","money_data.extractall('/content/dataset/')"],"metadata":{"id":"TDAyDRLT9hZS","executionInfo":{"status":"ok","timestamp":1679633850198,"user_tz":-540,"elapsed":11049,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## 2.데이터 전처리"],"metadata":{"id":"QyEd-WNIhoSc"}},{"cell_type":"markdown","source":["### (1) 폴더 구조 생성 및 파일 이동\n","---\n","- **세부요구사항**\n","    -  모델에서 요구하는 폴더 구조를 만들어야 합니다.\n","        - Hint : Image와 Label을 구분하는 폴더를 만들어 주세요\n","---\n","- 예제 코드에서는 glob, shutil 모듈을 이용하였습니다.\n","    - [glob document](https://docs.python.org/3/library/glob.html) | [shutil document](https://docs.python.org/3/library/shutil.html)\n","    - 해당 모듈 이외에 자신이 잘 알고 있는 방법을 사용해도 됩니다.\n","---"],"metadata":{"id":"P81d6utx-3LY"}},{"cell_type":"code","source":["# 1.폴더 구조 만들기\n","!mkdir /content/Dataset/\n","!mkdir /content/Dataset/images;\n","!mkdir /content/Dataset/images/train; mkdir /content/Dataset/images/val\n","\n","!mkdir /content/Dataset/labels;\n","!mkdir /content/Dataset/labels/train; mkdir /content/Dataset/labels/val"],"metadata":{"id":"YBqCJU5z_UI8","executionInfo":{"status":"ok","timestamp":1679634770880,"user_tz":-540,"elapsed":676,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import glob, shutil\n","from sklearn.model_selection import train_test_split\n","import os"],"metadata":{"id":"UuchlNA_DftJ","executionInfo":{"status":"ok","timestamp":1679635287399,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 2. Dataset metadata 입력\n","won_list = ['10', '50', '100', '500', '1000', '5000', '10000', '50000']\n","data_path = '/content/dataset/'"],"metadata":{"id":"Q3lnYcLS_UOy","executionInfo":{"status":"ok","timestamp":1679634614218,"user_tz":-540,"elapsed":1361,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/dataset"],"metadata":{"id":"uQuudundUYf6","executionInfo":{"status":"ok","timestamp":1679633872283,"user_tz":-540,"elapsed":6,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c27c6bc1-ed30-4c69-c8eb-55e4022a0a8b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/dataset’: File exists\n"]}]},{"cell_type":"markdown","source":["---\n","- 데이터를 Training set | Validation set으로 분할하세요.\n","    - 예시 : Training과 Validation은 8:2로 분리\n","- Hint : 이미지 데이터는 /images에, JSON 데이터는 /labels에 넣어주세요\n","    - 예시 : /dataset/images/train, /dataset/labels/train\n","    - 예제 코드에서는 glob, shutil 모듈을 이용하였습니다.\n","    - [glob document](https://docs.python.org/3/library/glob.html) | [shutil document](https://docs.python.org/3/library/shutil.html)\n","\n","    ※ 해당 모듈 이외에 자신이 잘 알고 있는 방법을 사용해도 됩니다.\n","    \n","---"],"metadata":{"id":"ihJgeqXJG1Ml"}},{"cell_type":"code","source":["########################\n","# 이 셀부터 코드 작성하세요\n","########################\n","# 3. 데이터를 Training set | Validation set으로 분할하세요.\n","jpg_file = glob.glob(data_path + won_list[0] + '/*.jpg')\n","jpg_file[0]"],"metadata":{"id":"1qfGCSqy_kL0","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1679634655790,"user_tz":-540,"elapsed":839,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"8316bc40-49d8-4091-df14-149c845db111"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/dataset/10/10_631_9.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["def batch_move_files(lis, sor_path, dst_path, label_dst_path):\n","    for file in lis:\n","        image = file.split('/')[-1]+'.jpg'\n","        json = file.split('/')[-1]+'.json'\n","        shutil.copy(os.path.join(sor_path, image), dst_path)\n","        shutil.copy(os.path.join(sor_path, json), label_dst_path)"],"metadata":{"id":"Ao6f_2Ew_4l1","executionInfo":{"status":"ok","timestamp":1679635156873,"user_tz":-540,"elapsed":545,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["train_dir = '/content/Dataset/images/train/'\n","val_dir =  '/content/Dataset/images/val/'\n","\n","label_train_dir =  '/content/Dataset/labels/train/'\n","label_val_dir =  '/content/Dataset/labels/val/'"],"metadata":{"id":"4HgVlCcgCqNk","executionInfo":{"status":"ok","timestamp":1679635347695,"user_tz":-540,"elapsed":1597,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"hyMtlnKX_4DK"}},{"cell_type":"code","source":["for won in won_list:\n","    jpg_files = glob.glob(data_path + won + '/*.jpg')\n","    img = [name.replace(\".jpg\",\"\") for name in jpg_files]\n","    train_img, val_img = train_test_split(img, test_size=0.2, random_state=23, shuffle=True)\n","    sor_dir = '/content/dataset/'+won+'/'\n","    batch_move_files(train_img, sor_dir, train_dir, label_train_dir)\n","    batch_move_files(val_img, sor_dir, val_dir, label_val_dir)\n"],"metadata":{"id":"98H29m7MP1pY","executionInfo":{"status":"ok","timestamp":1679635356833,"user_tz":-540,"elapsed":4522,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["print(len(glob.glob(train_dir+'/*.jpg')))\n","print(len(glob.glob(val_dir+'/*.jpg')))\n","\n","print(len(glob.glob(label_train_dir+'/*.json')))\n","print(len(glob.glob(label_val_dir+'/*.json')))"],"metadata":{"id":"j2S49-d-SQ_b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679635641403,"user_tz":-540,"elapsed":1492,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"ec52d62f-9c10-4f32-cbbb-799ac43a6bd2"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["4172\n","1046\n","4172\n","1046\n"]}]},{"cell_type":"code","source":["# import os\n","# import glob\n","# import shutil\n","# from sklearn.model_selection import train_test_split\n","\n","# for won in won_list:\n","#   jpg_file = glob.glob(data_path + won + '/*.jpg')\n","#   num = len(jpg_file)\n","#   for i in range(num):\n","#     jpg_file[i] = jpg_file[i][len(data_path + won):]\n","#     if i < round(num * 0.8):\n","#       shutil.move(data_path + won + jpg_file[i], data_path + 'images/train/' + jpg_file[i])\n","#     else:\n","#       shutil.move(data_path + won + jpg_file[i], data_path + 'images/val/' + jpg_file[i])\n","\n","#   json_file = glob.glob(data_path + won + '/*.json')\n","#   num2 = len(json_file)\n","#   for j in range(num2):\n","#     json_file[j] = json_file[j][len(data_path + won):]\n","#     if j < round(num2 * 0.8):\n","#       shutil.move(data_path + won + json_file[j], data_path + 'labels/train/' + json_file[j])\n","#     else:\n","#       shutil.move(data_path + won + json_file[j], data_path + 'labels/val/' + json_file[j])"],"metadata":{"id":"47hhnorGSQ13"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# temp_path = '/content/dataset/'\n","# for won in won_list:\n","#     files = glob.glob(temp_path + won + '/*.jpg')\n","#    for file in  \n","# won ='10'\n","# import re\n","# files = glob.glob(temp_path+won+ '/*.jpg')\n","# jsons  = glob.glob(temp_path+won+ '/*.json')\n","# lis=[]\n","# for file in files:\n","#     name = file.split('.')\n","#     lis.append(name[0])\n","# for json in jsons:\n","#     j_name = json.split('.')\n","#     if j_name[0] not in lis:\n","#         print(j_name[0])\n"],"metadata":{"id":"vOfQW-k4mbwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) json에서 정보 추출\n","\n","1.   항목 추가\n","\n","1.   항목 추가\n","2.   항목 추가\n","\n","\n","2.   항목 추가\n","\n","\n","---\n","- **세부요구사항**\n","    - json 파일에서 필요한 정보를 추출하세요:\n","        - 위치 정보 : x1, x2, y1, y2\n","        - 박스 정보 : shape_type\n","        - 클래스 정보 : labels\n","    - 화폐당 하나의 클래스로 변경하세요.\n","        - json 파일에는 화폐 클래스가 앞뒷면으로 구분되어 있습니다.\n","        - 화폐의 앞뒷면 구분을 없애주세요.\n","            - 예시 : 'ten_front', 'ten_back' -> 'ten'\n","    - 화폐의 위치 정보를 YOLO 모델 형식에 맞게 변경 해주세요.\n","        - 사용되는 이미지는 원본에서 1/4로 축소되어 있습니다.\n","        - json 파일의 정보는 원본 기준 데이터이므로 위치 정보 추출을 할 때 x값과 y값을 1/4로 줄여주세요.\n","    - 이렇게 변경된 정보를 YOLO label 형식에 맞게 txt파일로 저장 해 주세요.\n","        - Hint : YOLO Labeling Format [label, x-center, y-center, width-norm, height-norm]\n","---"],"metadata":{"id":"II_hsJ6bKYGn"}},{"cell_type":"code","source":["import os, json"],"metadata":{"id":"MgUoCewjM-Jf","executionInfo":{"status":"ok","timestamp":1679635837760,"user_tz":-540,"elapsed":738,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["json_path = '/content/Dataset/labels/'\n","temp_list = ['train', 'val']"],"metadata":{"id":"gBD1Zv9BKaxi","executionInfo":{"status":"ok","timestamp":1679635840149,"user_tz":-540,"elapsed":1104,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["########################\n","# 이 셀부터 코드 작성하세요\n","# Json 파일에서 필요한 정보만 골라 txt로 바꾸는 작업임을 기억하세요!\n","########################\n","\n","# files =  os.listdir('/content/Dataset/labels/train/')\n","# files[:100]\n","won_dict = {0:'10', 1:'50', 2:'100', 3:'500', 4:'1000', 5:'5000', 6:'10000', 7:'50000'}\n","for folder in temp_list:\n","    path = json_path+folder+'/'\n","    files = os.listdir(path)\n","    for json_file in files:\n","        file = open(path+json_file)\n","        data = json.load(file)\n","        arr = data['shapes']\n","        for a in arr:\n","            xy = a['points']\n","        \n","        x_center = ((xy[0][0] + xy[1][0])/2)/5\n","        y_center = ((xy[0][1] + xy[1][1])/2)/5\n","        width_norm = (xy[1][0] - xy[0][0])/5\n","        height_norm = (xy[1][1] - xy[0][1])/5\n","        x_center, y_center, width_norm, height_norm =  x_center/(data['imageWidth']/5),  y_center/(data['imageHeight']/5), width_norm/(data['imageWidth']/5), height_norm/(data['imageHeight']/5)\n","        m = data['imagePath'].split('_')[0]\n","        for key,value in won_dict.items():\n","            if value == m:\n","                label = str(key)\n","        f_name =data['imagePath'].split('.')\n","        txt_name = '.'.join(f_name[:-1])+'.txt'\n","        lis=[]\n","        lis.append(label)\n","        lis.append(str(x_center))\n","        lis.append(str(y_center))\n","        lis.append(str(width_norm))\n","        lis.append(str(height_norm))\n","        result = ' '.join(lis)\n","        f = open(path+txt_name, 'w')\n","        f.write(result)\n","        f.close()\n","        os.remove(path+json_file)\n"],"metadata":{"id":"Mzh2Y8doMEK1","executionInfo":{"status":"ok","timestamp":1679635847582,"user_tz":-540,"elapsed":1576,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# path = json_path+temp_list[0]\n","# lis = os.listdir(path)\n","# file = open(path+'/'+lis[0])\n","# data = json.load(file)\n","# arr = data['shapes']\n","# for a in arr:\n","#     xy =  a['points']\n","\n","# x_center = ((xy[0][0] + xy[1][0])/2)/5\n","# y_center = ((xy[0][1] + xy[1][1])/2)/5\n","# width_norm = abs((xy[1][0] - xy[0][0]))/5\n","# height_norm = abs((xy[1][1] - xy[0][1]))/5"],"metadata":{"id":"6dn6pmyCeIQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x_center, y_center,width_norm, height_norm"],"metadata":{"id":"VYjKhdrJeIOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x_center/(data['imageWidth']/5), width_norm/(data['imageWidth']/5), y_center/(data['imageHeight']/5), height_norm/(data['imageHeight']/5)"],"metadata":{"id":"GE1CHE92eIK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# m = data['imagePath'].split('_')[0]"],"metadata":{"id":"inAl1by6eICG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# won_dict = {0:'10', 1:'50', 2:'100', 3:'500', 4:'1000', 5:'5000', 6:'10000', 7:'50000'}\n","# for key,value in won_dict.items():\n","#     if value == m:\n","#         label =str(key)\n","# m =data['imagePath'].split('.')\n","# '.'.join(m[:-1])+'.txt'\n","# label"],"metadata":{"id":"c3ROINN1CsPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # str(label), str(x_center),str(y_center), str(width_norm), str(height_norm)\n","# lis=[]\n","# lis.append(label)\n","# lis.append(str(x_center))\n","# lis.append(str(y_center))\n","# lis.append(str(width_norm))\n","# lis.append(str(height_norm))\n","# result = ' '.join(lis)"],"metadata":{"id":"5wGOgT7YL9gR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# files =  os.listdir('/content/Dataset/labels/train/')\n","# files[:100]"],"metadata":{"id":"OoOdsHVQQUKT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tOQeEhApesWR"},"source":["### (3) 데이터셋 정보가 담긴 파일 생성\n","---\n","- **세부요구사항**\n","    - 파일 안에 있어야 할 정보는 아래와 같습니다.\n","        - 학습할 클래스 이름 정보\n","        - 학습할 클래스 수 정보\n","        - Training, Validation 데이터셋 위치 정보\n","---\n","- 가장 대중적으로 이용하는 라이브러리는 yaml 입니다.\n","    - [yaml document](https://pyyaml.org/wiki/PyYAMLDocumentation)\n","    - 해당 모듈 이외에 자신이 잘 알고 있는 방법을 사용해도 됩니다.\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pu1iQfQolBhJ"},"outputs":[],"source":["import yaml"]},{"cell_type":"code","source":["won_dict = {0:'10', 1:'50', 2:'100', 3:'500', 4:'1000', 5:'5000', 6:'10000', 7:'50000'}"],"metadata":{"id":"t1_uOeXcSvv3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvMQcHirmSnD"},"outputs":[],"source":["########################\n","# 이 셀부터 코드 작성하세요\n","########################\n","\n","#with open('/content/Dataset/money.yaml', 'w') as f :\n","\n","    "]},{"cell_type":"markdown","source":["## 3.모델링"],"metadata":{"id":"3btFvySXi2dt"}},{"cell_type":"markdown","metadata":{"id":"0pQ2gRbTYgLL"},"source":["### (1) 모델 라이브러리 설치\n","---"]},{"cell_type":"code","source":["!pip install jedi"],"metadata":{"id":"73a1l-ZQuHyF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679635934858,"user_tz":-540,"elapsed":7380,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"30f58555-21fd-4703-e45f-372dd58a2cf5"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jedi\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi) (0.8.3)\n","Installing collected packages: jedi\n","Successfully installed jedi-0.18.2\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/ultralytics/yolov5"],"metadata":{"id":"Biyr9AHkMyNf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679635955076,"user_tz":-540,"elapsed":4967,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"b720af52-73ed-443d-8c0e-b329d1f49436"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15338, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 15338 (delta 0), reused 2 (delta 0), pack-reused 15335\u001b[K\n","Receiving objects: 100% (15338/15338), 14.21 MiB | 8.73 MiB/s, done.\n","Resolving deltas: 100% (10520/10520), done.\n"]}]},{"cell_type":"code","source":["## yolov5 폴더 requirements.txt 수정 필요\n","## setuptools<=64.0.2\n","\n","temp_str = 'setuptools<=64.0.2\\n'\n","\n","f = open('/content/yolov5/requirements.txt', 'r')\n","f_str = f.readlines()\n","f.close()\n","\n","f2 = open('/content/yolov5/requirements.txt', 'w')\n","\n","for idx, val in enumerate(f_str) :\n","    if 'setuptools' in val :\n","        idx_v = idx\n","        f_str.remove(val)\n","        f_str.insert(idx_v, temp_str)\n","\n","for val in f_str :\n","    f2.write(val)\n","\n","f2.close() "],"metadata":{"id":"W3JjyVOpg26s","executionInfo":{"status":"ok","timestamp":1679635961074,"user_tz":-540,"elapsed":1693,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","execution_count":38,"metadata":{"id":"6xD6tBTdMyNg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679635976643,"user_tz":-540,"elapsed":10368,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"cd1c482b-c6e1-4153-ca38-b85fa4209f2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython>=3.1.30\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 9)) (8.4.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (5.9.4)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 12)) (2.27.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 13)) (1.10.1)\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 15)) (1.13.1+cu116)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 16)) (0.14.1+cu116)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 17)) (4.65.0)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 21)) (2.11.2)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 26)) (1.4.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 27)) (0.12.2)\n","Collecting setuptools<=64.0.2\n","  Downloading setuptools-64.0.2-py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (3.0.9)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (1.0.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (23.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (1.4.4)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (5.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (4.39.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2022.12.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->-r requirements.txt (line 15)) (4.5.0)\n","Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.19.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.16.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.8.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.51.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.4.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.6.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.40.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.4.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.2.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 26)) (2022.7.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.16.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.3.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->-r requirements.txt (line 6)) (3.15.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (6.1.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.1.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.2.2)\n","Installing collected packages: smmap, setuptools, thop, gitdb, gitpython\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.6.0\n","    Uninstalling setuptools-67.6.0:\n","      Successfully uninstalled setuptools-67.6.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 64.0.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gitdb-4.0.10 gitpython-3.1.31 setuptools-64.0.2 smmap-5.0.0 thop-0.1.1.post2209072238\n"]}],"source":["!cd yolov5; pip install -r requirements.txt"]},{"cell_type":"markdown","source":["### (2) 가중치 파일 다운로드\n","---\n","- **세부요구사항**\n","    - 모델 개발자가 제공하는 사전 학습 가중치 파일을 다운로드 하세요.\n","        - 해당 과정이 불필요하다면 넘어가셔도 됩니다!\n","---"],"metadata":{"id":"_mHMAspjR6Xp"}},{"cell_type":"code","source":["########################\n","# 이 셀부터 코드 작성하세요\n","########################\n","yaml_path = '/content/Dataset/money.yaml'"],"metadata":{"id":"sSVIqkMLDIOd","executionInfo":{"status":"ok","timestamp":1679635986620,"user_tz":-540,"elapsed":485,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/yolov5/pretrained"],"metadata":{"id":"V8wI1xHkXFrV","executionInfo":{"status":"ok","timestamp":1679635987396,"user_tz":-540,"elapsed":2,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["!wget -O /content/yolov5/pretrained/yolov5n.pt https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGDqVEa6XJ2K","executionInfo":{"status":"ok","timestamp":1679635994825,"user_tz":-540,"elapsed":1620,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"9ca0ef76-10e0-439a-f4c2-2474978eb0cd"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-03-24 05:33:13--  https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/9e501477-46e9-4b14-97d9-0ef1ad7b3f3f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230324T053313Z&X-Amz-Expires=300&X-Amz-Signature=f64afda3dba192d9d6b73d2b534bde9c5d754c35bdf9360ccd5668338cb1571e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5n.pt&response-content-type=application%2Foctet-stream [following]\n","--2023-03-24 05:33:13--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/9e501477-46e9-4b14-97d9-0ef1ad7b3f3f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230324T053313Z&X-Amz-Expires=300&X-Amz-Signature=f64afda3dba192d9d6b73d2b534bde9c5d754c35bdf9360ccd5668338cb1571e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5n.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4062133 (3.9M) [application/octet-stream]\n","Saving to: ‘/content/yolov5/pretrained/yolov5n.pt’\n","\n","/content/yolov5/pre 100%[===================>]   3.87M  --.-KB/s    in 0.1s    \n","\n","2023-03-24 05:33:14 (38.8 MB/s) - ‘/content/yolov5/pretrained/yolov5n.pt’ saved [4062133/4062133]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"W8-5lC4mfbwT"},"source":["### (3) 학습 : train.py\n","---\n","- **세부요구사항**\n","    - UltraLytics YOLO v5에는 아래의 데이터가 필요합니다.\n","        - 데이터셋 정보가 담긴 yaml 파일\n","        - 사용하려는 모델 구조에 대한 yaml 파일\n","        - 사용하려는 모델의 가중치 파일\n","---"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"4AYFDMaVfmTK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679639636861,"user_tz":-540,"elapsed":3632320,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"55d361b5-8225-4db8-8180-e48bead18eb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov5/pretrained/yolov5n.pt, cfg=/content/yolov5/models/yolov5n.yaml, data=/content/Dataset/money.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=trained, name=train_weight, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=3, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v7.0-128-gb96f35c Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir trained', view at http://localhost:6006/\n","2023-03-24 05:33:30.260548: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-24 05:33:31.184276: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-24 05:33:31.184431: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-24 05:33:31.184451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 21.4MB/s]\n","Overriding model.yaml nc=80 with nc=8\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n","  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n","  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n","  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n","  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n","  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n","  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n"," 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n"," 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 24      [17, 20, 23]  1     17589  models.yolo.Detect                      [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n","YOLOv5n summary: 214 layers, 1774741 parameters, 1774741 gradients, 4.3 GFLOPs\n","\n","Transferred 342/349 items from /content/yolov5/pretrained/yolov5n.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Dataset/labels/train... 4172 images, 0 backgrounds, 0 corrupt: 100% 4172/4172 [00:02<00:00, 1990.35it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Dataset/labels/train.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Dataset/labels/val... 1046 images, 0 backgrounds, 0 corrupt: 100% 1046/1046 [00:01<00:00, 823.89it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Dataset/labels/val.cache\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.00 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to trained/train_weight/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mtrained/train_weight\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/99      1.87G    0.06328    0.02359    0.05551         28        640: 100% 261/261 [02:52<00:00,  1.51it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:19<00:00,  1.73it/s]\n","                   all       1046       1046      0.238       0.62      0.368      0.211\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/99      2.68G    0.04074    0.01272    0.04253         24        640: 100% 261/261 [02:46<00:00,  1.57it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.79it/s]\n","                   all       1046       1046       0.49      0.778      0.615      0.348\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/99      2.68G    0.03837    0.01033    0.02946         23        640: 100% 261/261 [02:47<00:00,  1.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.75it/s]\n","                   all       1046       1046       0.64      0.882      0.721       0.49\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/99      2.68G    0.03251   0.009224    0.02523         24        640: 100% 261/261 [02:47<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:19<00:00,  1.72it/s]\n","                   all       1046       1046       0.47      0.784      0.561      0.381\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/99      2.68G    0.02802   0.008413    0.02317         23        640: 100% 261/261 [02:49<00:00,  1.54it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.82it/s]\n","                   all       1046       1046      0.651      0.918      0.758      0.588\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/99      2.68G    0.02516   0.007699    0.02111         35        640: 100% 261/261 [02:49<00:00,  1.54it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.81it/s]\n","                   all       1046       1046      0.657      0.912      0.746      0.632\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/99      2.68G    0.02297   0.007355    0.02107         26        640: 100% 261/261 [02:48<00:00,  1.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:21<00:00,  1.56it/s]\n","                   all       1046       1046       0.72      0.968       0.78      0.655\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/99      2.68G    0.02192   0.006924    0.02005         29        640: 100% 261/261 [02:51<00:00,  1.53it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.76it/s]\n","                   all       1046       1046      0.677      0.889      0.777      0.649\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/99      2.68G    0.02025   0.006629    0.01852         29        640: 100% 261/261 [02:49<00:00,  1.54it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:19<00:00,  1.69it/s]\n","                   all       1046       1046      0.718      0.859      0.846       0.76\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/99      2.68G       0.02   0.006446    0.01791         25        640: 100% 261/261 [02:50<00:00,  1.53it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.75it/s]\n","                   all       1046       1046      0.743      0.871      0.862      0.757\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/99      2.68G    0.01904   0.006411     0.0167         17        640: 100% 261/261 [02:47<00:00,  1.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.78it/s]\n","                   all       1046       1046      0.761      0.903      0.882      0.787\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      11/99      2.68G    0.01861   0.006212    0.01523         28        640: 100% 261/261 [02:47<00:00,  1.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.78it/s]\n","                   all       1046       1046      0.762      0.851      0.863      0.774\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      12/99      2.68G    0.01806   0.006234    0.01406         29        640: 100% 261/261 [02:46<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:19<00:00,  1.72it/s]\n","                   all       1046       1046      0.893      0.897      0.936      0.837\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      13/99      2.68G    0.01785   0.006128    0.01389         27        640: 100% 261/261 [02:44<00:00,  1.59it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:19<00:00,  1.68it/s]\n","                   all       1046       1046      0.899       0.93      0.961      0.859\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      14/99      2.68G    0.01702   0.006021    0.01291         28        640: 100% 261/261 [02:46<00:00,  1.57it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:19<00:00,  1.74it/s]\n","                   all       1046       1046      0.947      0.905      0.968      0.873\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      15/99      2.68G    0.01659   0.005808    0.01219         22        640: 100% 261/261 [02:47<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.82it/s]\n","                   all       1046       1046      0.897      0.932      0.965      0.883\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      16/99      2.68G    0.01671   0.005819    0.01145         22        640: 100% 261/261 [02:46<00:00,  1.57it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.82it/s]\n","                   all       1046       1046      0.886      0.863      0.952       0.88\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      17/99      2.68G    0.01605   0.005708    0.01101         26        640: 100% 261/261 [02:47<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.82it/s]\n","                   all       1046       1046      0.923      0.927      0.969      0.881\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      18/99      2.68G    0.01571   0.005674    0.01095         19        640: 100% 261/261 [02:47<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:18<00:00,  1.83it/s]\n","                   all       1046       1046      0.912      0.915      0.963      0.879\n","Stopping training early as no improvement observed in last 3 epochs. Best results observed at epoch 15, best model saved as best.pt.\n","To update EarlyStopping(patience=3) pass a new patience value, i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.\n","\n","19 epochs completed in 0.991 hours.\n","Optimizer stripped from trained/train_weight/weights/last.pt, 3.8MB\n","Optimizer stripped from trained/train_weight/weights/best.pt, 3.8MB\n","\n","Validating trained/train_weight/weights/best.pt...\n","Fusing layers... \n","YOLOv5n summary: 157 layers, 1769989 parameters, 0 gradients, 4.2 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 33/33 [00:20<00:00,  1.59it/s]\n","                   all       1046       1046      0.897      0.931      0.965      0.883\n","                     0       1046         88      0.973      0.977      0.987      0.897\n","                     1       1046         88       0.73      0.966      0.953      0.854\n","                     2       1046         88      0.908      0.561      0.872       0.77\n","                     3       1046         88      0.617      0.966      0.928      0.866\n","                     4       1046        172       0.99      0.994      0.995      0.922\n","                     5       1046        174          1      0.988      0.995       0.93\n","                     6       1046        174      0.958          1      0.995      0.925\n","                     7       1046        174          1      0.999      0.995      0.902\n","Results saved to \u001b[1mtrained/train_weight\u001b[0m\n"]}],"source":["########################\n","# 이 셀부터 코드 작성하세요\n","########################\n","!cd yolov5; python train.py \\\n","    --data '/content/Dataset/money.yaml' \\\n","    --cfg '/content/yolov5/models/yolov5n.yaml' \\\n","    --weights '/content/yolov5/pretrained/yolov5n.pt' \\\n","    --epochs 100 \\\n","    --patience 3 \\\n","    --img 640 \\\n","    --project 'trained' \\\n","    --name 'train_weight' \\\n","    --exist-ok\n","    # --device cpu"]},{"cell_type":"markdown","metadata":{"id":"u2YESAa5fc4M"},"source":["## 4.탐지 : detect.py\n","---\n","- **세부요구사항**\n","    - 학습 과정에서 생성된 가중치 파일을 이용하세요.\n","    - IoU threshold를 0.25 이하로 설정하세요.\n","    - confidence threshold를 0.75 이상으로 설정하세요.\n","---\n","- 여러분이 **직접 촬영한 화폐 사진과 동영상**을 탐지 과정에 이용하여 결과를 확인하세요.\n","    - 조건\n","        1. 화폐의 수를 늘려가며 촬영 해보세요.\n","            - ex) 50원 하나, 50원 둘, 50원 셋, ...\n","        2. 화폐의 종류를 늘려가며 촬영 해보세요.\n","            - ex) 50원 하나와 100원 하나, 50원 하나와 100원 하나와 1000원 하나, ...\n","        3. 사진은 최소 30장 이상, 동영상은 최소 하나 이상 촬영하여 사용 해보세요.\n","---"]},{"cell_type":"code","source":["########################\n","# 이 셀부터 코드 작성하세요\n","########################\n","#!mkdir /content/yolov5/data/images"],"metadata":{"id":"9rK0ClfTcjEZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679622258612,"user_tz":-540,"elapsed":650,"user":{"displayName":"김주환","userId":"07015590008728781094"}},"outputId":"fc526d6a-5ae4-4a06-9dff-62788045d70c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/yolov5/data/images’: File exists\n"]}]},{"cell_type":"code","source":["!cd yolov5; python detect.py \\\n","    --weights '/content/yolov5/trained/train_weight/weights/best.pt' \\\n","    --source '/content/yolov5/data/images/' \\\n","    --project '/content/yolov5/detected' \\\n","    --name 'images' \\\n","    --img 640 \\\n","    --conf-thres 0.75 \\\n","    --iou-thres 0.25 \\\n","    --line-thickness 3 \\\n","    --exist-ok \n","    # --device CPU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6O1VA6QSbtn","executionInfo":{"status":"ok","timestamp":1679639947357,"user_tz":-540,"elapsed":29009,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"aa2a12a9-8388-40c3-b795-6bf39615a6dd"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/trained/train_weight/weights/best.pt'], source=/content/yolov5/data/images/, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.75, iou_thres=0.25, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/yolov5/detected, name=images, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 🚀 v7.0-128-gb96f35c Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","YOLOv5n summary: 157 layers, 1769989 parameters, 0 gradients, 4.2 GFLOPs\n","image 1/58 /content/yolov5/data/images/1123.JPG: 416x640 1 6, 16.8ms\n","image 2/58 /content/yolov5/data/images/IMG_5901.JPG: 640x480 1 1, 16.5ms\n","image 3/58 /content/yolov5/data/images/IMG_5911.JPG: 640x480 (no detections), 10.4ms\n","image 4/58 /content/yolov5/data/images/IMG_5917.JPG: 640x480 (no detections), 11.6ms\n","image 5/58 /content/yolov5/data/images/IMG_5937.JPG: 640x640 1 1, 15.0ms\n","image 6/58 /content/yolov5/data/images/IMG_5938.JPG: 640x640 1 1, 7.9ms\n","image 7/58 /content/yolov5/data/images/IMG_5939.JPG: 640x640 1 3, 8.1ms\n","image 8/58 /content/yolov5/data/images/IMG_5940.JPG: 640x640 2 1s, 8.9ms\n","image 9/58 /content/yolov5/data/images/IMG_5941.JPG: 640x640 (no detections), 7.9ms\n","image 10/58 /content/yolov5/data/images/IMG_5942.JPG: 640x640 2 6s, 7.8ms\n","image 11/58 /content/yolov5/data/images/IMG_5943.JPG: 640x640 (no detections), 8.4ms\n","image 12/58 /content/yolov5/data/images/IMG_5944.JPG: 640x640 (no detections), 8.1ms\n","image 13/58 /content/yolov5/data/images/IMG_5945.JPG: 640x640 (no detections), 7.9ms\n","image 14/58 /content/yolov5/data/images/IMG_5946.JPG: 480x640 1 6, 2 7s, 11.8ms\n","image 15/58 /content/yolov5/data/images/IMG_5948.JPG: 480x640 1 4, 1 6, 2 7s, 8.0ms\n","image 16/58 /content/yolov5/data/images/IMG_5950.JPG: 480x640 1 4, 1 6, 1 7, 8.0ms\n","image 17/58 /content/yolov5/data/images/IMG_5951.JPG: 480x640 1 5, 1 6, 1 7, 7.7ms\n","image 18/58 /content/yolov5/data/images/IMG_5952.JPG: 480x640 3 1s, 3 3s, 7.6ms\n","image 19/58 /content/yolov5/data/images/IMG_5953.JPG: 480x640 2 1s, 1 3, 7.8ms\n","image 20/58 /content/yolov5/data/images/IMG_5954.JPG: 480x640 2 1s, 7.7ms\n","image 21/58 /content/yolov5/data/images/IMG_5955.JPG: 480x640 3 1s, 8.9ms\n","image 22/58 /content/yolov5/data/images/IMG_5956.JPG: 480x640 6 1s, 7.5ms\n","image 23/58 /content/yolov5/data/images/IMG_5957.JPG: 480x640 1 4, 1 7, 8.1ms\n","image 24/58 /content/yolov5/data/images/IMG_5958.JPG: 480x640 2 4s, 7.8ms\n","image 25/58 /content/yolov5/data/images/IMG_5959.JPG: 480x640 2 4s, 9.1ms\n","image 26/58 /content/yolov5/data/images/IMG_5960.JPG: 640x480 (no detections), 9.1ms\n","image 27/58 /content/yolov5/data/images/IMG_5961.JPG: 640x480 1 4, 8.7ms\n","image 28/58 /content/yolov5/data/images/IMG_5962.JPG: 640x640 (no detections), 8.3ms\n","image 29/58 /content/yolov5/data/images/IMG_5963.JPG: 640x640 (no detections), 7.9ms\n","image 30/58 /content/yolov5/data/images/IMG_5964.JPG: 640x640 (no detections), 8.0ms\n","image 31/58 /content/yolov5/data/images/IMG_5965.JPG: 640x640 1 6, 11.3ms\n","image 32/58 /content/yolov5/data/images/IMG_5966.JPG: 640x640 1 6, 15.7ms\n","image 33/58 /content/yolov5/data/images/IMG_5967.JPG: 640x640 (no detections), 10.3ms\n","image 34/58 /content/yolov5/data/images/IMG_5968.JPG: 640x640 1 4, 16.4ms\n","image 35/58 /content/yolov5/data/images/IMG_5969.JPG: 480x640 (no detections), 12.2ms\n","image 36/58 /content/yolov5/data/images/IMG_5970.JPG: 640x480 (no detections), 14.1ms\n","image 37/58 /content/yolov5/data/images/IMG_5971.JPG: 640x480 (no detections), 11.1ms\n","image 38/58 /content/yolov5/data/images/IMG_5972.JPG: 480x640 (no detections), 10.9ms\n","image 39/58 /content/yolov5/data/images/IMG_5973.JPG: 480x640 (no detections), 10.3ms\n","image 40/58 /content/yolov5/data/images/IMG_5974.JPG: 480x640 (no detections), 10.2ms\n","image 41/58 /content/yolov5/data/images/IMG_5975.JPG: 480x640 1 7, 7.9ms\n","image 42/58 /content/yolov5/data/images/IMG_5976.JPG: 640x480 (no detections), 8.3ms\n","image 43/58 /content/yolov5/data/images/IMG_5977.JPG: 640x480 (no detections), 7.9ms\n","image 44/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721.jpg: 640x480 (no detections), 8.4ms\n","image 45/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_01.jpg: 640x480 (no detections), 7.6ms\n","image 46/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_02.jpg: 640x480 (no detections), 7.3ms\n","image 47/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_03.jpg: 640x480 (no detections), 7.4ms\n","image 48/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_04.jpg: 640x480 (no detections), 7.3ms\n","image 49/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_05.jpg: 640x480 (no detections), 7.3ms\n","image 50/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_06.jpg: 640x480 (no detections), 7.8ms\n","image 51/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_07.jpg: 640x480 (no detections), 7.3ms\n","image 52/58 /content/yolov5/data/images/KakaoTalk_20230324_110058721_08.jpg: 640x480 (no detections), 10.3ms\n","image 53/58 /content/yolov5/data/images/MicrosoftTeams-image (1).png: 480x640 1 4, 7.3ms\n","image 54/58 /content/yolov5/data/images/bus.jpg: 640x480 (no detections), 7.5ms\n","image 55/58 /content/yolov5/data/images/fake.jpg: 448x640 (no detections), 10.7ms\n","image 56/58 /content/yolov5/data/images/fake2.jpg: 448x640 1 6, 7.1ms\n","image 57/58 /content/yolov5/data/images/zidane.jpg: 384x640 (no detections), 10.5ms\n","image 58/58 /content/yolov5/data/images/천원1.jpg: 320x640 (no detections), 10.7ms\n","Speed: 0.6ms pre-process, 9.5ms inference, 0.7ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1m/content/yolov5/detected/images\u001b[0m\n"]}]},{"cell_type":"code","source":["!zip -r /content/detected_images.zip /content/yolov5/detected/images"],"metadata":{"id":"NWAG6EsATs9I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679640187011,"user_tz":-540,"elapsed":735,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"540dec3c-51e5-4139-ebfc-9794342b4cc0"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: zip -r /content/detected_images.zip /content/yolov5/detected/images: No such file or directory\n"]}]},{"cell_type":"code","source":["!ls /content/yolov5/detected/images/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZjrzAfLeWSY6","executionInfo":{"status":"ok","timestamp":1679640154942,"user_tz":-540,"elapsed":602,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"3b38ca9c-3b42-4838-a0d0-46870dcd5587"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":[" 1123.JPG       IMG_5951.JPG   IMG_5971.JPG\n"," 천원1.jpg      IMG_5952.JPG   IMG_5972.JPG\n"," bus.jpg        IMG_5953.JPG   IMG_5973.JPG\n"," fake2.jpg      IMG_5954.JPG   IMG_5974.JPG\n"," fake.jpg       IMG_5955.JPG   IMG_5975.JPG\n"," IMG_5901.JPG   IMG_5956.JPG   IMG_5976.JPG\n"," IMG_5911.JPG   IMG_5957.JPG   IMG_5977.JPG\n"," IMG_5917.JPG   IMG_5958.JPG   KakaoTalk_20230324_110058721_01.jpg\n"," IMG_5937.JPG   IMG_5959.JPG   KakaoTalk_20230324_110058721_02.jpg\n"," IMG_5938.JPG   IMG_5960.JPG   KakaoTalk_20230324_110058721_03.jpg\n"," IMG_5939.JPG   IMG_5961.JPG   KakaoTalk_20230324_110058721_04.jpg\n"," IMG_5940.JPG   IMG_5962.JPG   KakaoTalk_20230324_110058721_05.jpg\n"," IMG_5941.JPG   IMG_5963.JPG   KakaoTalk_20230324_110058721_06.jpg\n"," IMG_5942.JPG   IMG_5964.JPG   KakaoTalk_20230324_110058721_07.jpg\n"," IMG_5943.JPG   IMG_5965.JPG   KakaoTalk_20230324_110058721_08.jpg\n"," IMG_5944.JPG   IMG_5966.JPG   KakaoTalk_20230324_110058721.jpg\n"," IMG_5945.JPG   IMG_5967.JPG  'MicrosoftTeams-image (1).png'\n"," IMG_5946.JPG   IMG_5968.JPG   zidane.jpg\n"," IMG_5948.JPG   IMG_5969.JPG\n"," IMG_5950.JPG   IMG_5970.JPG\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GJldGg7PWsjs"},"execution_count":null,"outputs":[]}]}