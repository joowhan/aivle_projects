{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Aivle 스쿨 지원 질문, 답변 챗봇 만들기**\n","# 단계2 : 모델링"],"metadata":{"id":"3xIPZjFU5rjt"}},{"cell_type":"markdown","source":["## 0.미션"],"metadata":{"id":"-FPypzell2uc"}},{"cell_type":"markdown","source":["* 다음 세가지 챗봇을 만들고 비교해 봅시다.\n","* 챗봇1. Word2Vec 임베딩 벡터 기반 머신러닝 분류 모델링\n","    * Word2Vec 모델을 만들고 임베딩 벡터를 생성합니다.\n","    * 임베딩 벡터를 이용하여 intent를 분류하는 모델링을 수행합니다.\n","        * 이때, LightGBM을 추천하지만, 다른 알고리즘을 이용할수 있습니다.\n","    * 예측된 intent의 답변 중 임의의 하나를 선정하여 출력합니다.\n","* 챗봇2. 단계별 모델링1\n","    * 1단계 : type(일상대화 0, 에이블스쿨Q&A 1) 분류 모델 만들기\n","        * Embedding + LSTM 모델링\n","    * 2단계 : 사전학습된 Word2Vec 모델을 로딩하여 train의 임베딩벡터 저장\n","    * 코사인 유사도로 intent 찾아 답변 출력\n","        * 새로운 문장의 임베딩벡터와 train의 임베딩 벡터간의 코사인 유사도 계산\n","        * 가장 유사도가 높은 질문의 intent를 찾아 답변 출력하기\n","* 챗봇3. 단계별 모델링2\n","    * 1단계 : 챗봇2의 1단계 모델을 그대로 활용\n","    * 2단계 : FastText 모델 생성하여 train의 임베딩벡터 저장\n","    * 코사인 유사도로 intent 찾아 답변 출력\n","        * 새로운 문장의 임베딩벡터와 train의 임베딩 벡터간의 코사인 유사도 계산\n","        * 가장 유사도가 높은 질문의 intent를 찾아 답변 출력하기\n","\n","* 챗봇3개에 대해서 몇가지 질문을 입력하고 각각의 답변을 비교해 봅시다.\n"],"metadata":{"id":"AC6wpFtQtR5y"}},{"cell_type":"markdown","source":["## 1.환경준비"],"metadata":{"id":"pvBAaxSgrkt7"}},{"cell_type":"markdown","source":["### (1)라이브러리 설치"],"metadata":{"id":"AvBsTv3s-a3X"}},{"cell_type":"markdown","source":["#### 1) gensim 3.8.3 설치"],"metadata":{"id":"EdhloaEO-mFZ"}},{"cell_type":"code","source":["#gensim은 자연어 처리를 위한 오픈소스 라이브러리입니다. 토픽 모델링, 워드 임베딩 등 다양한 자연어 처리 기능을 제공\n","# 현재 4.x 버전이 최신이지만, 3.8.3 버전으로 진행\n","!pip install gensim==3.8.3"],"metadata":{"id":"Pkk8tlp2-Q3g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797920623,"user_tz":-540,"elapsed":4808,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"78b9a393-76a5-4cdf-cae7-6a2485aa9619"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.9/dist-packages (3.8.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (6.3.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (1.16.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (1.22.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim==3.8.3) (1.10.1)\n"]}]},{"cell_type":"markdown","source":["* [코랩] 위 라이브러리 설치후 런타임 재시작 필요!"],"metadata":{"id":"Yvo0_3so-dA8"}},{"cell_type":"markdown","source":["#### 2) 형태소 분석을 위한 라이브러리"],"metadata":{"id":"w9I84wAV-EQ7"}},{"cell_type":"code","source":["# mecab 설치를 위한 관련 패키지 설치\n","!apt-get install curl git\n","!apt-get install build-essential\n","!apt-get install cmake\n","!apt-get install g++\n","!apt-get install flex\n","!apt-get install bison\n","!apt-get install python-dev\n","!pip install cython\n","!pip install mecab-python"],"metadata":{"id":"eRgLSScn0QFG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797947237,"user_tz":-540,"elapsed":26626,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"4d27a359-f9e9-4a17-dac9-a69a4bab847b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","curl is already the newest version (7.68.0-1ubuntu2.18).\n","git is already the newest version (1:2.25.1-1ubuntu3.10).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","build-essential is already the newest version (12.8ubuntu1.1).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","g++ is already the newest version (4:9.3.0-1ubuntu2).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","flex is already the newest version (2.6.4-6.2).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","bison is already the newest version (2:3.5.1+dfsg-1).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Note, selecting 'python-dev-is-python2' instead of 'python-dev'\n","python-dev-is-python2 is already the newest version (2.7.17-4).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (0.29.34)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mecab-python in /usr/local/lib/python3.9/dist-packages (1.0.0)\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.9/dist-packages (from mecab-python) (1.0.6)\n"]}]},{"cell_type":"code","source":["# 형태소 기반 토크나이징 (Konlpy)\n","!python3 -m pip install konlpy\n","# mecab (ubuntu: linux, mac os 기준)\n","# 다른 os 설치 방법 및 자세한 내용은 다음 참고: https://konlpy.org/ko/latest/install/#id1\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"],"metadata":{"id":"7zwNEdPo3Rpm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797953048,"user_tz":-540,"elapsed":5824,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"b5bcd00b-5f8f-4e33-a146-1205d326cc46"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.9/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n","mecab-ko is already installed\n","mecab-ko-dic is already installed\n","mecab-python is already installed\n","Done.\n"]}]},{"cell_type":"markdown","source":["### (2) 라이브러리 불러오기"],"metadata":{"id":"FlRWJB2w6Ip6"}},{"cell_type":"markdown","source":["* 세부 요구사항\n","    - 기본적으로 필요한 라이브러리를 import 하도록 코드가 작성되어 있습니다.\n","    - 필요하다고 판단되는 라이브러리를 추가하세요."],"metadata":{"id":"-TRDCUpP6Ip6"}},{"cell_type":"code","metadata":{"id":"h1IYbPd_6Ip6","executionInfo":{"status":"ok","timestamp":1681797959484,"user_tz":-540,"elapsed":6448,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import joblib\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# 필요하다고 판단되는 라이브러리를 추가하세요.\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import * \n","\n","import tensorflow as tf\n","from keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n","from keras import Input, Model\n","from keras import optimizers\n","from keras.models import Sequential, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["* 형태소 분석을 위한 함수를 제공합니다."],"metadata":{"id":"ERab2qbnVloB"}},{"cell_type":"code","source":["from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n","\n","# 다양한 토크나이저를 사용할 수 있는 함수\n","def get_tokenizer(tokenizer_name):\n","    if tokenizer_name == \"komoran\":\n","        tokenizer = Komoran()\n","    elif tokenizer_name == \"okt\":\n","        tokenizer = Okt()\n","    elif tokenizer_name == \"mecab\":\n","        tokenizer = Mecab()\n","    elif tokenizer_name == \"hannanum\":\n","        tokenizer = Hannanum()\n","    else:\n","        # \"kkma\":\n","        tokenizer = Kkma()\n","        \n","    return tokenizer"],"metadata":{"id":"dGr3phdYVloC","executionInfo":{"status":"ok","timestamp":1681797959485,"user_tz":-540,"elapsed":15,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 형태소 분석을 수행하는 함수\n","\n","def tokenize(tokenizer_name, original_sent, nouns=False):\n","    # 미리 정의된 몇 가지 tokenizer 중 하나를 선택\n","    tokenizer = get_tokenizer(tokenizer_name)\n","\n","    # tokenizer를 이용하여 original_sent를 토큰화하여 tokenized_sent에 저장하고, 이를 반환합니다.\n","    sentence = original_sent.replace('\\n', '').strip()\n","    if nouns:       \n","        # tokenizer.nouns(sentence) -> 명사만 추출\n","        tokens = tokenizer.nouns(sentence)\n","    else:\n","        tokens = tokenizer.morphs(sentence)\n","    # tokenized_sent = ' '.join(tokens)\n","    return tokens\n","    # return tokenized_sent"],"metadata":{"id":"f1kGJuX6VloD","executionInfo":{"status":"ok","timestamp":1681797959485,"user_tz":-540,"elapsed":14,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### (3) 데이터 로딩\n","* 전처리 단계에서 생성한 데이터들을 로딩합니다.\n","    * train, test\n","    * 형태소분석 결과 데이터 : clean_train_questions, clean_test_questions"],"metadata":{"id":"wsLDv9tZc_i1"}},{"cell_type":"markdown","source":["* 구글 드라이브 연결"],"metadata":{"id":"Uc_kIeeJeDgi"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dd0SPbYdfhS9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797962066,"user_tz":-540,"elapsed":2595,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"1ea19401-cc22-487c-c01f-7c2b983c46a2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/aivle/miniProject/project6/'"],"metadata":{"id":"y5OIDazoeIN4","executionInfo":{"status":"ok","timestamp":1681797962066,"user_tz":-540,"elapsed":5,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["* 저장된 .pkl 파일들을 불러옵니다.\n","* 불러 온 후에는 shape를 확인해 봅시다."],"metadata":{"id":"GH3ApIzofYPb"}},{"cell_type":"code","source":["import joblib\n","train_data = joblib.load(path+'train.pkl')\n","test_data = joblib.load(path+'test.pkl')\n","clean_train_questions = joblib.load(path+'clean_train_questions.pkl')\n","clean_test_questions = joblib.load(path+'clean_test_questions.pkl')"],"metadata":{"id":"FT_JFnclfcQ4","executionInfo":{"status":"ok","timestamp":1681797962066,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vjQCIzsUsKJD","executionInfo":{"status":"ok","timestamp":1681797962066,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sY4abSWpW4lb","executionInfo":{"status":"ok","timestamp":1681797962066,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## 2.챗봇1"],"metadata":{"id":"mMQvj3AmIDEy"}},{"cell_type":"markdown","source":["* **상세요구사항**\n","    * Word2Vec을 활용한 LightGBM 모델링(intent 분류)\n","        * Word2Vec을 이용하여 임베딩벡터 생성하기\n","            * Word Embedding으로 문장벡터 구하기\n","        * 임베딩 벡터를 이용하여 ML기반 모델링 수행하기\n","            * LightGBM 권장(다른 알고리즘을 이용할수 있습니다.)\n","    * 챗봇 : 모델의 예측결과(intent)에 따라 답변하는 챗봇 만들기\n","        * 질문을 입력받아, 답변하는 함수 생성"],"metadata":{"id":"_x9L3XAfJPAh"}},{"cell_type":"markdown","source":["### (1) Word2Vec을 이용하여 임베딩벡터 생성하기\n","* 'mecab' 형태소 분석기를 이용하여 문장을 tokenize\n","    * Word2Vec 모델을 만들기 위해서 입력 데이터는 리스트 형태여야 합니다.\n","    * 그래서 다시 리스트로 저장되도록 토크나이즈 해 봅시다.\n","* Word Embedding으로 문장벡터를 생성합니다.\n","    * 먼저 Word2Vec 모델을 만들고, train의 질문들을 문장벡터로 만듭시다.\n"],"metadata":{"id":"1lQMnaY2SIKM"}},{"cell_type":"markdown","source":["#### 1) 'mecab' 형태소 분석기를 이용하여 문장을 tokenize"],"metadata":{"id":"XZtMR3HVRfCI"}},{"cell_type":"code","source":["train_data.drop('str_len',axis=1, inplace=True)"],"metadata":{"id":"FHwKdpLvJPc-","executionInfo":{"status":"ok","timestamp":1681797962067,"user_tz":-540,"elapsed":4,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["test_data.drop('str_len',axis=1, inplace=True)"],"metadata":{"id":"YLHDaEeKATKL","executionInfo":{"status":"ok","timestamp":1681797962067,"user_tz":-540,"elapsed":4,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# train_data['Q'] = train_data['Q'].apply(lambda x:tokenize('mecab',x))\n","# test_data['Q'] = test_data['Q'].apply(lambda x:tokenize('mecab',x))"],"metadata":{"id":"HQh_cYRT9pIv","executionInfo":{"status":"ok","timestamp":1681797962067,"user_tz":-540,"elapsed":4,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["train_token = []\n","for sentence in train_data['Q']:\n","    tokenized = tokenize('mecab', sentence)\n","    train_token.append(tokenized)"],"metadata":{"id":"fOTMPtmR-pSC","executionInfo":{"status":"ok","timestamp":1681797964831,"user_tz":-540,"elapsed":2767,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_token[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QWigXciDvkM","executionInfo":{"status":"ok","timestamp":1681797964832,"user_tz":-540,"elapsed":13,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"55ad6738-1d04-4b8e-b08e-649cc6e34e59"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['떨어뜨려서', '핸드폰', '액정', '나갔', '어'],\n"," ['액정', '나갔', '어'],\n"," ['핸드폰', '떨어뜨려서', '고장', '났', '나', '봐'],\n"," ['노트북', '키보드', '가', '안', '먹히', '네'],\n"," ['노트북', '을', '떨어뜨려서', '고장', '난', '것', '같', '아', '.']]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["test_token = []\n","for sentence in test_data['Q']:\n","    tokenized = tokenize('mecab', sentence)\n","    test_token.append(tokenized)"],"metadata":{"id":"fca_Iu9dBuXT","executionInfo":{"status":"ok","timestamp":1681797964832,"user_tz":-540,"elapsed":11,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["#### 2) Word Embedding으로 문장벡터 구하기\n","* Word2Vec\n","    * 위에서 저장한 입력 데이터를 사용하여 Word2Vec 모델이 생성\n","    * 모델은 size(단어 벡터의 차원), \n","    * window(컨텍스트 창의 크기), \n","    * max_vocab_size(고려할 최대 어휘 크기), \n","    * min_count(포함할 단어의 최소 빈도)와 같은 특정 하이퍼파라미터로 훈련됩니다.\n","    * sg : 사용할 훈련 알고리즘 - 1은 skip-gram, 0은 CBOW )"],"metadata":{"id":"ZsDPBh8Nhxy2"}},{"cell_type":"code","source":[],"metadata":{"id":"V6Y26nTvJ1si","executionInfo":{"status":"ok","timestamp":1681797964832,"user_tz":-540,"elapsed":11,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","# Word2Vec 모델 생성\n","wv_model = Word2Vec(sentences=train_token, size=100, window=5, min_count=2, sg=0, batch_words=8)"],"metadata":{"id":"s42Yr9cbJ3uf","executionInfo":{"status":"ok","timestamp":1681797965296,"user_tz":-540,"elapsed":475,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["* Word2Vec 모델로부터 데이터를 벡터화하기 위한 함수 생성"],"metadata":{"id":"PVSSgw6jz-RH"}},{"cell_type":"code","source":["# Word2Vec 모델로부터 하나의 문장을 벡터화 시키는 함수\n","def get_sent_embedding(model, embedding_size, tokenized_words):\n","    # 임베딩 벡터를 0으로 초기화\n","    feature_vec = np.zeros((embedding_size,), dtype='float32')\n","    # 단어 개수 초기화\n","    n_words = 0\n","    # 모델 단어 집합 생성\n","    index2word_set = set(model.wv.index2word)\n","    # 문장의 단어들을 하나씩 반복\n","    for word in tokenized_words:\n","        # 모델 단어 집합에 해당하는 단어일 경우에만\n","        if word in index2word_set:\n","            # 단어 개수 1 증가\n","            n_words += 1\n","            # 임베딩 벡터에 해당 단어의 벡터를 더함\n","            feature_vec = np.add(feature_vec, model[word])\n","    # 단어 개수가 0보다 큰 경우 벡터를 단어 개수로 나눠줌 (평균 임베딩 벡터 계산)\n","    if (n_words > 0):\n","        feature_vec = np.divide(feature_vec, n_words)\n","    return feature_vec"],"metadata":{"id":"vGKxMURH0L0R","executionInfo":{"status":"ok","timestamp":1681797965296,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 문장벡터 데이터 셋 만들기\n","def get_dataset(sentences, model, num_features):\n","    dataset = list()\n","\n","    # 각 문장을 벡터화해서 리스트에 저장\n","    for sent in sentences:\n","        dataset.append(get_sent_embedding(model, num_features, sent))\n","\n","    # 리스트를 numpy 배열로 변환하여 반환\n","    sent_embedding_vectors = np.stack(dataset)\n","    \n","    return sent_embedding_vectors"],"metadata":{"id":"sZS9j5lgKDGQ","executionInfo":{"status":"ok","timestamp":1681797965608,"user_tz":-540,"elapsed":314,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["* 이제 학습데이터의 Q를 Word2Vec 모델을 사용하여 벡터화 합니다."],"metadata":{"id":"DEjSR247a9iw"}},{"cell_type":"code","source":["# 학습 데이터의 문장들을 Word2Vec 모델을 사용하여 벡터화\n","train_data_vecs = get_dataset(train_token, wv_model, 100)\n"],"metadata":{"id":"bIoehvZna82t","executionInfo":{"status":"ok","timestamp":1681797965608,"user_tz":-540,"elapsed":4,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["len(train_data_vecs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sM2CtpaXLB5m","executionInfo":{"status":"ok","timestamp":1681797965609,"user_tz":-540,"elapsed":4,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"9188356e-67be-4335-fc0e-fc6bf54d27a0"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1107"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":[],"metadata":{"id":"bt0wW9ipMj6h","executionInfo":{"status":"ok","timestamp":1681797965609,"user_tz":-540,"elapsed":4,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["* 훈련된 Word2Vec 모델을 사용하여 문장 목록에 대한 문장 임베딩을 생성하고 이를 2차원 numpy 배열에 저장합니다. \n","* 그런 다음 이러한 임베딩을 다양한 기계 학습 모델의 입력 기능으로 사용할 수 있습니다"],"metadata":{"id":"l9XFPz28RmlE"}},{"cell_type":"markdown","source":["### (2) 분류 모델링\n","* 데이터 분할\n","    * x, y\n","        * x : 이전 단계에서 저장된 임베딩벡터(train_data_vecs)\n","        * y : intent 값들\n","    * train, val\n","        * train_test_split 활용\n","* 머신러닝 모델링\n","    * lightGBM, RandomForest 등을 활용하여 학습\n","    * 필요하다면 hyper parameter 튜닝을 시도해도 좋습니다.\n","* validation set으로 검증해 봅시다."],"metadata":{"id":"NOo2RwzWRr0c"}},{"cell_type":"code","source":["# X와 y 데이터 분리\n","x = train_data_vecs\n","y = train_data['intent']\n","\n","# Train-Test split\n","x_train,x_val, y_train,y_val = train_test_split(x,y, test_size=0.2)\n"],"metadata":{"id":"vvS1F0EoKTv-","executionInfo":{"status":"ok","timestamp":1681797965609,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["* 모델1"],"metadata":{"id":"qKuYKmQ2dbP1"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"MXDMP5gUbQsr","executionInfo":{"status":"ok","timestamp":1681797965609,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["model = RandomForestClassifier()"],"metadata":{"id":"CPPQ3lejdWwq","executionInfo":{"status":"ok","timestamp":1681797965609,"user_tz":-540,"elapsed":3,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["model.fit(x_train,y_train)"],"metadata":{"id":"RCWZOlIlBMAK","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1681797967636,"user_tz":-540,"elapsed":2030,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"0022630c-1bf3-46b5-902c-b132cf7c8149"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["* 모델2"],"metadata":{"id":"uq63HIFjdc8c"}},{"cell_type":"code","source":[],"metadata":{"id":"4GQPYXMncvkh","executionInfo":{"status":"ok","timestamp":1681797967637,"user_tz":-540,"elapsed":25,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PEXD7iWmDEem","executionInfo":{"status":"ok","timestamp":1681797967638,"user_tz":-540,"elapsed":25,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0ysrB0NyBMVD","executionInfo":{"status":"ok","timestamp":1681797967638,"user_tz":-540,"elapsed":25,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축"],"metadata":{"id":"NxeL02Q1TecY"}},{"cell_type":"markdown","source":["* **상세요구사항**\n","    * 챗봇 flow : input 질문 -> 분류 모델로 intent 예측 --> intent에 해당하는 답변 출력\n","        * 하나의 intent 에는 여러 답변이 있습니다. 이중 한가지를 랜덤하게 선택합니다."],"metadata":{"id":"thqkcWLsTiwc"}},{"cell_type":"markdown","source":["#### 1) 데이터 중 하나에 대해서 테스트"],"metadata":{"id":"GLHnFhw-3dWl"}},{"cell_type":"code","source":["train_data.loc[train_data['intent']==2,'A'].unique()"],"metadata":{"id":"8HcLTNykBQHh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797967639,"user_tz":-540,"elapsed":24,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"40d6facf-50a9-4a8c-8bf6-993e0b3a4f3d"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['가장 중요한 거예요.', '가장 중요한 목표네요.'], dtype=object)"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["\n","y_pred = model.predict(x_val)\n"],"metadata":{"id":"A55XDY5VBQE5","executionInfo":{"status":"ok","timestamp":1681797967639,"user_tz":-540,"elapsed":23,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# y_pred = model.predict(test_sen)"],"metadata":{"id":"8R-7o3Q9BQB3","executionInfo":{"status":"ok","timestamp":1681797967639,"user_tz":-540,"elapsed":23,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["y_pred[0]\n","y_val"],"metadata":{"id":"901OF6AvBP-u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797967639,"user_tz":-540,"elapsed":22,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"1ac1c182-c3c6-446d-c821-79e92bd05825"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["156      8\n","3        1\n","339     26\n","364     28\n","794     43\n","        ..\n","705     39\n","1098    53\n","742     40\n","562     32\n","384     29\n","Name: intent, Length: 222, dtype: int64"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":[],"metadata":{"id":"Yar1bTRTBP7N","executionInfo":{"status":"ok","timestamp":1681797967639,"user_tz":-540,"elapsed":20,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 챗봇 함수 만들기\n","* 테스트 코드를 바탕으로 질문을 받아 답변을 하는 함수를 생성합시다.\n","* 성능이 좋은 모델 사용. "],"metadata":{"id":"ZuF2udm93j7W"}},{"cell_type":"code","source":["def get_answer1(question): \n","\n","\n","\n","\n","    return "],"metadata":{"id":"o6YSzGDe3n0M","executionInfo":{"status":"ok","timestamp":1681797967640,"user_tz":-540,"elapsed":20,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oUi-QgPD4HnP","executionInfo":{"status":"ok","timestamp":1681797967640,"user_tz":-540,"elapsed":20,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"V1a2FMV5yqwi"}},{"cell_type":"code","source":[],"metadata":{"id":"B8nXCSHX92HI","executionInfo":{"status":"ok","timestamp":1681797967640,"user_tz":-540,"elapsed":20,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RWQ_Tv9tBdyO","executionInfo":{"status":"ok","timestamp":1681797967640,"user_tz":-540,"elapsed":20,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## 3.챗봇2\n","\n","* **세부요구사항**\n","    * 단계별 챗봇을 만들어 봅시다.\n","        * 1단계 : type을 0과 1로 분류하는 모델 생성(Embedding + LSTM 모델)\n","        * 2단계 : \n","            * 각 type에 맞게, 사전학습된 Word2Vec 모델을 사용하여 임베딩 벡터(train)를 만들고\n","        * 3단계 : 챗봇 만들기\n","            * input 문장과 train 임베딩 벡터와 코사인 유사도 계산\n","            * 가장 유사도가 높은 질문의 intent 찾아\n","            * 해당 intent의 답변 중 무작위로 하나를 선정하여 답변하기"],"metadata":{"id":"6FAB06MnP5qf"}},{"cell_type":"markdown","source":["### (1) 1단계 : type 분류 모델링(LSTM)\n","- LSTM"],"metadata":{"id":"XvAzrVuvVQT9"}},{"cell_type":"markdown","source":["#### 1) 데이터 준비\n","* 학습용 데이터를 만들어 봅시다.\n","    * 시작 데이터 : clean_train_questions, clean_test_questions\n","    * 각 토큰에 인덱스를 부여하는 토크나이저를 만들고 적용\n","        * from tensorflow.keras.preprocessing.text import Tokenizer 를 사용\n","    * 문장별 길이에 대한 분포를 확인하고 적절하게 정의."],"metadata":{"id":"VklTJM3-tuDQ"}},{"cell_type":"code","source":["# 각각의 토큰에 인덱스 부여하는 토크나이저 선언\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","max_words=35000\n","# .fit_on_tests 이용하여 토크나이저 만들기\n","tokenizer = Tokenizer(num_words=max_words, lower=False)\n","tokenizer.fit_on_texts(clean_train_questions)\n","word_dic = tokenizer.word_index\n","print(word_dic)"],"metadata":{"id":"TbpMdJT3t228","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797967640,"user_tz":-540,"elapsed":18,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"67a2c1bc-2340-4f98-aa07-801ed83b23c7"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["{'나요': 1, '이': 2, '있': 3, '는': 4, '가': 5, '하': 6, '교육': 7, '지원': 8, '을': 9, '에': 10, '되': 11, '수': 12, '할': 13, '어': 14, '도': 15, '한가요': 16, '가능': 17, '경우': 18, '은': 19, '고': 20, '어떻게': 21, '으로': 22, '면': 23, '를': 24, '나': 25, '중': 26, '로': 27, '없': 28, '서류': 29, '한': 30, '궁금': 31, '노트북': 32, '는데': 33, '해야': 34, '어떤': 35, '합니다': 36, '에서': 37, '졸업': 38, '과': 39, '진행': 40, '의': 41, '다': 42, '수강': 43, '시': 44, '싶': 45, '해': 46, '받': 47, '인': 48, '채용': 49, '과정': 50, '만': 51, '인가요': 52, '연계': 53, '지': 54, '다른': 55, '아': 56, '게': 57, '보': 58, '안': 59, '먹': 60, '주': 61, '대면': 62, '지역': 63, '했': 64, '제공': 65, '자': 66, '시간': 67, '제출': 68, '취업': 69, '증명서': 70, '합격': 71, '좋': 72, '공부': 73, '뭔가요': 74, '추가': 75, '거': 76, '같': 77, '들': 78, '기준': 79, '미취': 80, '업자': 81, '생': 82, '을까요': 83, '비대': 84, '것': 85, '너무': 86, '참여': 87, '일': 88, '와': 89, '해도': 90, '수료': 91, '프리랜서': 92, '재': 93, '검사': 94, '장소': 95, '기간': 96, '실업': 97, '급여': 98, 'KT': 99, '오늘': 100, '기': 101, '뭐': 102, '대학': 103, '테스트': 104, '네': 105, '불': 106, '적': 107, '혜택': 108, '겠': 109, '인데': 110, '후': 111, '핸드폰': 112, '내': 113, '야': 114, '죽': 115, '자격': 116, '아르바이트': 117, '면접': 118, '친구': 119, '무엇': 120, '았': 121, '해서': 122, '세요': 123, '신청': 124, '외': 125, '성': 126, '관련': 127, '사용': 128, '변경': 129, '미뤄졌': 130, '숙소': 131, '볼까': 132, '좀': 133, '이나': 134, '동일': 135, '복무': 136, '휴가': 137, '병행': 138, '대여': 139, '많이': 140, '알': 141, '대해': 142, '대학원생': 143, '필요': 144, '전': 145, '인적': 146, '미리': 147, '코딩': 148, '과목': 149, '수업': 150, '기숙사': 151, '컴퓨터': 152, '않': 153, '집': 154, '었': 155, '때': 156, '말': 157, '는지': 158, '알려': 159, '사업자': 160, '여부': 161, '괜찮': 162, '시험': 163, '국비': 164, '으면': 165, '재직': 166, '현재': 167, '군': 168, '사항': 169, '봐': 170, '년': 171, '비': 172, '뻔': 173, '생각': 174, '보다': 175, '어디': 176, '못': 177, '시작': 178, '데': 179, '대': 180, '대상': 181, '습니다': 182, '점': 183, '동안': 184, '수령': 185, '포기': 186, '오프라인': 187, '휴학': 188, '미국': 189, '따라갈': 190, '전공': 191, '출석': 192, '결제': 193, '코': 194, '한테': 195, '을까': 196, '많': 197, '원': 198, '다시': 199, '돈': 200, '어야지': 201, '대학원': 202, '4': 203, '제': 204, '나이': 205, '이후': 206, '자영업': 207, '수령자': 208, '불합격': 209, '프로그램': 210, '제도': 211, '적성': 212, '발급': 213, '기업': 214, '올해': 215, '줄': 216, '싫': 217, '해야지': 218, '할까': 219, '직장': 220, '생겼': 221, '저녁': 222, '놀': 223, '면서': 224, '술': 225, '롭': 226, '같이': 227, '따라': 228, '대학교': 229, '예정': 230, '절차': 231, '요소': 232, '방식': 233, '비전': 234, '공자': 235, '커리큘럼': 236, '길': 237, '이번': 238, '러': 239, '학원': 240, '라고': 241, '계속': 242, '소개팅': 243, '려고': 244, '선': 245, '몇': 246, '달': 247, '조건': 248, '보험': 249, '퇴사': 250, '이전': 251, '할까요': 252, '수강료': 253, '등록': 254, '중도': 255, '는가요': 256, '대체': 257, '성적': 258, '증빙': 259, '중요': 260, '지방': 261, '무료': 262, '사양': 263, '예정일': 264, '채': 265, '중점': 266, '비교': 267, '돼': 268, '살': 269, '잘': 270, '바람': 271, '막혀': 272, '얼': 273, '추워': 274, '자고': 275, '될까': 276, '애': 277, '아니': 278, '한다': 279, '랑': 280, '카드': 281, '사람': 282, '제한': 283, '고용': 284, '분류': 285, '근로자': 286, '별도': 287, '해제': 288, '최종': 289, '외국': 290, '따라가': 291, '결석': 292, '출석률': 293, '된': 294, '과제': 295, '서울': 296, '자유': 297, '및': 298, '용연': 299, '블': 300, '스쿨': 301, '차이점': 302, '터': 303, '아서': 304, '아파': 305, '부네': 306, '춥': 307, '될': 308, '2': 309, '왜': 310, '자꾸': 311, '화': 312, '생리통': 313, '갚': 314, '다이어트': 315, '진짜': 316, '치': 317, '메뉴': 318, '니': 319, '차': 320, '벌': 321, '선물': 322, '미세먼지': 323, '볼': 324, '엄마': 325, '왔': 326, '만나': 327, '이랑': 328, '용돈': 329, '다니': 330, '상': 331, '자세히': 332, '예정자': 333, '재학': 334, '세': 335, '늦': 336, '겹치': 337, '운영': 338, '으려면': 339, '방법': 340, '지원금': 341, '불합격자': 342, '차이': 343, '번': 344, '모두': 345, '온라인': 346, '얼마나': 347, '다면': 348, '내용': 349, '휴학생': 350, '대신': 351, '성적표': 352, '학력': 353, '국내': 354, '졸업장': 355, '여도': 356, '사전': 357, '개인': 358, '다른가요': 359, '위치': 360, '이루': 361, '들어도': 362, '실습': 363, '국민': 364, '내일': 365, '배움': 366, '비용': 367, '특별': 368, '떨어뜨려서': 369, '어요': 370, '컴': 371, '갔': 372, '상관없': 373, '인가': 374, '사귀': 375, '맞': 376, '옷': 377, '젖': 378, '동상': 379, '걸릴': 380, '놔': 381, '날씨': 382, '추워서': 383, '힘들': 384, '갈까': 385, '잠': 386, '또': 387, '날': 388, '뒷담': 389, '까': 390, '짜증': 391, '방': 392, '냐구': 393, '심해': 394, '대출': 395, '갈': 396, '듯': 397, '나와': 398, '그냥': 399, '출근': 400, '발목': 401, '새': 402, '샀': 403, '이사': 404, '이렇게': 405, '마음': 406, '끝났으면': 407, '호텔': 408, '프로젝트': 409, '준비': 410, '학교': 411, '올게': 412, '택시': 413, '백수': 414, '걸': 415, '뿌': 416, '얘': 417, '시켜야': 418, '도시락': 419, '싸': 420, '딸기': 421, '밥': 422, '추천': 423, '문': 424, '다가': 425, '소화': 426, '퇴근': 427, '아빠': 428, '주말': 429, '가족': 430, '조금': 431, '해요': 432, '따로': 433, '존재': 434, '35': 435, '가입': 436, '근무': 437, '직장인': 438, '건가요': 439, '훈련': 440, '장려금': 441, '라': 442, '질까': 443, '요': 444, '전역': 445, '지원서': 446, '취소': 447, '결과': 448, '발표': 449, '위해서': 450, '자주': 451, '문제': 452, '마감': 453, '작성': 454, '지식': 455, '학습': 456, '출결': 457, '영향': 458, '특정': 459, '부분': 460, '언제': 461, '정해져': 462, '태블릿': 463, '참': 464, '가시': 465, '성능': 466, '스펙': 467, '중간': 468, '졌': 469, '얼마': 470, '상담': 471, '계시': 472, '멘토링': 473, '이용': 474, '액정': 475, '나갔': 476, '고장': 477, '났': 478, '난': 479, '고장났': 480, '건강': 481, '행복': 482, '꼭': 483, '찾': 484, '인지': 485, '열나': 486, '맹맹': 487, '막혀서': 488, '일요일': 489, '교회': 490, '끝': 491, '바다': 492, '놀러': 493, '개곡': 494, '졸린데': 495, '불면증': 496, '영어': 497, '취직': 498, '한자': 499, '엄청': 500, '친한': 501, '그래': 502, '속상해': 503, '머리': 504, '열심히': 505, '더니': 506, '히스테리': 507, '병': 508, '결혼': 509, '노처녀': 510, '때문': 511, '회사': 512, '고구마': 513, '신경': 514, '황당': 515, '건들': 516, '나온다': 517, '때려': 518, '다크서클': 519, '스트레스': 520, '공황': 521, '장애': 522, '듣': 523, '이해': 524, '대답': 525, '란': 526, '잡': 527, '한숨': 528, '미팅': 529, '빨리': 530, '축제': 531, '느라': 532, '바빠': 533, '사업': 534, '탈출': 535, '게임': 536, '출퇴근': 537, '기름': 538, '값': 539, '올랐': 540, '걱정': 541, '1': 542, '덕수궁': 543, '돌담길': 544, '둘': 545, '별': 546, '불러': 547, '공기': 548, '치킨': 549, '감': 550, '랭': 551, '바나나': 552, '빵': 553, '냐': 554, '참외': 555, '맛있': 556, '점심': 557, '줘': 558, '회식': 559, '당했': 560, '긁': 561, '콕': 562, '접촉': 563, '사고': 564, '박': 565, '미끄러질': 566, '넘어질': 567, '모기': 568, '무서워서': 569, '침대': 570, '폰': 571, '떨': 572, '과식': 573, '더': 574, '마시': 575, '서': 576, '마셔': 577, '여유': 578, '막힌다': 579, '생각난다': 580, '짝': 581, '남': 582, '에게': 583, '빼빼': 584, '줘도': 585, '부담': 586, '짝사랑': 587, '고백': 588, '하고': 589, '됐': 590, '편': 591, '하나': 592, '곳': 593, '재밌': 594, '카톡': 595, '아무': 596, '연락': 597, '여행': 598, '야구장': 599, '유': 600, '기견': 601, '부터': 602, '부모': 603, '님': 604, '이벤트': 605, '월급': 606, '꽃': 607, '탕': 608, '던': 609, '성공': 610, '만족': 611, '이제한': 612, '모든': 613, '30': 614, '미성년자': 615, '대한': 616, '계약직': 617, '15': 618, '미만': 619, '정확': 620, '미': 621, '가입자': 622, '지만': 623, '교육비': 624, '상태': 625, '군복': 626, '무자': 627, '포함': 628, '입영': 629, '다를까요': 630, '소집': 631, '어도': 632, '지급': 633, '어야': 634, '고려': 635, '여': 636, '탈락자': 637, '은데': 638, '해당': 639, '생기': 640, '문의': 641, 'AI': 642, '개발자': 643, '언어': 644, '한지': 645, '출제': 646, '한데': 647, '어려운데': 648, '학기': 649, '복학': 650, '대학생': 651, '끝나': 652, '학업': 653, '입력': 654, '기입': 655, '공학': 656, '무리': 657, '여서': 658, '미치': 659, '이익': 660, '낮': 661, '100': 662, '규정': 663, '용무': 664, '사정': 665, '반영': 666, '등': 667, '지정': 668, '려면': 669, '코로나': 670, '제약': 671, '정해진': 672, '는다면': 673, '타': 674, '플랫폼': 675, '없이': 676, '어느': 677, '정도': 678, '종료': 679, '풀': 680, '아니면': 681, '계': 682, '전액': 683, '발생': 684, '컨설팅': 685, '서비스': 686, '일자리': 687, '아닌': 688, '거주': 689, '궁': 690, '급': 691, '식사': 692, '시설': 693, '장점': 694, '키보드': 695, '먹히': 696, '제대로': 697, '작동': 698, '아요': 699, '에러': 700, '메시지': 701, '띄우': 702, '맛': 703, '의지': 704, '지로': 705, '아프': 706, '장만': 707, '여자': 708, '볼려고': 709, '몸': 710, '얼어붙': 711, '차갑': 712, '쐬': 713, '나갈까': 714, '산책': 715, 'mmm': 716, '항상': 717, '선택': 718, '온': 719, '누워': 720, '다녀': 721, '다닐까': 722, '새로운': 723, '쉽': 724, '친군데': 725, '호박씨': 726, '깜': 727, '친군': 728, '뒤통수': 729, '맞음': 730, '막': 731, '깠': 732, '네일': 733, '발톱': 734, '손질': 735, '손톱': 736, '정리': 737, '으아': 738, '패디': 739, '단발': 740, '우울': 741, '요즘': 742, '우울해': 743, '다운': 744, '집중': 745, '업무': 746, '쉴': 747, '봤': 748, '기운': 749, '청소': 750, '낸다': 751, '배': 752, '평생': 753, '학자금': 754, '전세': 755, '푸드': 756, '바늘': 757, '날카로워진다': 758, '다는': 759, '어떻': 760, '어이없': 761, '왤케': 762, '쓰이': 763, '괜히': 764, '내버려': 765, '두': 766, '아이고': 767, '소리': 768, '절로': 769, '치우': 770, '내려왔': 771, '턱밑': 772, '괴롭혀': 773, '따돌림': 774, '당함': 775, '왕따': 776, '당하': 777, '괴롭': 778, '이직각': 779, '이직': 780, '할지': 781, '고민': 782, '자체': 783, '삐끗': 784, '삔': 785, '접': 786, '질렀': 787, '우울증': 788, '감기': 789, '걸렸': 790, '기로': 791, '엔': 792, '아침': 793, '놈': 794, '정말': 795, '고고씽': 796, '힘든': 797, '로비': 798, '구상': 799, '갔다왔': 800, '놀이': 801, '동산': 802, '버': 803, '랜드': 804, '심부름': 805, '다녀올': 806, '오래': 807, '걸려': 808, '줄어들': 809, '탔': 810, '썼': 811, '비싸': 812, '유지비': 813, '실력': 814, '쩌': 815, '쓸': 816, '위': 817, '연예인': 818, '건물': 819, '따라서': 820, '반지': 821, '시골': 822, '자장가': 823, '달랬': 824, '줬': 825, '먼지': 826, '심하': 827, '들어오': 828, '자리': 829, '들어왔': 830, '간장': 831, '봐야지': 832, '옴': 833, '을래': 834, '개': 835, '스': 836, '테': 837, '끼': 838, '스테이크': 839, '으러': 840, '골라': 841, '저녁밥': 842, '교통사고': 843, '누가': 844, '열': 845, '앞차': 846, '뒤차': 847, '미끄러워서': 848, '어서': 849, '웽웽': 850, '거려': 851, '달려왔': 852, '뛰': 853, '걷': 854, '일어나': 855, '니까': 856, '떨어져': 857, '자다': 858, '떨어졌': 859, '떨궜': 860, '뜨': 861, '렸': 862, '움직이': 863, '체한': 864, '얹힌': 865, '속': 866, '부룩': 867, '어제': 868, '기억': 869, '한잔': 870, '어지러워': 871, '끊': 872, '그만': 873, '마셔야': 874, '숙취': 875, '늘': 876, '빡빡': 877, '힘드': 878, '어떻하': 879, '아닌데': 880, '효녀': 881, '불효': 882, '불효자': 883, '웁': 884, '니다': 885, '번호': 886, '우산': 887, '쓰': 888, '진부': 889, '한가': 890, '스러워': 891, '어딘가': 892, '떠나': 893, '부자': 894, '되게': 895, '해외여행': 896, '공연': 897, '커피': 898, '백조': 899, '심심': 900, '꺼': 901, '솔로': 902, '맘': 903, '편한': 904, '위로': 905, '부르': 906, '초대': 907, '고고': 908, '책': 909, '읽': 910, '분양': 911, '으려고': 912, '입양': 913, '지금': 914, '갈려구': 915, '쉬': 916, '효도': 917, '께': 918, '드려야지': 919, '으니까': 920, '집들이': 921, '이제': 922, '성인': 923, '당첨': 924, '지롱': 925, '는대': 926, '드디어': 927, '천일': 928, '올라서': 929, '기분': 930, '음': 931, '헤헤': 932, '밥도둑': 933, '수다': 934, '힐링': 935, '외식': 936, '남친': 937, '프로': 938, '포즈': 939, '편입': 940, '적금': 941, '해지': 942, '누구': 943, '연령': 944, '대가': 945, '따른': 946, '이상': 947, '후반': 948, '상관': 949, '무관': 950, '상관없이': 951, '판단': 952, '근로': 953, '수입': 954, '내정자': 955, '죠': 956, '원하': 957, '일반': 958, '계약': 959, '원천': 960, '징수': 961, '며': 962, '시간대': 963, '출시': 964, '어려운': 965, '대처': 966, '입대': 967, '예비역': 968, '공익': 969, '사회': 970, '요원': 971, '군인': 972, '복무자': 973, '못하': 974, '아도': 975, '정부': 976, '지급액': 977, '지불': 978, '기존': 979, '떨어지': 980, '기회': 981, '주어지': 982, '선발': 983, '결정': 984, '가요': 985, '생겨서': 986, '못할': 987, '다음': 988, '이력서': 989, '표기': 990, '확정': 991, '자신': 992, '고일': 993, '대상자': 994, '합격자': 995, '올': 996, '적용': 997, '형태': 998, '묻': 999, '질문': 1000, '보통': 1001, '걸리': 1002, '평가': 1003, '식': 1004, '안내': 1005, '실': 1006, '쳐야': 1007, '파이썬': 1008, '코테': 1009, '자세': 1010, '지원자': 1011, '유형': 1012, '은가요': 1013, '추후': 1014, '측': 1015, '입니다': 1016, '아직': 1017, '그래서': 1018, '걸로': 1019, '올리': 1020, '될까요': 1021, '이럴': 1022, '학부': 1023, '석사': 1024, '적어도': 1025, '던데': 1026, '라는': 1027, '여쭙': 1028, '다녔': 1029, '모르': 1030, '지장': 1031, '몰라도': 1032, '잇': 1033, '기초': 1034, '놓': 1035, '수월': 1036, '할려고': 1037, '불참': 1038, '지각': 1039, '조퇴': 1040, '저조': 1041, '일부': 1042, '월차': 1043, '급한': 1044, '사유': 1045, '인정': 1046, '무조건': 1047, '까지': 1048, '간격': 1049, '위해': 1050, '확인': 1051, '위한': 1052, '율': 1053, '충족': 1054, '못했': 1055, '마다': 1056, '접수': 1057, '최초': 1058, '잘못': 1059, '허용': 1060, '원래': 1061, '관계없이': 1062, '된다면': 1063, '실시간': 1064, '동시': 1065, '일정': 1066, '조정': 1067, '인해': 1068, '부족': 1069, '미칠까요': 1070, '강의': 1071, '패드': 1072, '기기': 1073, '들어야': 1074, '참가자': 1075, '수행': 1076, '사': 1077, '용도': 1078, '빌릴': 1079, '준다고': 1080, '반납': 1081, '잃어버린': 1082, 'CPU': 1083, 'RAM': 1084, '기가': 1085, '저장': 1086, '공간': 1087, '용량': 1088, '정규': 1089, '점검': 1090, '들으면': 1091, '인터넷': 1092, '진단': 1093, '종류': 1094, '개별': 1095, '날짜': 1096, '일괄': 1097, '교과목': 1098, '유예': 1099, '보조금': 1100, '자기': 1101, '부담금': 1102, '유료': 1103, '전혀': 1104, '금액': 1105, '세미나': 1106, '박람회': 1107, '진로': 1108, '탐색': 1109, '해외': 1110, '관한': 1111, '용시': 1112, '장학금': 1113, '교재': 1114, '도구': 1115, '튜': 1116, '링': 1117, '특강': 1118, '주제': 1119, '참여시': 1120, '거쳐야': 1121, '생활': 1122, '용품': 1123, '기관': 1124, '거리': 1125, '학생': 1126, '가깝': 1127, '예약': 1128, '객실': 1129, '크기': 1130, '이용료': 1131, '할인': 1132, '교통비': 1133, '숙박비': 1134, '머무를': 1135, 'IT': 1136, '똑같': 1137, '아닌가요': 1138, '우수': 1139, '강점': 1140, '상세히': 1141, '설명': 1142}\n"]}]},{"cell_type":"code","source":["# 전체 토큰의 수 확인\n","# train_seq = tokenizer.texts_to_sequences(clean_train_questions)\n","# print(train_seq)\n","# padded = pad_sequences(train_seq)\n","# print(padded)\n"],"metadata":{"id":"ft3qCoGbuIt4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797967640,"user_tz":-540,"elapsed":15,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"9483b178-8c5f-4f0e-f35b-9a2abc142ca2"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[[369, 112, 475, 476, 14], [475, 476, 14], [112, 369, 477, 478, 25, 170], [32, 695, 5, 59, 696, 105], [32, 9, 369, 477, 479, 85, 77, 56], [152, 5, 697, 698, 6, 54, 153, 699], [152, 5, 700, 701, 24, 702, 20, 3, 370], [371, 303, 480, 25, 170], [371, 303, 5, 703, 2, 372, 14], [371, 303, 5, 59, 268], [112, 480, 25, 170], [112, 59, 268], [32, 59, 268], [113, 704, 4, 373, 25, 170], [113, 41, 705, 59, 11, 4, 88, 374, 170], [706, 23, 59, 11, 33], [215, 15, 481, 6, 237], [215, 15, 482, 6, 237], [215, 69, 6, 237], [215, 71, 6, 237], [215, 4, 483, 69, 13, 76, 114], [238, 171, 15, 4, 483, 154, 9, 707, 13, 76, 114], [708, 119, 24, 375, 20, 45, 14], [215, 15, 481, 6, 20, 482, 6, 57, 269, 76, 114], [113, 5, 270, 6, 4, 88, 9, 484, 20, 45, 14], [113, 5, 72, 56, 6, 4, 88, 19, 120, 485, 484, 56, 709], [172, 376, 304, 377, 378, 155, 14], [377, 2, 42, 378, 155, 14], [486, 20, 305], [194, 487], [379, 380, 173, 64, 14], [379, 380, 76, 77, 56], [271, 140, 306], [271, 2, 86, 140, 306], [174, 175, 100, 86, 307, 105], [56, 381, 194, 272], [273, 14, 115, 4, 216], [273, 14, 115, 4, 216, 141, 121, 14], [100, 174, 175, 307, 105], [274, 115, 9, 173, 64, 105], [274, 115, 109, 42], [194, 272], [172, 376, 304, 377, 378, 155, 14], [486, 20, 305], [194, 487], [379, 380, 76, 77, 56], [271, 140, 306], [271, 2, 86, 140, 306], [174, 175, 100, 86, 307, 105], [56, 381, 194, 272], [273, 14, 115, 4, 216], [273, 14, 115, 4, 216, 141, 121, 14], [100, 174, 175, 307, 105], [274, 115, 9, 173, 64, 105], [274, 115, 109, 42], [194, 272], [100, 382, 5, 174, 175, 274], [194, 5, 488, 217, 14], [86, 383, 710, 2, 711, 9, 85, 77, 56], [86, 383, 115, 9, 85, 77, 121, 14], [100, 382, 5, 174, 175, 712, 105], [383, 115, 9, 85, 77, 56], [194, 5, 488, 384, 14], [489, 10, 490, 491, 25, 20, 176, 5, 20, 45, 14], [489, 10, 490, 51, 5], [271, 713, 239, 714], [492, 5, 66, 20, 6, 23, 385], [492, 493, 5, 66, 20, 46, 132], [494, 5, 275, 6, 23, 385], [494, 493, 5, 66, 20, 46, 132], [715, 5, 132], [716], [717, 718, 13, 156, 384, 14], [495, 177, 66, 109, 14], [495, 386, 2, 59, 89], [496, 719, 76, 77, 56], [496, 374, 170], [720, 3, 33, 386, 2, 59, 89], [386, 2, 387, 59, 89], [497, 240, 721, 132], [497, 240, 133, 722], [498, 6, 23, 73, 59, 90, 11, 4, 216, 141, 121, 33], [498, 6, 23, 73, 59, 90, 308, 216, 141, 121, 14], [499, 73, 15, 133, 34, 11, 33], [499, 73, 218], [73, 178, 90, 276], [723, 73, 178, 90, 276], [119, 241, 174, 64, 33, 388, 500, 724, 57, 174, 64, 25, 170], [25, 51, 119, 27, 174, 64, 25, 170], [501, 725, 726, 727], [501, 728, 195, 729, 730], [119, 5, 25, 24, 731, 142], [119, 5, 113, 389, 732, 14], [119, 110, 21, 502], [733, 219], [734, 735, 47, 196], [503], [736, 737, 133, 219], [738, 503], [739, 25, 219], [504, 66, 24, 390], [740, 27, 66, 24, 390], [741, 46], [391, 25], [742, 242, 743, 54, 20, 744, 11, 105], [242, 505, 73, 6, 4, 179, 745, 2, 270, 59, 11, 105], [746, 5, 86, 197, 304, 747, 67, 2, 28, 105], [242, 154, 37, 152, 51, 748, 506, 749, 2, 28, 105], [392, 750, 25, 46, 132], [26, 309, 507, 141, 56], [26, 309, 508], [26, 309, 508, 48, 76, 77, 56], [509, 134, 6, 54, 310, 311, 25, 195, 312, 113, 393], [510, 507], [510, 110, 311, 312, 751], [313, 511, 10, 752, 305], [313, 394], [56, 381, 313], [313], [395, 314, 42, 115, 109, 14], [395, 314, 165, 30, 753, 42, 396, 397], [88, 51, 6, 42, 115, 9, 85, 77, 56], [512, 88, 2, 86, 197, 56], [513, 315, 218], [513, 51, 60, 20, 315, 218], [754, 314, 42, 115, 109, 14], [755, 395, 314, 42, 115, 109, 14], [198, 756, 315, 218], [315, 218], [514, 2, 757, 491, 77, 56], [311, 758], [25, 389, 312, 6, 4, 277, 21, 219], [25, 389, 312, 6, 4, 277, 3, 759, 179, 760, 57, 6, 54], [515, 46, 115, 109, 42], [515, 122, 157, 15, 59, 398], [761, 14], [316, 21, 502], [762, 514, 763, 54], [764, 516, 54, 157, 241], [25, 133, 516, 54, 157, 241, 46], [25, 133, 765, 766, 23, 72, 109, 14], [60, 20, 269, 101, 384, 42], [767, 768, 5, 769, 517], [42, 518, 770, 20, 45, 14], [518, 317, 20, 45, 42], [399, 66, 4, 76, 278, 54], [519, 771, 14], [519, 772, 10, 3, 42], [2, 512, 4, 25, 51, 88, 6, 4, 85, 77, 56], [220, 37, 86, 773], [220, 37, 774, 775], [220, 37, 776, 777, 20, 3, 14], [220, 2, 778, 42], [779, 374], [780, 34, 781, 782, 2, 114], [220, 5, 101, 217, 14], [400, 783, 5, 520, 114], [400, 2, 520, 114], [400, 6, 101, 217, 14], [521, 522, 221, 14], [521, 522, 3, 14], [401, 784, 64, 14], [401, 785, 76, 77, 277], [401, 786, 787, 14], [788, 3, 14], [789, 790, 25, 170], [113, 154, 2, 221, 14], [402, 154, 5, 101, 27, 64, 14], [402, 154, 403, 14], [402, 154, 22, 404, 5, 791, 64, 14], [154, 9, 403, 14], [238, 10, 154, 403, 14], [316, 113, 154, 2, 221, 14], [60, 9, 76, 102, 28, 25], [102, 60, 9, 76, 28, 25], [238, 792, 387, 102, 46, 60, 54], [222, 318, 102, 60, 196], [222, 22, 102, 60, 196], [793, 318, 102, 60, 196], [157, 9, 310, 405, 177, 141, 56, 523, 319], [157, 9, 524, 24, 177, 46], [405, 177, 141, 56, 523, 319], [310, 157, 9, 177, 46, 157, 9, 177, 6, 393], [310, 157, 9, 524, 24, 177, 6, 393], [525, 270, 6, 319], [525, 270, 46], [25, 526, 794], [479, 795, 59, 11, 109, 42], [406, 527, 20, 796], [406, 527, 20, 199, 178], [270, 11, 109, 54], [528, 398], [528, 51, 311, 517], [797, 85, 133, 407], [25, 529, 279], [25, 243, 279], [529, 279], [243, 279], [408, 798, 37, 243, 6, 101, 27, 64, 14], [408, 37, 243, 6, 101, 27, 64, 14], [409, 407, 72, 109, 42], [409, 530, 407, 72, 109, 42], [531, 410, 6, 532, 533], [411, 531, 410, 6, 532, 533], [534, 799], [392, 535, 536, 800, 14], [392, 535, 536, 64, 14], [801, 802, 5, 20, 45, 14], [277, 803, 804, 5, 20, 45, 14], [88, 6, 20, 412], [805, 806, 57], [411, 372, 42, 412], [240, 372, 42, 412], [537, 67, 86, 807, 808], [537, 67, 2, 133, 809, 155, 165, 72, 109, 42], [538, 539, 540, 14], [413, 86, 140, 810, 14], [413, 172, 27, 200, 9, 86, 140, 811, 14], [538, 539, 86, 812], [320, 813, 5, 86, 140, 78, 14], [413, 172, 5, 86, 140, 398], [113, 814, 133, 815, 4, 397], [223, 224, 200, 321, 20, 45, 14], [816, 179, 28, 4, 541, 542, 817, 818, 541], [88, 59, 6, 20, 200, 321, 20, 45, 14], [200, 197, 19, 414, 6, 20, 45, 14], [819, 61, 6, 20, 45, 14], [543, 544, 415, 155, 14], [543, 544, 820, 545, 2, 415, 155, 14], [821, 322, 6, 244], [546, 58, 239, 5, 20, 45, 42], [546, 58, 239, 822, 5, 20, 45, 42], [823, 547, 824, 506, 547, 825, 14], [323, 511, 10, 391, 25], [323, 5, 86, 197, 304, 391, 25], [826, 217, 42], [323, 827, 180], [100, 15, 548, 5, 416, 417], [100, 15, 382, 5, 416, 417], [323, 394], [548, 5, 416, 417], [399, 245, 132], [245, 140, 324, 76, 77, 56], [245, 132], [245, 2, 140, 828, 105], [245, 829, 830, 14], [325, 5, 245, 58, 241, 6, 44, 33], [243, 219], [243, 46, 132], [831, 549, 418, 54], [550, 157, 551, 2, 60, 20, 45, 42], [550, 157, 551, 2, 60, 201], [419, 420, 832], [419, 420, 833], [419, 420, 326, 14], [421, 60, 201], [421, 60, 834], [552, 60, 20, 45, 42], [422, 60, 66], [422, 134, 60, 66], [553, 60, 201], [553, 30, 835, 51, 60, 201], [836, 837, 838, 60, 19, 76, 246, 171, 51, 2, 554], [839, 60, 840, 5, 20, 45, 14], [555, 556, 42], [555, 60, 201], [549, 418, 54], [422, 102, 60, 196], [100, 222, 102, 60, 196], [100, 222, 19], [100, 557, 102, 60, 196], [100, 557, 19], [100, 19, 102, 60, 196], [222, 318, 133, 841, 558], [222, 318, 423, 46, 558], [222, 102, 60, 54], [842, 102, 60, 554], [100, 559, 59, 6, 25], [559, 64, 165, 72, 109, 42], [843, 560, 14], [844, 424, 845, 42, 113, 320, 561, 155, 14], [424, 562, 560, 14], [424, 562, 30, 397], [563, 564, 478, 14], [563, 564, 221, 14], [846, 280, 565, 121, 14], [847, 280, 565, 121, 14], [237, 2, 848, 566, 173, 64, 14], [237, 2, 273, 849, 566, 173, 64, 14], [567, 173, 64, 14], [568, 5, 86, 197, 56], [568, 5, 850, 851], [569, 852, 14], [569, 853, 14, 326, 14], [106, 388, 173], [106, 388, 173, 64, 14], [530, 854, 425, 567, 173], [855, 58, 856, 570, 37, 857, 3, 155, 14], [858, 5, 570, 37, 859, 14], [571, 860, 14], [112, 572, 14, 861, 862, 14], [86, 140, 60, 155, 14], [573, 122, 426, 5, 59, 268], [426, 5, 59, 268], [426, 418, 6, 4, 179, 863, 101, 5, 217, 14], [573, 64, 14], [864, 76, 77, 56], [865, 76, 77, 56], [866, 574, 867, 46], [868, 225, 575, 20, 25, 576, 869, 59, 25], [225, 870, 51, 577, 15, 871], [225, 9, 86, 177, 577], [225, 872, 9, 76, 114], [225, 133, 873, 874, 54], [225, 60, 20, 281, 561, 155, 14], [875, 86, 394], [225, 60, 20, 504, 305], [876, 877, 6, 57, 269, 101, 878, 105], [578, 226, 57, 269, 20, 45, 14], [2, 133, 578, 226, 57, 269, 20, 45, 42], [320, 579], [320, 316, 579, 879, 319], [427, 67, 15, 880, 500, 272], [88, 2, 86, 197, 56], [325, 428, 58, 20, 45, 42], [325, 580], [113, 5, 106, 881, 114], [113, 5, 405, 387, 882, 24, 279], [883, 4, 884, 885], [428, 58, 20, 45, 42], [428, 580], [427, 111, 10, 327, 275, 46, 132], [427, 6, 20, 327, 275, 157, 46, 132], [886, 247, 241, 46, 132], [887, 28, 33, 227, 888, 275, 219, 86, 889, 890], [581, 582, 583, 584, 27, 585, 276], [581, 582, 195, 322, 61, 23, 586, 891, 219], [587, 6, 4, 282, 195, 322, 61, 23, 586, 276], [587, 195, 584, 27, 585, 276], [588, 46, 132], [429, 10, 327, 275, 46, 132], [72, 56, 6, 4, 277, 5, 221, 14], [223, 239, 5, 20, 45, 42], [421, 5, 72, 56], [552, 5, 72, 56], [892, 893, 20, 45, 42], [88, 6, 101, 217, 42], [430, 78, 58, 20, 45, 14], [200, 321, 20, 45, 14], [894, 895, 46, 61, 123], [100, 228, 325, 58, 20, 45, 14], [896, 5, 20, 45, 42], [897, 58, 239, 5, 20, 45, 14], [200, 197, 19, 414, 6, 20, 45, 14], [898, 575, 20, 45, 14], [25, 414, 114], [102, 589, 223, 54], [899, 590, 14], [900, 6, 42], [429, 10, 223, 901, 423], [429, 10, 102, 589, 223, 390], [102, 6, 54], [25, 51, 902, 114], [113, 591, 2, 28, 4, 76, 77, 56], [113, 591, 2, 241, 4, 592, 15, 28, 4, 76, 77, 56], [406, 545, 593, 2, 28, 14], [903, 904, 179, 5, 28, 14], [905, 46, 216, 282, 2, 28, 14], [594, 4, 76, 102, 28, 25], [594, 4, 88, 2, 28, 14], [119, 140, 375, 20, 45, 14], [119, 375, 20, 45, 14], [595, 13, 282, 2, 592, 15, 28, 105], [595, 13, 119, 5, 28, 105], [223, 282, 2, 28, 105], [73, 6, 101, 217, 14], [596, 195, 15, 597, 2, 59, 89], [119, 78, 906, 244], [119, 78, 907, 6, 244], [430, 78, 328, 598, 396, 76, 114], [430, 328, 598, 5, 244], [599, 908], [599, 134, 385], [909, 133, 910, 201], [600, 601, 911, 47, 912], [600, 601, 913, 6, 244], [914, 602, 73, 13, 76, 114], [119, 280, 223, 239, 915], [431, 51, 916, 425, 73, 218], [505, 200, 321, 201], [603, 604, 195, 917, 218], [603, 604, 918, 329, 133, 919], [404, 64, 920, 921, 34, 109, 42], [922, 25, 15, 923, 2, 114], [38, 2, 42], [25, 15, 605, 5, 11, 330], [605, 924, 590, 14], [25, 100, 331, 47, 121, 925], [100, 331, 47, 121, 42], [25, 331, 47, 926], [331, 47, 9, 76, 77, 56], [927, 928], [606, 929, 930, 72, 56], [606, 540, 14], [329, 47, 931], [932, 329, 47, 121, 14], [607, 57, 608, 556, 42], [607, 57, 608, 316, 933], [119, 280, 934, 572, 20, 326, 14], [119, 280, 935, 6, 20, 326, 14], [936, 64, 14], [322, 47, 121, 14], [329, 47, 121, 14], [25, 238, 10, 509, 46], [937, 195, 408, 37, 938, 939, 47, 121, 14], [198, 6, 609, 229, 10, 71, 64, 14], [940, 610, 64, 14], [588, 610, 122, 100, 602, 542, 88, 2, 114], [238, 10, 941, 942, 122, 200, 221, 42], [35, 248, 9, 611, 6, 23, 8, 13, 12, 3, 158, 141, 20, 45, 370], [8, 116, 2, 21, 11, 1], [8, 248, 2, 31, 432], [8, 116, 79, 2, 3, 1], [8, 13, 156, 116, 79, 2, 433, 3, 1], [8, 248, 2, 3, 1], [8, 13, 156, 116, 79, 9, 159, 61, 123], [596, 25, 8, 13, 12, 3, 1, 116, 79, 9, 159, 61, 123], [8, 17, 181, 2, 943, 52], [8, 116, 10, 142, 332, 159, 61, 123], [38, 333, 15, 8, 116, 2, 11, 1], [143, 15, 8, 13, 12, 3, 1], [229, 38, 230, 110, 8, 13, 12, 3, 1], [202, 334, 26, 110, 8, 13, 12, 3, 1], [203, 171, 204, 103, 38, 66, 51, 8, 17, 16], [8, 116, 10, 205, 4, 373, 1], [25, 612, 2, 3, 1], [613, 944, 945, 8, 17, 16], [25, 612, 2, 434, 6, 1], [51, 435, 335, 4, 8, 13, 12, 28, 1], [205, 10, 946, 8, 116, 2, 3, 1], [614, 335, 947, 15, 8, 17, 16], [103, 9, 336, 57, 38, 64, 182, 614, 180, 948, 110, 8, 17, 16], [8, 116, 10, 205, 283, 2, 3, 1], [8, 116, 22, 205, 5, 949, 3, 1], [435, 335, 4, 8, 13, 12, 28, 1], [615, 15, 8, 17, 16], [615, 5, 8, 6, 23, 21, 11, 1], [205, 89, 950, 6, 57, 8, 13, 12, 3, 1], [205, 89, 951, 8, 17, 16], [8, 116, 79, 37, 205, 283, 2, 3, 158, 159, 61, 123], [8, 116, 205, 4, 327, 2, 52], [29, 68, 111, 10, 51, 435, 335, 5, 11, 4, 18, 4, 8, 13, 12, 28, 1], [80, 81, 10, 616, 79, 19, 21, 952, 6, 1], [8, 17, 30, 80, 81, 41, 79, 9, 332, 159, 61, 123], [284, 249, 10, 436, 11, 54, 153, 121, 165, 80, 81, 27, 324, 12, 3, 1], [617, 22, 437, 6, 20, 3, 33, 80, 81, 27, 324, 12, 3, 1], [617, 48, 18, 15, 8, 17, 16], [61, 618, 67, 619, 953, 48, 18, 80, 81, 27, 324, 12, 3, 1], [80, 81, 79, 2, 21, 11, 1], [35, 18, 80, 81, 27, 58, 1], [954, 2, 28, 4, 18, 10, 51, 80, 81, 27, 58, 1], [80, 81, 79, 9, 159, 61, 123], [80, 81, 27, 285, 11, 4, 79, 2, 31, 36], [35, 18, 10, 80, 81, 27, 285, 11, 1], [80, 81, 41, 620, 30, 79, 2, 74], [80, 81, 41, 620, 30, 79, 2, 21, 11, 1], [284, 249, 621, 622, 4, 80, 81, 27, 58, 1], [284, 249, 10, 436, 11, 14, 3, 623, 153, 165, 80, 81, 27, 58, 1], [284, 249, 621, 622, 4, 80, 81, 52], [438, 15, 8, 13, 12, 3, 1], [166, 66, 15, 8, 13, 12, 3, 1], [250, 333, 15, 8, 13, 12, 3, 1], [250, 955, 15, 8, 13, 12, 3, 1], [7, 178, 88, 251, 10, 250, 6, 4, 18, 8, 13, 12, 3, 1], [7, 178, 88, 206, 10, 250, 6, 4, 18, 8, 13, 12, 3, 1], [438, 15, 43, 13, 12, 3, 1], [166, 66, 15, 43, 13, 12, 3, 1], [438, 15, 87, 13, 12, 3, 1], [167, 220, 10, 166, 26, 110, 7, 10, 8, 13, 12, 3, 1], [220, 10, 330, 20, 3, 4, 18, 10, 4, 8, 13, 12, 28, 1], [166, 66, 15, 8, 17, 16], [250, 230, 2, 623, 167, 166, 26, 110, 8, 2, 17, 16], [88, 6, 20, 3, 4, 18, 10, 15, 7, 10, 87, 13, 12, 3, 1], [166, 26, 10, 15, 43, 17, 16], [166, 26, 10, 4, 87, 106, 17, 16], [166, 26, 10, 4, 43, 106, 17, 16], [166, 26, 48, 18, 10, 15, 8, 17, 16], [166, 26, 48, 18, 10, 15, 124, 17, 16], [117, 24, 6, 20, 3, 33, 8, 13, 12, 3, 1], [117, 24, 6, 20, 3, 4, 18, 8, 231, 5, 31, 36], [117, 246, 67, 2, 6, 23, 8, 2, 17, 16], [117, 24, 6, 20, 3, 4, 18, 10, 286, 27, 285, 11, 1], [117, 24, 6, 20, 3, 4, 18, 10, 80, 81, 27, 285, 11, 1], [117, 82, 15, 8, 13, 12, 3, 1], [117, 82, 15, 8, 17, 16], [117, 82, 19, 8, 13, 12, 28, 1], [117, 24, 6, 20, 3, 33, 61, 618, 67, 619, 19, 8, 17, 6, 956], [117, 24, 6, 20, 3, 4, 18, 7, 9, 43, 13, 12, 28, 1], [117, 67, 39, 7, 67, 2, 337, 54, 153, 165, 8, 13, 12, 3, 4, 439], [117, 24, 6, 20, 3, 4, 18, 15, 87, 17, 16], [117, 82, 19, 80, 81, 27, 58, 1], [117, 82, 15, 286, 27, 58, 1], [117, 82, 19, 80, 81, 27, 58, 1, 286, 27, 58, 1], [160, 15, 8, 13, 12, 3, 1], [207, 66, 15, 8, 13, 12, 3, 1], [160, 41, 18, 8, 231, 5, 31, 36], [160, 41, 18, 15, 440, 441, 9, 47, 9, 12, 3, 1], [160, 15, 43, 13, 12, 3, 1], [207, 66, 15, 43, 13, 12, 3, 1], [160, 15, 8, 17, 16], [207, 66, 15, 8, 17, 16], [160, 15, 7, 9, 78, 9, 12, 3, 1], [160, 15, 87, 13, 12, 3, 1], [207, 66, 15, 87, 13, 12, 3, 1], [160, 4, 87, 13, 12, 28, 1], [207, 66, 4, 87, 13, 12, 28, 1], [160, 25, 207, 66, 15, 8, 13, 12, 3, 1], [534, 9, 338, 6, 224, 7, 9, 957, 4, 18, 8, 116, 2, 283, 11, 4, 18, 5, 3, 1], [160, 25, 207, 66, 4, 8, 106, 17, 16], [160, 25, 207, 66, 15, 958, 69, 8, 39, 135, 6, 57, 8, 2, 17, 16], [92, 15, 8, 13, 12, 3, 1], [92, 8, 231, 10, 142, 31, 36], [92, 15, 8, 17, 16], [92, 959, 19, 373, 83, 203, 180, 249, 19, 40, 6, 54, 153, 20, 960, 961, 51, 40, 6, 962, 963, 4, 337, 54, 153, 182], [92, 4, 8, 106, 17, 16], [92, 15, 286, 27, 58, 1], [92, 4, 80, 81, 52], [92, 27, 88, 9, 6, 224, 7, 8, 13, 12, 3, 1], [92, 41, 7, 8, 181, 161, 4, 437, 96, 10, 228, 247, 442, 443, 444], [92, 27, 88, 6, 224, 15, 284, 249, 436, 161, 10, 228, 80, 81, 27, 285, 11, 4, 18, 5, 3, 83], [92, 48, 18, 8, 13, 12, 28, 1], [92, 27, 88, 6, 224, 7, 9, 47, 4, 18, 624, 8, 125, 10, 75, 107, 48, 8, 2, 17, 16], [92, 4, 286, 52, 80, 81, 52], [92, 24, 80, 81, 27, 58, 1], [92, 27, 88, 6, 224, 7, 9, 47, 339, 287, 41, 124, 231, 5, 144, 16], [92, 41, 7, 8, 181, 161, 4, 437, 67, 10, 228, 247, 442, 443, 444], [92, 15, 124, 13, 12, 3, 1], [92, 4, 7, 9, 78, 9, 12, 28, 1], [92, 4, 43, 13, 12, 28, 1], [92, 4, 124, 13, 12, 28, 1], [167, 168, 136, 26, 48, 625, 110, 8, 2, 17, 16], [167, 168, 136, 26, 48, 18, 7, 8, 13, 12, 3, 4, 340, 2, 3, 83], [445, 333, 15, 8, 2, 17, 16], [29, 68, 206, 10, 136, 288, 5, 11, 33, 8, 17, 16], [626, 627, 5, 7, 9, 47, 339, 287, 41, 124, 231, 5, 144, 16], [29, 204, 964, 136, 288, 145, 2, 23, 8, 2, 106, 17, 16], [168, 136, 26, 48, 18, 7, 8, 2, 965, 183, 2, 3, 33, 21, 966, 34, 252, 8, 19, 17, 16], [168, 136, 26, 10, 15, 43, 17, 16], [168, 136, 26, 48, 18, 7, 8, 181, 161, 4, 168, 136, 96, 184, 967, 6, 57, 308, 968, 628, 161, 10, 228, 247, 442, 443, 444], [969, 48, 18, 8, 2, 17, 16], [970, 136, 971, 48, 18, 8, 2, 17, 16], [168, 136, 26, 48, 18, 7, 8, 181, 161, 4, 629, 145, 111, 445, 145, 111, 10, 228, 630], [631, 288, 230, 110, 8, 17, 16], [168, 136, 26, 48, 972, 15, 7, 8, 181, 2, 308, 12, 3, 1], [168, 973, 4, 8, 2, 106, 17, 16], [626, 627, 5, 7, 9, 47, 339, 287, 41, 124, 231, 5, 144, 16], [136, 288, 230, 110, 8, 2, 17, 16], [168, 136, 26, 48, 18, 7, 8, 181, 161, 4, 629, 145, 111, 445, 145, 111, 10, 228, 630], [631, 288, 230, 110, 8, 2, 17, 252], [250, 111, 97, 98, 185, 26, 110, 8, 13, 12, 3, 1], [97, 98, 185, 26, 10, 15, 8, 13, 12, 3, 1], [97, 98, 185, 26, 10, 15, 7, 9, 78, 9, 12, 3, 1], [97, 98, 185, 26, 110, 8, 13, 12, 3, 1], [97, 98, 185, 26, 48, 18, 440, 441, 9, 47, 54, 974, 4, 85, 22, 141, 20, 3, 4, 179, 376, 1], [97, 98, 185, 26, 2, 23, 440, 441, 19, 177, 47, 975, 8, 19, 17, 16], [97, 98, 185, 26, 2, 23, 8, 106, 17, 16], [97, 98, 185, 26, 110, 8, 17, 16], [97, 98, 185, 26, 2, 632, 8, 17, 16], [97, 98, 208, 5, 7, 43, 26, 48, 184, 97, 98, 24, 242, 122, 633, 47, 9, 12, 3, 1], [97, 98, 208, 15, 8, 17, 16], [7, 43, 26, 48, 97, 98, 208, 41, 253, 24, 8, 6, 4, 976, 341, 2, 3, 1], [97, 98, 208, 4, 8, 106, 17, 16], [97, 98, 208, 5, 7, 43, 26, 48, 18, 69, 341, 9, 47, 9, 12, 3, 158, 161, 4, 21, 11, 1], [97, 98, 208, 15, 7, 9, 78, 9, 12, 3, 1], [7, 43, 26, 48, 97, 98, 208, 41, 253, 4, 97, 98, 977, 10, 628, 11, 1], [97, 98, 208, 15, 7, 43, 44, 624, 24, 978, 34, 6, 1], [251, 10, 209, 64, 9, 18, 238, 10, 199, 8, 13, 12, 3, 1], [342, 5, 93, 8, 6, 4, 18, 979, 446, 89, 343, 5, 3, 634, 252], [30, 344, 980, 23, 199, 4, 8, 13, 12, 28, 1], [251, 10, 209, 64, 33, 199, 8, 13, 12, 3, 1], [209, 30, 18, 10, 15, 93, 8, 2, 17, 16], [251, 10, 209, 30, 18, 93, 8, 2, 17, 16], [342, 15, 93, 8, 2, 17, 16], [342, 5, 93, 8, 13, 18, 75, 107, 48, 118, 981, 5, 982, 1], [209, 30, 18, 93, 8, 10, 142, 31, 36], [209, 30, 18, 199, 8, 13, 12, 28, 1], [342, 5, 199, 8, 13, 18, 75, 107, 48, 29, 68, 2, 144, 16], [209, 30, 18, 15, 199, 8, 13, 12, 3, 1], [209, 30, 251, 446, 89, 93, 8, 30, 446, 345, 635, 6, 636, 983, 161, 24, 984, 6, 4, 985], [146, 126, 94, 637, 15, 93, 8, 13, 12, 3, 1], [637, 15, 93, 8, 13, 12, 3, 1], [71, 111, 50, 9, 186, 6, 23, 93, 8, 2, 106, 17, 16], [88, 2, 986, 7, 10, 87, 6, 54, 987, 76, 77, 638, 447, 6, 20, 988, 10, 8, 17, 16], [71, 111, 7, 254, 124, 9, 6, 54, 153, 165, 93, 8, 2, 17, 16], [71, 111, 50, 9, 255, 186, 30, 18, 93, 8, 10, 142, 31, 36], [50, 9, 186, 6, 23, 93, 8, 2, 106, 17, 16], [7, 254, 9, 447, 30, 18, 254, 96, 113, 10, 199, 254, 13, 12, 3, 256], [7, 254, 124, 9, 6, 54, 153, 19, 18, 93, 8, 2, 17, 16], [7, 254, 124, 9, 447, 30, 18, 93, 8, 2, 17, 16], [7, 50, 9, 255, 186, 13, 18, 69, 8, 210, 41, 108, 9, 47, 9, 12, 3, 256], [50, 9, 255, 186, 30, 18, 93, 8, 9, 13, 12, 3, 1], [255, 186, 30, 18, 93, 8, 9, 13, 12, 3, 1], [7, 50, 9, 255, 186, 13, 18, 989, 10, 639, 50, 91, 161, 24, 21, 990, 34, 252], [991, 992, 993, 251, 10, 255, 186, 6, 23, 93, 8, 13, 12, 3, 1], [75, 71, 211, 5, 434, 6, 158, 31, 36], [75, 71, 211, 124, 994, 4, 433, 3, 1], [71, 30, 282, 2, 50, 9, 186, 6, 4, 18, 75, 995, 5, 640, 1], [448, 449, 206, 10, 75, 71, 597, 2, 996, 12, 3, 1], [75, 27, 71, 2, 11, 1], [75, 27, 71, 2, 11, 101, 15, 6, 1], [75, 71, 211, 41, 181, 2, 11, 101, 450, 4, 35, 248, 9, 611, 34, 6, 1], [75, 71, 2, 3, 158, 31, 36], [448, 449, 206, 10, 75, 71, 2, 308, 12, 3, 1], [448, 449, 206, 10, 75, 71, 2, 3, 1], [639, 7, 50, 37, 75, 71, 211, 5, 997, 11, 1], [75, 71, 15, 434, 6, 1], [75, 71, 211, 89, 127, 122, 31, 30, 169, 2, 3, 165, 176, 10, 641, 34, 6, 1], [75, 71, 19, 28, 1], [75, 71, 211, 4, 35, 998, 27, 338, 11, 1], [75, 71, 10, 142, 159, 61, 123], [75, 71, 211, 89, 127, 122, 31, 30, 169, 2, 3, 165, 176, 10, 641, 34, 6, 1], [118, 19, 21, 40, 11, 1], [118, 19, 346, 22, 40, 11, 1], [29, 71, 44, 118, 2, 3, 1], [118, 37, 451, 999, 4, 1000, 19, 35, 85, 78, 2, 3, 1], [84, 23, 118, 2, 3, 1], [118, 67, 19, 1001, 347, 1002, 1], [118, 40, 44, 635, 11, 4, 1003, 232, 4, 120, 52], [118, 19, 35, 1004, 22, 40, 11, 1], [118, 40, 233, 2, 31, 36], [62, 118, 2, 3, 1], [118, 2, 3, 348, 187, 22, 40, 11, 1], [118, 19, 346, 52, 187, 52], [118, 40, 233, 10, 142, 147, 1005, 46, 61, 1006, 12, 3, 1], [118, 50, 10, 142, 159, 61, 123], [642, 643, 10, 8, 30, 18, 148, 104, 24, 1007, 6, 1], [148, 104, 4, 642, 643, 10, 8, 30, 18, 10, 51, 40, 11, 1], [148, 104, 24, 58, 101, 145, 10, 410, 34, 13, 169, 2, 3, 1], [148, 104, 10, 128, 17, 30, 644, 5, 21, 11, 1], [148, 104, 37, 1008, 9, 128, 34, 6, 1], [148, 104, 10, 142, 332, 141, 20, 45, 182], [1009, 10, 142, 159, 61, 123], [148, 104, 128, 644, 5, 21, 11, 1], [148, 104, 1010, 30, 349, 9, 141, 20, 45, 182], [148, 104, 4, 1011, 345, 317, 1], [148, 104, 4, 8, 44, 345, 317, 1], [148, 104, 4, 21, 40, 11, 4, 439], [146, 126, 94, 4, 99, 49, 39, 77, 19, 149, 52], [146, 126, 94, 5, 99, 48, 212, 94, 89, 135, 645, 31, 36], [146, 126, 94, 149, 2, 21, 11, 1], [146, 126, 94, 37, 451, 646, 11, 4, 452, 1012, 19, 35, 85, 2, 3, 1], [146, 126, 94, 149, 2, 31, 36], [146, 126, 94, 5, 99, 89, 135, 16], [146, 126, 94, 4, 99, 48, 212, 94, 52], [146, 126, 94, 5, 99, 48, 212, 94, 89, 135, 16], [99, 48, 212, 94, 89, 135, 16], [99, 48, 212, 94, 149, 39, 135, 16], [99, 48, 212, 94, 89, 149, 2, 77, 1013], [146, 126, 94, 149, 19, 99, 48, 212, 94, 149, 39, 77, 1], [146, 126, 94, 149, 19, 99, 48, 212, 94, 149, 39, 135, 16], [99, 49, 39, 135, 6, 57, 146, 126, 94, 24, 40, 6, 4, 439], [38, 70, 5, 213, 2, 59, 11, 4, 18, 21, 34, 6, 1], [167, 38, 70, 213, 2, 106, 17, 647, 1014, 10, 68, 13, 12, 3, 1], [167, 38, 70, 213, 2, 106, 17, 647, 29, 453, 206, 10, 15, 68, 13, 12, 3, 1], [188, 26, 48, 18, 35, 29, 24, 68, 34, 6, 1], [350, 68, 29, 5, 31, 36], [350, 38, 70, 213, 2, 59, 11, 4, 18, 21, 34, 6, 1], [350, 38, 70, 24, 257, 13, 12, 3, 4, 29, 5, 3, 1], [188, 26, 38, 70, 5, 213, 2, 59, 11, 4, 18, 21, 34, 6, 1], [167, 411, 1015, 37, 38, 230, 70, 213, 2, 648, 257, 13, 12, 3, 4, 29, 5, 3, 1], [188, 26, 38, 70, 5, 213, 2, 59, 11, 23, 8, 13, 12, 28, 1], [167, 188, 625, 2, 20, 215, 309, 649, 10, 650, 13, 230, 1016, 1017, 650, 124, 96, 2, 59, 11, 576, 38, 70, 5, 433, 213, 2, 59, 11, 4, 85, 77, 182, 1018, 188, 70, 93, 107, 70, 91, 70, 26, 35, 1019, 1020, 23, 1021], [350, 38, 70, 24, 351, 13, 12, 3, 4, 29, 5, 3, 1], [229, 188, 26, 38, 70, 24, 351, 13, 12, 3, 4, 29, 5, 3, 1], [229, 188, 26, 38, 70, 24, 257, 13, 12, 3, 4, 29, 5, 3, 1], [651, 188, 26, 38, 70, 24, 351, 13, 12, 3, 4, 29, 5, 3, 1], [651, 188, 26, 38, 70, 24, 257, 13, 12, 3, 4, 29, 5, 3, 1], [202, 334, 26, 22, 352, 5, 28, 4, 18, 21, 6, 1], [202, 10, 334, 26, 48, 18, 68, 34, 6, 4, 29, 5, 31, 36], [143, 41, 18, 68, 34, 6, 4, 29, 5, 31, 36], [202, 334, 26, 22, 649, 5, 652, 54, 153, 56, 352, 5, 28, 33, 653, 50, 10, 21, 654, 34, 252, 1022, 18, 10, 4, 289, 353, 19, 1023, 27, 654, 6, 20, 653, 50, 10, 1024, 75, 34, 6, 1], [143, 41, 18, 68, 34, 6, 4, 29, 5, 21, 11, 1], [202, 82, 48, 18, 29, 24, 21, 454, 34, 6, 1], [202, 10, 330, 20, 3, 33, 289, 353, 9, 229, 27, 1025, 11, 1], [143, 68, 29, 5, 74], [143, 19, 289, 353, 9, 21, 454, 34, 6, 1], [143, 19, 289, 353, 9, 21, 655, 34, 6, 1], [143, 2, 68, 34, 6, 4, 29, 5, 74], [143, 68, 29, 5, 31, 36], [143, 68, 29, 5, 21, 11, 1], [143, 19, 258, 70, 5, 28, 33, 21, 34, 6, 1], [202, 126, 107, 70, 5, 28, 33, 103, 126, 107, 70, 24, 68, 90, 11, 1], [143, 29, 5, 31, 36], [143, 29, 5, 21, 11, 1], [202, 82, 48, 18, 29, 5, 21, 11, 1], [189, 203, 171, 204, 103, 41, 18, 259, 29, 5, 120, 52], [354, 125, 103, 41, 18, 68, 34, 6, 4, 29, 5, 31, 36], [68, 29, 26, 10, 38, 70, 258, 70, 5, 3, 1026, 189, 203, 171, 204, 103, 41, 18, 10, 4, 38, 70, 89, 258, 70, 1027, 57, 28, 20, 399, 355, 328, 352, 51, 3, 182, 38, 70, 89, 258, 70, 68, 9, 355, 328, 352, 68, 27, 257, 5, 17, 645, 1028, 182], [290, 103, 41, 18, 355, 51, 68, 6, 23, 11, 1], [290, 103, 41, 18, 355, 125, 10, 75, 27, 68, 34, 6, 4, 29, 5, 3, 1], [189, 37, 229, 24, 1029, 33, 35, 29, 24, 68, 34, 6, 158, 1030, 109, 182], [189, 203, 171, 204, 103, 144, 30, 29, 5, 74], [189, 203, 171, 204, 103, 259, 29, 5, 31, 36], [189, 203, 171, 204, 103, 68, 29, 5, 31, 36], [189, 203, 171, 204, 144, 30, 29, 5, 31, 36], [354, 125, 103, 259, 29, 5, 74], [354, 125, 103, 68, 29, 5, 74], [354, 125, 103, 144, 30, 29, 5, 74], [290, 103, 259, 29, 5, 74], [290, 103, 68, 29, 5, 74], [290, 103, 144, 30, 29, 5, 74], [189, 103, 41, 18, 259, 29, 5, 120, 52], [189, 229, 41, 18, 259, 29, 5, 120, 52], [152, 656, 234, 235, 15, 236, 9, 190, 12, 3, 83], [234, 235, 15, 236, 9, 190, 12, 3, 83], [234, 235, 15, 150, 9, 190, 12, 3, 83], [234, 235, 15, 236, 9, 291, 33, 657, 5, 28, 83], [234, 235, 15, 150, 9, 291, 33, 657, 5, 28, 83], [234, 235, 15, 236, 9, 291, 33, 1031, 2, 28, 83], [148, 9, 1032, 162, 1], [152, 656, 234, 235, 15, 150, 9, 190, 12, 3, 83], [191, 455, 2, 28, 4, 179, 162, 1], [236, 9, 291, 33, 191, 455, 2, 144, 16], [191, 66, 5, 278, 356, 162, 1], [191, 66, 5, 278, 356, 190, 12, 1033, 1], [191, 66, 5, 278, 356, 150, 9, 190, 12, 3, 1], [191, 66, 5, 278, 356, 236, 9, 190, 12, 3, 1], [191, 1034, 5, 28, 4, 179, 162, 1], [191, 455, 2, 28, 33, 236, 9, 190, 12, 3, 1], [7, 145, 10, 147, 73, 6, 23, 72, 19, 85, 2, 3, 83], [357, 10, 456, 46, 1035, 165, 50, 10, 291, 101, 1036, 30, 349, 2, 3, 1], [50, 145, 10, 147, 73, 6, 23, 72, 19, 85, 2, 3, 83], [7, 178, 145, 10, 147, 73, 6, 23, 72, 19, 85, 2, 3, 83], [150, 178, 145, 10, 147, 73, 6, 23, 72, 19, 85, 2, 3, 83], [178, 145, 10, 147, 73, 6, 23, 72, 19, 85, 2, 3, 83], [147, 73, 6, 23, 72, 19, 85, 2, 3, 83], [147, 73, 6, 23, 72, 19, 85, 2, 3, 1], [172, 191, 66, 658, 147, 431, 73, 24, 1037, 6, 33, 35, 415, 147, 73, 6, 23, 72, 83], [35, 149, 9, 147, 73, 6, 23, 72, 83], [35, 149, 9, 147, 73, 6, 23, 72, 1], [357, 10, 35, 73, 24, 6, 23, 72, 1], [357, 10, 35, 73, 24, 6, 23, 72, 83], [357, 10, 73, 6, 23, 72, 19, 149, 2, 3, 1], [178, 145, 10, 73, 6, 23, 72, 19, 85, 9, 423, 46, 61, 123], [292, 79, 2, 21, 11, 1], [457, 2, 91, 10, 458, 9, 659, 1], [7, 1038, 44, 106, 660, 2, 3, 1], [292, 44, 21, 11, 1], [457, 79, 2, 31, 36], [7, 37, 41, 192, 79, 19, 21, 11, 1], [457, 2, 260, 16], [293, 79, 2, 31, 36], [293, 79, 2, 3, 1], [1039, 44, 21, 11, 1], [1040, 44, 21, 11, 1], [293, 2, 661, 19, 18, 21, 11, 1], [192, 2, 1041, 30, 18, 21, 11, 1], [662, 192, 34, 6, 1], [613, 150, 10, 87, 34, 6, 1], [1042, 292, 2, 17, 30, 18, 5, 3, 1], [137, 24, 128, 13, 12, 3, 1], [1043, 5, 3, 1], [137, 4, 347, 128, 13, 12, 3, 1], [137, 4, 633, 2, 11, 1], [137, 663, 2, 21, 11, 1], [1044, 664, 5, 3, 9, 156, 128, 13, 12, 3, 4, 137, 5, 3, 83], [292, 1045, 24, 68, 6, 23, 192, 22, 1046, 11, 1], [358, 664, 5, 3, 9, 156, 128, 13, 12, 3, 4, 137, 5, 3, 1], [358, 665, 2, 221, 9, 156, 21, 34, 6, 1], [358, 665, 2, 640, 23, 1047, 292, 34, 6, 1], [137, 211, 5, 3, 1], [7, 96, 26, 137, 24, 128, 13, 12, 3, 1], [137, 89, 127, 294, 663, 2, 31, 36], [7, 96, 26, 137, 24, 246, 344, 128, 13, 12, 3, 1], [30, 247, 10, 246, 344, 137, 24, 128, 13, 12, 3, 1], [30, 247, 10, 246, 344, 1048, 137, 24, 128, 13, 12, 3, 1], [137, 24, 128, 13, 12, 3, 4, 1049, 2, 21, 11, 1], [7, 91, 79, 19, 120, 52], [7, 9, 91, 6, 101, 1050, 35, 85, 2, 260, 16], [91, 79, 9, 21, 1051, 13, 12, 3, 1], [7, 91, 24, 1052, 192, 79, 2, 31, 36], [293, 2, 661, 19, 18, 10, 15, 7, 9, 91, 13, 12, 3, 1], [662, 192, 44, 10, 51, 7, 9, 91, 13, 12, 3, 1], [7, 91, 44, 192, 1053, 2, 666, 11, 1], [295, 25, 163, 667, 41, 258, 2, 91, 10, 458, 9, 659, 1], [21, 7, 91, 24, 13, 12, 3, 1], [7, 91, 4, 21, 6, 1], [7, 91, 10, 666, 11, 4, 232, 5, 31, 36], [91, 79, 9, 1054, 6, 54, 1055, 9, 18, 93, 43, 2, 17, 16], [7, 9, 91, 6, 101, 450, 4, 21, 34, 6, 1], [91, 79, 19, 7, 50, 1056, 359], [7, 95, 63, 4, 21, 11, 1], [62, 7, 9, 40, 6, 57, 11, 4, 18, 7, 95, 4, 176, 52], [62, 7, 19, 296, 37, 40, 11, 1], [62, 7, 95, 4, 176, 52], [187, 7, 95, 4, 176, 52], [187, 7, 63, 19, 176, 52], [459, 95, 37, 51, 7, 9, 78, 9, 12, 3, 1], [459, 63, 37, 51, 7, 9, 78, 9, 12, 3, 1], [7, 95, 5, 3, 4, 63, 2, 31, 36], [7, 63, 19, 21, 11, 1], [62, 7, 95, 4, 21, 11, 1], [62, 7, 63, 19, 21, 11, 1], [187, 7, 95, 4, 21, 11, 1], [187, 7, 63, 19, 21, 11, 1], [261, 10, 15, 7, 95, 5, 3, 1], [7, 95, 360, 5, 31, 36], [187, 7, 95, 360, 5, 31, 36], [8, 63, 7, 95, 9, 129, 13, 12, 3, 1], [63, 9, 129, 13, 12, 3, 1], [95, 24, 129, 13, 12, 3, 1], [29, 453, 206, 10, 8, 63, 9, 129, 13, 12, 3, 1], [29, 453, 206, 10, 7, 95, 24, 129, 13, 12, 3, 1], [55, 95, 37, 15, 7, 2, 17, 16], [29, 1057, 44, 454, 30, 63, 37, 51, 7, 2, 17, 16], [1058, 668, 30, 63, 37, 51, 7, 2, 17, 16], [71, 294, 63, 37, 51, 7, 2, 17, 16], [7, 95, 24, 129, 13, 12, 3, 1], [404, 24, 5, 57, 11, 4, 18, 7, 95, 24, 129, 13, 12, 3, 1], [29, 68, 44, 8, 95, 24, 1059, 655, 64, 33, 129, 13, 12, 3, 1], [8, 95, 24, 129, 13, 12, 3, 1], [7, 63, 129, 9, 124, 6, 669, 21, 34, 6, 1], [7, 63, 129, 2, 17, 30, 18, 5, 3, 1], [7, 63, 129, 124, 2, 1060, 11, 4, 96, 2, 3, 1], [7, 63, 129, 9, 6, 23, 1061, 43, 6, 244, 64, 609, 50, 39, 4, 55, 7, 50, 9, 43, 34, 6, 1], [289, 71, 294, 95, 24, 129, 13, 12, 3, 1], [62, 7, 15, 40, 6, 1], [84, 23, 7, 50, 52], [670, 5, 59, 652, 15, 62, 22, 40, 11, 1], [62, 22, 40, 11, 4, 460, 15, 3, 1], [461, 62, 22, 40, 11, 1], [62, 22, 51, 40, 11, 1], [172, 62, 22, 51, 40, 11, 1], [84, 23, 7, 51, 40, 6, 1], [409, 44, 62, 22, 40, 6, 1], [670, 89, 1062, 62, 22, 40, 11, 1], [62, 84, 23, 50, 9, 138, 6, 1], [167, 7, 338, 19, 62, 39, 84, 23, 35, 233, 22, 361, 14, 54, 1], [7, 50, 26, 180, 23, 84, 23, 79, 2, 21, 11, 1], [7, 50, 26, 461, 62, 22, 40, 11, 1], [7, 50, 26, 461, 84, 23, 22, 40, 11, 1], [346, 187, 50, 9, 138, 6, 1], [346, 187, 9, 138, 6, 1], [172, 62, 22, 40, 1063, 358, 2, 198, 6, 4, 67, 180, 10, 87, 13, 12, 3, 1], [84, 23, 44, 10, 15, 7, 67, 2, 135, 16], [84, 23, 44, 10, 15, 7, 67, 2, 462, 3, 1], [84, 23, 44, 67, 671, 2, 28, 1], [84, 23, 44, 297, 226, 57, 43, 17, 16], [62, 7, 37, 4, 35, 233, 22, 150, 2, 361, 14, 54, 1], [84, 23, 44, 198, 6, 4, 7, 67, 180, 10, 87, 13, 12, 3, 1], [84, 23, 44, 10, 15, 180, 23, 7, 39, 135, 6, 57, 40, 11, 1], [84, 23, 44, 10, 15, 180, 23, 7, 39, 67, 2, 135, 16], [84, 23, 44, 62, 7, 39, 67, 2, 359], [84, 23, 7, 19, 297, 226, 57, 78, 9, 12, 3, 1], [84, 23, 7, 19, 198, 6, 4, 67, 10, 78, 9, 12, 3, 1], [172, 62, 22, 7, 2, 40, 11, 4, 18, 10, 297, 226, 57, 78, 9, 12, 3, 1], [84, 23, 7, 19, 672, 67, 2, 28, 1], [84, 23, 7, 15, 1064, 22, 40, 11, 1], [84, 23, 7, 19, 198, 6, 4, 67, 10, 87, 90, 11, 1], [84, 23, 7, 19, 198, 6, 4, 67, 180, 10, 87, 90, 11, 1], [84, 23, 7, 19, 67, 671, 2, 28, 1], [172, 62, 22, 7, 9, 40, 6, 4, 18, 10, 15, 135, 30, 67, 10, 7, 9, 40, 6, 1], [84, 23, 7, 19, 297, 226, 57, 43, 17, 16], [7, 43, 26, 110, 55, 7, 9, 1065, 10, 43, 6, 20, 45, 19, 18, 21, 34, 6, 1], [55, 50, 39, 227, 78, 9, 12, 3, 1], [67, 2, 337, 54, 153, 673, 55, 7, 39, 227, 362, 11, 1], [138, 7, 2, 3, 9, 18, 7, 1066, 1067, 2, 17, 16], [55, 7, 50, 39, 138, 13, 12, 3, 1], [55, 7, 39, 138, 122, 7, 43, 2, 17, 16], [674, 675, 39, 138, 13, 12, 3, 1], [55, 164, 8, 39, 138, 13, 12, 3, 1], [7, 43, 26, 110, 138, 7, 22, 1068, 293, 2, 1069, 46, 54, 23, 21, 11, 1], [55, 7, 39, 227, 362, 11, 1], [67, 2, 337, 54, 153, 673, 55, 7, 50, 39, 138, 90, 162, 1], [55, 240, 39, 227, 362, 11, 1], [55, 240, 39, 138, 13, 12, 3, 1], [55, 7, 39, 138, 90, 11, 1], [138, 7, 9, 43, 13, 18, 7, 258, 10, 35, 458, 9, 1070], [674, 675, 39, 227, 362, 11, 1], [240, 10, 330, 224, 7, 10, 87, 90, 11, 1], [55, 7, 39, 227, 40, 90, 11, 1], [112, 22, 15, 43, 13, 12, 3, 1], [7, 9, 112, 22, 43, 90, 11, 1], [32, 676, 112, 22, 7, 9, 190, 12, 3, 1], [112, 22, 1071, 192, 2, 17, 16], [32, 351, 463, 22, 43, 90, 11, 1], [1072, 27, 15, 43, 13, 12, 3, 1], [32, 2, 3, 201, 43, 17, 16], [32, 125, 41, 1073, 27, 15, 43, 13, 12, 3, 1], [65, 47, 19, 32, 22, 51, 43, 34, 6, 1], [65, 47, 19, 32, 125, 10, 15, 43, 17, 16], [112, 22, 43, 90, 11, 1], [112, 22, 7, 10, 87, 90, 162, 1], [192, 19, 112, 22, 90, 11, 1], [7, 19, 112, 22, 363, 19, 32, 22, 40, 90, 11, 1], [32, 22, 51, 1074, 6, 1], [112, 22, 43, 90, 162, 1], [7, 43, 363, 345, 32, 22, 40, 34, 6, 1], [571, 22, 43, 90, 162, 1], [363, 9, 112, 22, 40, 13, 12, 4, 28, 1], [463, 22, 43, 90, 162, 1], [1075, 583, 32, 9, 139, 46, 61, 1], [7, 96, 184, 32, 9, 262, 27, 139, 46, 61, 1], [32, 9, 139, 46, 61, 1], [167, 32, 22, 363, 9, 1076, 6, 101, 648, 32, 9, 8, 47, 9, 12, 3, 83], [32, 139, 5, 17, 16], [154, 10, 152, 5, 28, 33, 32, 134, 152, 24, 139, 13, 12, 3, 1], [7, 96, 184, 32, 9, 139, 46, 61, 1], [154, 10, 152, 5, 28, 4, 18, 21, 34, 6, 1, 1077, 114, 6, 1], [7, 96, 184, 32, 139, 5, 17, 16], [152, 5, 28, 33, 463, 22, 15, 150, 10, 87, 13, 12, 3, 1], [464, 465, 32, 9, 139, 46, 61, 1], [464, 465, 65, 11, 4, 32, 2, 3, 1], [7, 82, 8, 169, 10, 32, 8, 15, 3, 1], [7, 44, 65, 11, 4, 32, 2, 3, 1], [32, 139, 124, 19, 21, 6, 23, 11, 1], [7, 44, 32, 9, 139, 46, 61, 1], [464, 465, 32, 9, 8, 46, 61, 1], [139, 17, 30, 32, 2, 3, 1], [7, 125, 107, 48, 1078, 27, 32, 128, 2, 17, 16], [32, 9, 1079, 12, 3, 1], [7, 96, 65, 6, 4, 32, 263, 2, 31, 36], [32, 263, 2, 677, 678, 11, 1], [7, 96, 32, 9, 65, 46, 1080, 78, 155, 33, 263, 2, 21, 11, 1], [7, 2, 679, 294, 111, 32, 1081, 2, 144, 16], [32, 139, 26, 1082, 18, 257, 13, 12, 3, 4, 340, 2, 3, 1], [65, 11, 4, 32, 466, 2, 21, 11, 1], [65, 11, 4, 32, 467, 2, 21, 11, 1], [32, 139, 96, 19, 21, 11, 1], [65, 11, 4, 152, 466, 2, 21, 11, 1], [7, 44, 65, 11, 4, 32, 41, 467, 19, 21, 11, 1], [152, 263, 2, 21, 11, 1], [32, 263, 10, 142, 332, 159, 61, 123], [32, 466, 19, 72, 1], [32, 467, 2, 21, 11, 158, 31, 36], [1083, 263, 2, 21, 11, 1], [1084, 19, 246, 1085, 52], [32, 1086, 1087, 2, 347, 3, 1], [32, 1088, 2, 347, 11, 1], [1089, 7, 50, 125, 10, 287, 27, 163, 134, 295, 5, 3, 1], [163, 2, 3, 1], [295, 5, 3, 1], [7, 96, 26, 104, 298, 163, 19, 21, 40, 11, 1], [468, 1090, 2, 3, 1], [7, 96, 184, 41, 104, 298, 163, 37, 4, 672, 67, 59, 10, 452, 24, 680, 634, 6, 1, 681, 283, 676, 680, 12, 3, 1], [150, 51, 1091, 11, 1], [7, 96, 26, 104, 298, 163, 37, 4, 1092, 9, 128, 13, 12, 3, 1], [150, 111, 66, 5, 1093, 13, 12, 3, 4, 340, 2, 3, 1], [468, 10, 163, 2, 3, 1], [7, 50, 9, 40, 6, 425, 163, 9, 317, 101, 15, 6, 1], [468, 10, 295, 5, 3, 1], [150, 125, 10, 163, 134, 295, 5, 3, 1], [7, 96, 184, 41, 104, 298, 163, 37, 4, 35, 1094, 41, 452, 5, 646, 11, 1], [150, 125, 10, 104, 5, 3, 1], [7, 96, 184, 41, 104, 298, 163, 19, 1095, 107, 22, 40, 11, 1, 681, 459, 1096, 10, 1097, 107, 22, 40, 11, 1], [1098, 679, 44, 104, 5, 3, 1], [7, 43, 125, 10, 75, 107, 22, 34, 6, 4, 169, 2, 3, 1], [7, 43, 26, 38, 2, 1099, 294, 18, 21, 11, 1], [38, 264, 2, 130, 33, 162, 1], [38, 264, 2, 130, 33, 21, 34, 6, 1], [38, 333, 658, 8, 64, 33, 38, 2, 130, 370], [38, 264, 2, 130, 33, 7, 10, 242, 87, 17, 16], [38, 264, 2, 130, 33, 91, 5, 17, 16], [38, 264, 2, 130, 33, 265, 299, 682, 5, 17, 16], [38, 2, 431, 336, 14, 469, 33, 135, 6, 57, 49, 53, 24, 47, 9, 12, 3, 83], [38, 264, 2, 130, 33, 49, 127, 8, 9, 47, 9, 12, 3, 1], [38, 2, 130, 33, 162, 1], [38, 2, 130, 33, 106, 660, 19, 28, 1], [38, 2, 130, 33, 21, 34, 6, 1], [38, 2, 336, 14, 469, 33, 21, 34, 6, 1], [38, 2, 130, 33, 7, 10, 242, 87, 17, 16], [38, 2, 130, 33, 91, 5, 17, 16], [38, 2, 130, 33, 49, 53, 5, 17, 16], [38, 2, 130, 33, 38, 111, 10, 49, 53, 24, 47, 9, 12, 3, 1], [38, 2, 336, 14, 469, 33, 49, 127, 8, 9, 47, 9, 12, 3, 1], [99, 10, 2, 300, 301, 19, 262, 52], [683, 262, 27, 43, 13, 12, 3, 1], [364, 365, 366, 281, 27, 193, 11, 1], [7, 253, 193, 89, 127, 6, 636, 1100, 134, 341, 9, 47, 9, 12, 3, 4, 18, 5, 3, 1], [364, 365, 366, 281, 27, 367, 9, 8, 47, 9, 12, 3, 1], [7, 253, 4, 21, 193, 6, 1], [193, 5, 144, 16], [253, 5, 470, 52], [1101, 1102, 2, 470, 52], [364, 365, 366, 281, 5, 28, 4, 18, 10, 4, 1103, 27, 40, 13, 12, 3, 1], [683, 262, 52], [7, 253, 193, 111, 75, 107, 48, 367, 2, 684, 13, 12, 3, 4, 18, 5, 3, 1], [193, 5, 3, 1], [193, 24, 59, 90, 11, 1], [367, 2, 1104, 28, 1], [193, 34, 6, 4, 1105, 2, 470, 52], [262, 27, 78, 9, 12, 3, 1], [364, 365, 366, 281, 5, 3, 4, 18, 10, 51, 262, 27, 43, 17, 16], [49, 53, 127, 35, 8, 2, 3, 1], [49, 39, 127, 122, 35, 8, 78, 2, 3, 1], [69, 127, 35, 8, 2, 3, 1], [49, 127, 8, 10, 142, 31, 36], [49, 53, 125, 10, 15, 49, 127, 8, 2, 3, 1], [69, 685, 2, 3, 1], [69, 127, 122, 471, 9, 47, 9, 12, 3, 83], [69, 1106, 5, 3, 1], [69, 1107, 5, 3, 1], [1108, 1109, 2, 3, 1], [49, 53, 8, 2, 120, 52], [69, 8, 2, 120, 52], [7, 82, 181, 22, 69, 685, 15, 40, 6, 1], [69, 471, 15, 17, 16], [69, 9, 450, 35, 8, 2, 3, 1], [69, 8, 2, 31, 36], [69, 8, 349, 2, 31, 36], [7, 91, 111, 10, 69, 8, 686, 5, 3, 1], [687, 8, 349, 2, 31, 36], [687, 127, 8, 2, 3, 1], [49, 44, 63, 283, 2, 3, 1], [49, 53, 5, 17, 30, 63, 2, 462, 3, 1], [49, 53, 5, 11, 4, 63, 2, 31, 36], [49, 53, 5, 11, 4, 63, 2, 21, 11, 1], [265, 299, 472, 63, 2, 21, 11, 1], [265, 299, 472, 668, 11, 4, 63, 2, 3, 1], [265, 299, 472, 198, 6, 4, 63, 22, 396, 12, 3, 1], [296, 2, 688, 261, 37, 88, 6, 20, 45, 638, 265, 299, 682, 5, 17, 16], [296, 27, 49, 53, 5, 17, 16], [49, 53, 4, 296, 51, 17, 16], [49, 53, 63, 2, 21, 11, 1], [261, 22, 15, 49, 53, 5, 11, 1], [1110, 27, 15, 49, 53, 5, 17, 16], [49, 63, 2, 462, 3, 1], [49, 63, 2, 21, 11, 1], [49, 53, 63, 2, 31, 36], [49, 53, 44, 17, 30, 63, 2, 176, 52], [261, 10, 689, 6, 20, 3, 632, 296, 27, 69, 53, 5, 17, 252], [49, 53, 44, 261, 22, 15, 17, 16], [99, 49, 53, 44, 266, 107, 22, 58, 4, 232, 5, 3, 1], [35, 69, 53, 210, 2, 65, 11, 1], [49, 53, 10, 1111, 169, 2, 31, 36], [69, 53, 210, 37, 65, 11, 4, 69, 8, 686, 4, 120, 52], [49, 53, 5, 11, 4, 214, 78, 2, 31, 36], [69, 53, 210, 10, 87, 6, 669, 35, 116, 248, 2, 144, 16], [99, 27, 49, 53, 24, 46, 61, 1], [99, 127, 214, 2, 688, 55, 214, 22, 15, 49, 53, 5, 17, 16], [214, 37, 266, 107, 22, 58, 4, 232, 5, 31, 36], [69, 53, 210, 37, 4, 35, 214, 39, 53, 11, 1], [49, 53, 37, 260, 6, 57, 58, 4, 183, 2, 74], [7, 91, 111, 49, 53, 156, 266, 107, 22, 58, 4, 460, 2, 74], [49, 53, 44, 260, 6, 57, 58, 4, 183, 2, 74], [69, 53, 210, 37, 65, 6, 4, 69, 473, 2, 526, 120, 52], [49, 53, 44, 266, 107, 22, 58, 4, 232, 5, 74], [49, 53, 44, 260, 6, 57, 58, 4, 232, 5, 74], [214, 37, 260, 6, 57, 58, 4, 183, 2, 74], [214, 37, 265, 1112, 266, 107, 22, 58, 4, 183, 2, 3, 1, 3, 348, 74], [214, 37, 266, 107, 22, 58, 4, 232, 5, 74], [7, 82, 8, 108, 22, 65, 11, 4, 1113, 134, 341, 19, 35, 248, 37, 47, 9, 12, 3, 256], [8, 169, 2, 3, 1], [7, 82, 8, 2, 3, 1], [7, 82, 8, 108, 22, 65, 11, 4, 1114, 25, 456, 1115, 8, 19, 35, 233, 22, 361, 14, 54, 1], [108, 169, 2, 3, 1], [7, 82, 8, 108, 22, 65, 11, 4, 1116, 303, 1117, 134, 456, 8, 210, 19, 35, 233, 22, 338, 11, 1], [7, 82, 108, 22, 4, 35, 85, 78, 2, 3, 1], [7, 82, 8, 108, 22, 65, 11, 4, 473, 134, 1118, 667, 41, 210, 19, 35, 1119, 27, 40, 11, 1], [7, 82, 108, 2, 690, 691, 36], [7, 82, 22, 87, 6, 57, 11, 23, 35, 108, 2, 3, 158, 31, 36], [7, 82, 108, 169, 2, 690, 691, 36], [7, 82, 8, 108, 22, 65, 11, 4, 69, 473, 134, 471, 19, 35, 233, 22, 361, 14, 54, 1], [35, 108, 2, 3, 1], [7, 10, 1120, 8, 11, 4, 169, 78, 2, 3, 1], [35, 7, 82, 108, 2, 3, 1], [7, 82, 8, 108, 9, 47, 339, 35, 231, 24, 1121, 6, 1], [35, 7, 82, 108, 134, 8, 2, 3, 1], [7, 10, 87, 6, 23, 47, 9, 12, 3, 4, 8, 134, 108, 2, 3, 348, 141, 20, 45, 182], [62, 7, 44, 131, 5, 65, 11, 1], [131, 25, 151, 37, 4, 35, 1122, 1123, 9, 65, 6, 1, 75, 367, 2, 684, 6, 4, 18, 5, 3, 256], [62, 7, 44, 131, 10, 616, 8, 9, 47, 9, 12, 3, 1], [62, 7, 9, 47, 9, 156, 131, 25, 151, 24, 65, 47, 9, 12, 3, 158, 161, 24, 159, 61, 123], [62, 7, 44, 131, 25, 151, 5, 65, 11, 1], [131, 25, 151, 41, 360, 4, 176, 10, 3, 1, 7, 1124, 39, 41, 1125, 4, 677, 678, 52], [131, 25, 151, 37, 4, 35, 692, 24, 65, 6, 1, 692, 4, 297, 226, 57, 474, 13, 12, 3, 256], [62, 7, 44, 151, 5, 3, 1], [131, 5, 65, 11, 1], [261, 10, 689, 6, 4, 18, 151, 5, 65, 11, 1], [131, 25, 151, 5, 65, 11, 1], [131, 5, 65, 11, 158, 31, 36], [131, 25, 151, 4, 1126, 78, 2, 451, 474, 6, 4, 693, 39, 1127, 57, 360, 46, 3, 1], [131, 25, 151, 4, 35, 96, 184, 474, 13, 12, 3, 1, 1128, 340, 19, 21, 11, 1], [131, 25, 151, 37, 65, 6, 4, 1129, 41, 1130, 89, 693, 19, 21, 11, 1], [131, 25, 151, 1131, 4, 21, 193, 6, 1, 1132, 108, 2, 3, 256], [62, 7, 44, 287, 27, 1133, 25, 1134, 24, 8, 46, 61, 1], [62, 7, 156, 1135, 12, 3, 4, 593, 2, 3, 1], [164, 8, 1136, 7, 2, 197, 19, 179, 1137, 19, 76, 1138, 102, 5, 359], [55, 164, 8, 7, 39, 102, 5, 359], [2, 7, 50, 19, 55, 7, 50, 39, 267, 122, 35, 460, 37, 574, 1139, 16], [55, 164, 8, 7, 39, 302, 2, 74], [55, 164, 8, 7, 50, 39, 302, 2, 74], [55, 7, 50, 39, 267, 122, 2, 7, 50, 19, 35, 343, 5, 3, 1], [99, 10, 2, 300, 301, 41, 694, 2, 74], [2, 7, 50, 2, 55, 7, 50, 175, 1140, 2, 3, 348, 120, 52], [99, 10, 2, 300, 301, 41, 368, 30, 183, 2, 74], [99, 10, 2, 300, 301, 41, 55, 7, 50, 39, 267, 122, 368, 30, 183, 2, 31, 432], [55, 164, 8, 39, 55, 57, 74], [55, 164, 8, 39, 302, 2, 3, 1], [55, 164, 8, 39, 267, 122, 343, 5, 74], [55, 164, 8, 39, 267, 13, 156, 55, 183, 2, 120, 52], [55, 164, 8, 39, 267, 13, 156, 343, 5, 74], [99, 10, 2, 300, 301, 51, 41, 694, 2, 120, 485, 1141, 1142, 46, 61, 123], [55, 7, 39, 41, 302, 19, 120, 52], [55, 7, 39, 302, 2, 3, 1], [55, 164, 8, 7, 175, 368, 30, 183, 2, 3, 158, 31, 432], [55, 7, 175, 368, 30, 85, 2, 3, 1]]\n","[[  0   0   0 ... 475 476  14]\n"," [  0   0   0 ... 475 476  14]\n"," [  0   0   0 ... 478  25 170]\n"," ...\n"," [  0   0   0 ...   2   3   1]\n"," [  0   0   0 ... 158  31 432]\n"," [  0   0   0 ...   2   3   1]]\n"]}]},{"cell_type":"code","source":["# 전체 토큰의 수로 vocab_size 지정\n","vocab_size = len(word_dic)\n","\n","# fit_on_texts을 위에서 한번만 해도 되지만, vocab 사이즈를 확인하고 줄이거나 하는 시도를 할 수도 있기에 다시 수행\n","tokenizer.fit_on_texts(clean_train_questions)\n","# .texts_to_sequences : 토크나이즈 된 데이터를 가지고 모두 시퀀스로 변환\n","x_train= tokenizer.texts_to_sequences(clean_train_questions)\n","x_val =  tokenizer.texts_to_sequences(clean_test_questions)"],"metadata":{"id":"mOpw0-EwuJ9b","executionInfo":{"status":"ok","timestamp":1681797968226,"user_tz":-540,"elapsed":592,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zD-mj4RxBX7h","executionInfo":{"status":"ok","timestamp":1681798828261,"user_tz":-540,"elapsed":307,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"284f9451-1e0e-4b5b-e2f2-74b888e8f41d"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1107, 54)"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["x_val.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APXui3lMBZZQ","executionInfo":{"status":"ok","timestamp":1681798834371,"user_tz":-540,"elapsed":400,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"6b6b3152-8967-48af-f351-17c266343522"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(104, 19)"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["# 각 토큰과 인덱스로 구성된 딕셔너리 생성,\n","word_dic = tokenizer.word_index\n","\n","# <PAD> 는 0으로 추가\n","x_train = pad_sequences(x_train,maxlen=54)\n","x_val = pad_sequences(x_val,maxlen=54)"],"metadata":{"id":"sVHleK_muYl8","executionInfo":{"status":"ok","timestamp":1681798866632,"user_tz":-540,"elapsed":296,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["* 문장별 토큰수에 대해 탐색적 분석을 수행해 봅시다."],"metadata":{"id":"38FwBFRll05a"}},{"cell_type":"code","source":["len(x_train[0])"],"metadata":{"id":"6hwGM_0dubPR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797968227,"user_tz":-540,"elapsed":11,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"d3c6a76a-d311-48fa-f3c0-0f441ba681de"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["54"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["* 문장별 토큰이 가장 큰 것이 57개 입니다. "],"metadata":{"id":"nevMXd0vmj8H"}},{"cell_type":"markdown","source":["* 학습 입력을 위한 데이터 크기 맞추기\n","    * 문장이 짧기 때문에 MAX_SEQUENCE_LENGTH는 정하지 않아도 되지만,\n","    * 그러나 분포를 보고 적절하게 자릅시다.\n","    * 그리고 pad_sequences 함수를 이용하여 시퀀스데이터로 변환하기\n","* y는 train['type'] 와 test['type'] 입니다."],"metadata":{"id":"wQfbaY3HmIMj"}},{"cell_type":"code","source":["y_train = np.array(train_data['type'].to_list())\n","y_val = np.array(test_data['type'].to_list())"],"metadata":{"id":"8M2QWrD-ups7","executionInfo":{"status":"ok","timestamp":1681798700569,"user_tz":-540,"elapsed":298,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["x_train"],"metadata":{"id":"6nlnE9dju5Dh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681797968227,"user_tz":-540,"elapsed":10,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"33776e6c-8eae-4ccd-e693-c1b1abc1129e"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0,   0,   0, ..., 475, 476,  14],\n","       [  0,   0,   0, ..., 475, 476,  14],\n","       [  0,   0,   0, ..., 478,  25, 170],\n","       ...,\n","       [  0,   0,   0, ...,   2,   3,   1],\n","       [  0,   0,   0, ..., 158,  31, 432],\n","       [  0,   0,   0, ...,   2,   3,   1]], dtype=int32)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"id":"EMlo9o1VngW2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681798200216,"user_tz":-540,"elapsed":869,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"01525c55-3532-4d38-e541-431b0d8ab710"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1107, 54)"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["#### 2) 모델링\n","\n","* 토크나이징 한 데이터를 입력으로 받아 \n","* Embedding 레이어와 LSTM 레이어를 결합하여 이진 분류 모델링을 수행합니다."],"metadata":{"id":"WlBeVPA_6ePq"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import LSTM, Embedding"],"metadata":{"id":"8ilRqzlKScHr","executionInfo":{"status":"ok","timestamp":1681797986067,"user_tz":-540,"elapsed":304,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["x_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZ7eKIcI-W2d","executionInfo":{"status":"ok","timestamp":1681798639658,"user_tz":-540,"elapsed":473,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"72430df0-7148-4e70-b0fc-9fad50ab64f4"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0,   0,   0, ..., 475, 476,  14],\n","       [  0,   0,   0, ..., 475, 476,  14],\n","       [  0,   0,   0, ..., 478,  25, 170],\n","       ...,\n","       [  0,   0,   0, ...,   2,   3,   1],\n","       [  0,   0,   0, ..., 158,  31, 432],\n","       [  0,   0,   0, ...,   2,   3,   1]], dtype=int32)"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["y_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LgfGrFXAtaq","executionInfo":{"status":"ok","timestamp":1681798706667,"user_tz":-540,"elapsed":316,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"3b04b80b-667b-4cab-b4a4-84f2cec42629"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, ..., 1, 1, 1])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["x_train.shape,x_val.shape,y_train.shape,y_val.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFyCkQvfBEgG","executionInfo":{"status":"ok","timestamp":1681798880293,"user_tz":-540,"elapsed":312,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"799b5573-db03-458f-cda8-614f7aba6fb4"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1107, 54), (104, 54), (1107,), (104,))"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["max_words=30000\n","embedding_dim = 128\n","max_len = 54\n","\n","tf.keras.backend.clear_session()\n","\n","model = keras.models.Sequential()\n","\n","model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n","model.add(keras.layers.LSTM(16,activation='tanh' ) )\n","model.add(   keras.layers.Dense(1,activation='sigmoid') )\n","model.compile(  loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'] )\n","\n","# 학습\n","\n","history = model.fit( x_train , y_train  , epochs = 30 , validation_data=(x_val, y_val))"],"metadata":{"id":"0CaxzPmySjte","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681798983750,"user_tz":-540,"elapsed":101342,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"09065cd8-c27a-4dd3-9cc9-5c00e5ed462f"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","35/35 [==============================] - 7s 135ms/step - loss: 0.5785 - val_loss: 0.4226\n","Epoch 2/30\n","35/35 [==============================] - 3s 80ms/step - loss: 0.2207 - val_loss: 0.1540\n","Epoch 3/30\n","35/35 [==============================] - 4s 104ms/step - loss: 0.0798 - val_loss: 0.0758\n","Epoch 4/30\n","35/35 [==============================] - 5s 156ms/step - loss: 0.0364 - val_loss: 0.0457\n","Epoch 5/30\n","35/35 [==============================] - 7s 188ms/step - loss: 0.0208 - val_loss: 0.0314\n","Epoch 6/30\n","35/35 [==============================] - 3s 83ms/step - loss: 0.0135 - val_loss: 0.0258\n","Epoch 7/30\n","35/35 [==============================] - 3s 78ms/step - loss: 0.0097 - val_loss: 0.0197\n","Epoch 8/30\n","35/35 [==============================] - 3s 79ms/step - loss: 0.0073 - val_loss: 0.0173\n","Epoch 9/30\n","35/35 [==============================] - 4s 112ms/step - loss: 0.0057 - val_loss: 0.0140\n","Epoch 10/30\n","35/35 [==============================] - 3s 79ms/step - loss: 0.0047 - val_loss: 0.0122\n","Epoch 11/30\n","35/35 [==============================] - 3s 77ms/step - loss: 0.1383 - val_loss: 1.2084\n","Epoch 12/30\n","35/35 [==============================] - 3s 77ms/step - loss: 0.1613 - val_loss: 0.1064\n","Epoch 13/30\n","35/35 [==============================] - 3s 87ms/step - loss: 0.0294 - val_loss: 0.0792\n","Epoch 14/30\n","35/35 [==============================] - 4s 103ms/step - loss: 0.0163 - val_loss: 0.0689\n","Epoch 15/30\n","35/35 [==============================] - 3s 77ms/step - loss: 0.0115 - val_loss: 0.0623\n","Epoch 16/30\n","35/35 [==============================] - 3s 78ms/step - loss: 0.0087 - val_loss: 0.0584\n","Epoch 17/30\n","35/35 [==============================] - 3s 78ms/step - loss: 0.0070 - val_loss: 0.0552\n","Epoch 18/30\n","35/35 [==============================] - 3s 96ms/step - loss: 0.0058 - val_loss: 0.0526\n","Epoch 19/30\n","35/35 [==============================] - 3s 95ms/step - loss: 0.0049 - val_loss: 0.0508\n","Epoch 20/30\n","35/35 [==============================] - 3s 76ms/step - loss: 0.0042 - val_loss: 0.0491\n","Epoch 21/30\n","35/35 [==============================] - 3s 78ms/step - loss: 0.0037 - val_loss: 0.0478\n","Epoch 22/30\n","35/35 [==============================] - 3s 82ms/step - loss: 0.0033 - val_loss: 0.0466\n","Epoch 23/30\n","35/35 [==============================] - 4s 109ms/step - loss: 0.0030 - val_loss: 0.0454\n","Epoch 24/30\n","35/35 [==============================] - 3s 84ms/step - loss: 0.0027 - val_loss: 0.0447\n","Epoch 25/30\n","35/35 [==============================] - 3s 78ms/step - loss: 0.0024 - val_loss: 0.0438\n","Epoch 26/30\n","35/35 [==============================] - 3s 77ms/step - loss: 0.0022 - val_loss: 0.0431\n","Epoch 27/30\n","35/35 [==============================] - 3s 85ms/step - loss: 0.0020 - val_loss: 0.0423\n","Epoch 28/30\n","35/35 [==============================] - 4s 107ms/step - loss: 0.0019 - val_loss: 0.0418\n","Epoch 29/30\n","35/35 [==============================] - 3s 77ms/step - loss: 0.0018 - val_loss: 0.0413\n","Epoch 30/30\n","35/35 [==============================] - 3s 78ms/step - loss: 0.0016 - val_loss: 0.0407\n"]}]},{"cell_type":"code","source":["max(train_data['Q'].str.len())"],"metadata":{"id":"11jhI4eOSlOI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681804869741,"user_tz":-540,"elapsed":357,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"98f4898a-8d01-4717-9525-448fa29f1c76"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["127"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":[],"metadata":{"id":"qBeQBXhzSm0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KOhx2rzGFmiy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) 사전학습모델(Word2Vec)"],"metadata":{"id":"NqpAqObGdgbu"}},{"cell_type":"markdown","source":["* Pre-trained Word2Vec model\n","    * 이미 다운로드 받아서 제공되었습니다.\n","        * ko.bin, ko.tsv\n","    * 참고 사이트: https://github.com/Kyubyong/wordvectors\n","        * 모델 파일 다운로드 사이트: https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg\n","* 사전학습 모델을 로딩하고, \n","* train 데이터셋의 질문(Q)을 임베딩벡터로 만들어, 열(Column)로 추가합니다."],"metadata":{"id":"eJWm6cKwIZjf"}},{"cell_type":"markdown","source":["#### 1) 모델 로딩\n","* 사전 학습된 모델을 로딩 : gensim.models.Word2Vec.load()\n","* 로딩 후 벡터 크기를 조회합시다. .vector_size"],"metadata":{"id":"a5iIl1X57hef"}},{"cell_type":"code","source":["import gensim\n","pre_wv_model = gensim.models.Word2Vec.load(path + 'ko.bin')"],"metadata":{"id":"Wu2_fSn9Vjxx","executionInfo":{"status":"ok","timestamp":1681786490379,"user_tz":-540,"elapsed":1970,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["# 모델의 벡터크기 조회\n","pre_wv_model.vector_size"],"metadata":{"id":"Xmq6Fh5lWN0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681793397876,"user_tz":-540,"elapsed":319,"user":{"displayName":"김주환","userId":"01656125298467744437"}},"outputId":"9cbc30b1-1e25-47b0-ee45-0ecba505a611"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["200"]},"metadata":{},"execution_count":97}]},{"cell_type":"markdown","source":["#### 2) train 에 임베딩벡터 결과 저장\n","* get_sent_embedding 함수를 이용하여 train의 질문별 임베딩 결과를 저장합니다.\n","    * .apply(lambda .....) 를 활용하세요."],"metadata":{"id":"-reuWNEy7ogd"}},{"cell_type":"code","source":["train_data['embedded'] = train_data['Q'].apply(lambda x: get_sent_embedding(pre_wv_model, 200, tokenize('mecab',x)))"],"metadata":{"id":"I17jau4YWSD1","executionInfo":{"status":"ok","timestamp":1681796804954,"user_tz":-540,"elapsed":4639,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Sq3lQ798WVCF","executionInfo":{"status":"ok","timestamp":1681796906857,"user_tz":-540,"elapsed":447,"user":{"displayName":"김주환","userId":"01656125298467744437"}}},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축\n","* 아래 절차대로 수행하는 함수 만들기\n","    * input 질문 \n","    * 1단계 : 모델을 이용하여 type 0, 1로 분류\n","    * 2단계 : \n","        * train의 모든 Q와 input 문장의 임베딩 벡터간의 코사인 유사도 계산\n","        * 코사인 유사도가 가장 높은 Q를 선택\n","        * 선택한 Q의 intent에 맵핑된 답변 중 하나를 무작위로 선택"],"metadata":{"id":"rilxqf-cX4cN"}},{"cell_type":"markdown","source":["#### 1) 하나의 질문으로 테스트해보기"],"metadata":{"id":"4fUJIN1Sc0YJ"}},{"cell_type":"markdown","source":["* 선택된 질문과 답변"],"metadata":{"id":"8ozGvY19c4xd"}},{"cell_type":"code","source":[],"metadata":{"id":"Zo41eD7Vnd6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측을 위한 입력 형태로 변환\n","    * 학습을 위한 전처리 과정을 test 데이터에도 적용합니다. "],"metadata":{"id":"23bZbPscdA0n"}},{"cell_type":"code","source":[],"metadata":{"id":"qbrsu2szX6xL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 1단계 : type 분류"],"metadata":{"id":"GZzJEnFKdnPe"}},{"cell_type":"code","source":[],"metadata":{"id":"1U6yjYqTp3Nj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 2단계 : 질문에 대한 벡터 만들고 코사인 유사도 계산\n","    * Word2Vec 사전 학습 모델로 부터 벡터 만들기"],"metadata":{"id":"BTC3Rn9Tdtwo"}},{"cell_type":"code","source":[],"metadata":{"id":"dGpWRXtDqNti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* train의 질문 벡터들과 유사도 계산\n","    * Word2Vec 으로 만든 벡터들과 유사도 계산"],"metadata":{"id":"JOmz_Ib_d6-R"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","\n","\n"],"metadata":{"id":"EQchLoOJZOPN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 챗봇 함수 만들기\n","* 위 테스트 결과를 바탕으로 코드를 정리하고 함수로 생성합니다."],"metadata":{"id":"OPSCaDE5ro5F"}},{"cell_type":"code","source":["def get_answer2(question): \n","\n","\n","\n","\n","\n","    return"],"metadata":{"id":"vK9rO8w6rTRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7icv4nEt9aLQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"C8cu77Si9rzu"}},{"cell_type":"code","source":[],"metadata":{"id":"EJL_C-Mmtk9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.챗봇3\n","\n","* **세부요구사항**\n","    * 단계별 챗봇을 만들어 봅시다.\n","        * 1단계 : 챗봇2의 1단계 모델을 사용합니다.\n","        * 2단계 : \n","            * 각 type에 맞게, 사전학습된 Word2Vec 모델을 사용하여 임베딩 벡터(train)를 만들고\n","        * 3단계 : 챗봇 만들기\n","            * input 문장과 train 임베딩 벡터와 코사인 유사도 계산\n","            * 가장 유사도가 높은 질문의 intent 찾아\n","            * 해당 intent의 답변 중 무작위로 하나를 선정하여 답변하기"],"metadata":{"id":"TNn40nu96PI4"}},{"cell_type":"markdown","source":["### (1) 1단계 : type 분류 모델링\n","- LSTM : 3-(1) 모델을 그대로 사용합니다."],"metadata":{"id":"iK7s4ZCt6PI_"}},{"cell_type":"code","source":[],"metadata":{"id":"pOftkHKGAGXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) FastText 모델"],"metadata":{"id":"ebdw6-K9AR_V"}},{"cell_type":"markdown","source":["-  FastText 모델 학습을 위한 입력 포맷 2차원 리스트 형태 입니다.\n","  ```\n","  [['나', '는', '학생', '이다'], ['오늘', '은', '날씨', '가', '좋다']]\n","  ```"],"metadata":{"id":"9RQnEBcnAZNt"}},{"cell_type":"markdown","source":["- Word2Vec계열의 FastText를 학습하는 이유\n","  - n-gram이 추가된 fasttext 모델은 유사한 단어에 대한 임베딩을 word2vec보다 잘 해결할 수 있으며, 오탈자 등에 대한 임베딩 처리가 가능하다.\n","  - 예) 체크카드, 쳌카드는 word2vec에서는 전혀 다른 단어이지만 fasttext는 character n-gram으로 비교적 같은 단어로 처리할 수 있다.\n","- 참고: https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText\n"],"metadata":{"id":"ghuzogkoAZNt"}},{"cell_type":"markdown","source":["#### 1) 데이터 준비\n","* 시작데이터 : clean_train_questions, clean_test_questions"],"metadata":{"id":"HTGtH4bPAh-8"}},{"cell_type":"markdown","source":["* FastText를 위한 입력 데이터 구조 만들기"],"metadata":{"id":"cQIJ4IPZ0b9C"}},{"cell_type":"code","source":[],"metadata":{"id":"Y-kEQiETAZNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) FastText 모델 생성\n","* FastText 문법\n","    * FastText( input데이터,  min_count = , size= , window=  )\n","        * input데이터 : 학습에 사용할 문장으로 이루어진 리스트\n","        * min_count : 모델에 사용할 단어의 최소 빈도수. 이 값보다 적게 출현한 단어는 모델에 포함되지 않음. 기본값 = 5\n","        * size : 단어의 벡터 차원 지정. 기본값 = 100\n","        * window : 학습할 때 한 단어의 좌우 몇 개의 단어를 보고 예측을 할 것인지를 지정. 기본값 = 5\n","    * 참조 : https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"],"metadata":{"id":"46o8NG_LAk7c"}},{"cell_type":"code","source":["from gensim.models.fasttext import FastText\n","import gensim.models.word2vec\n","\n"],"metadata":{"id":"xlWLTr2nAZNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) train에 임베딩벡터 결과 저장\n","* get_sent_embedding 함수를 이용하여 train의 질문별 임베딩 결과를 저장합니다.\n","    * .apply(lambda .....) 를 활용하세요."],"metadata":{"id":"NilgkdYPAo0s"}},{"cell_type":"code","source":[],"metadata":{"id":"K2iZx5hRAZNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축\n","- input 질문\n","- intent classifier로 common와 faq 중 하나를 예측\n","- 예측된 intent에 속한 train의 모든 Q와 input 문장의 임베딩 벡터간의 코사인 유사도 계산\n","- 코사인 유사도가 가장 높은 top-3개의 Q를 선택\n","- 선택한 Q에 맵핑된 답변 중 하나를 선택하고 실제 답변과 비교"],"metadata":{"id":"HUoQgTf46PJD"}},{"cell_type":"markdown","source":["#### 1) 하나의 질문으로 테스트해보기"],"metadata":{"id":"esu6rcsJ6PJD"}},{"cell_type":"markdown","source":["* 선택된 질문과 답변"],"metadata":{"id":"dCxYTX2W6PJD"}},{"cell_type":"code","source":[],"metadata":{"id":"9Nl6O3Di6PJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측을 위한 입력 형태로 변환"],"metadata":{"id":"g-8Ck-Mh6PJD"}},{"cell_type":"code","source":[],"metadata":{"id":"j0yGO9yQ6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측하기"],"metadata":{"id":"-yp2MBmo6PJE"}},{"cell_type":"code","source":[],"metadata":{"id":"BYm1AODK6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 질문에 대한 벡터 만들기\n","    * FestText 모델로 부터 벡터 만들기"],"metadata":{"id":"uF0D3SH16PJE"}},{"cell_type":"code","source":[],"metadata":{"id":"i_6k-Tjf6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* train의 질문 벡터들과 유사도 계산\n","    * FastText 로 만들 벡터들과 유사도 계산"],"metadata":{"id":"rgEVj1WG6PJE"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","\n"],"metadata":{"id":"OSX-j8WV6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### 2) 함수로 생성하기"],"metadata":{"id":"q-IJcMH26PJF"}},{"cell_type":"code","source":["def get_answer3(question): \n","\n","\n","\n","\n","\n","\n","\n","    return "],"metadata":{"id":"dYlaEeNN6PJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gONYObLpBwgt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"GNfo5m5Hyswe"}},{"cell_type":"code","source":[],"metadata":{"id":"IKFBp5LMBwgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fF1JMFRs6PJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.질문에 대한 답변 비교해보기\n","\n","* **세부요구사항**\n","    * 세가지 챗봇을 생성해 보았습니다. \n","    * 질문을 입력하여 답변을 비교해 봅시다. 어떤 챗봇이 좀 더 정확한 답변을 하나요?\n"],"metadata":{"id":"BEjQHVAJCKkN"}},{"cell_type":"code","source":[],"metadata":{"id":"c8q4Vu5CCvSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-i0609ZGDYYd"},"execution_count":null,"outputs":[]}]}